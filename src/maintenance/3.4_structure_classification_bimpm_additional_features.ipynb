{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary structure classification used in tree building\n",
    "\n",
    "1. prepare train/test sets\n",
    "2. generate config file for bimpm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from utils.file_reading import read_edus, read_gold, read_negative, read_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_sequence(sequence):\n",
    "    symbol_map = {\n",
    "        'x': '—Ö',\n",
    "        'X': 'X',\n",
    "        'y': '—É',\n",
    "        '‚Äî': '-',\n",
    "        '‚Äú': '¬´',\n",
    "        '‚Äò': '¬´',\n",
    "        '‚Äù': '¬ª',\n",
    "        '‚Äô': '¬ª',\n",
    "        'üòÜ': 'üòÑ',\n",
    "        'üòä': 'üòÑ',\n",
    "        'üòë': 'üòÑ',\n",
    "        'üòî': 'üòÑ',\n",
    "        'üòâ': 'üòÑ',\n",
    "        '‚ùó': 'üòÑ',\n",
    "        'ü§î': 'üòÑ',\n",
    "        'üòÖ': 'üòÑ',\n",
    "        '‚öì': 'üòÑ',\n",
    "        'Œµ': 'Œ±',\n",
    "        'Œ∂': 'Œ±',\n",
    "        'Œ∑': 'Œ±',\n",
    "        'Œº': 'Œ±',\n",
    "        'Œ¥': 'Œ±',\n",
    "        'Œª': 'Œ±',\n",
    "        'ŒΩ': 'Œ±',\n",
    "        'Œ≤': 'Œ±',\n",
    "        'Œ≥': 'Œ±',\n",
    "        '„Å®': 'Â∞ã',\n",
    "        '„ÅÆ': 'Â∞ã',\n",
    "        'Á•û': 'Â∞ã',\n",
    "        'Èö†': 'Â∞ã',\n",
    "        '„Åó': 'Â∞ã',\n",
    "    }\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for token in sequence.split():\n",
    "\n",
    "        for key, value in symbol_map.items():\n",
    "            token = token.replace(key, value)\n",
    "\n",
    "        for keyword in ['www', 'http']:\n",
    "            if keyword in token:\n",
    "                token = '_html_'\n",
    "\n",
    "        result.append(token)\n",
    "\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_samples(row):\n",
    "    if row.snippet_x[0] in (',', '.'):\n",
    "        row.snippet_x = row.snippet_x[1:].strip()\n",
    "    if row.snippet_y[0] in (',', '.'):\n",
    "        row.snippet_x += row.snippet_y[0]\n",
    "        row.snippet_y = row.snippet_y[1:].strip()\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = 'models/structure_predictor_lstm'\n",
    "! mkdir $MODEL_PATH\n",
    "\n",
    "TRAIN_FILE_PATH = os.path.join(MODEL_PATH, 'structure_cf4_train.tsv')\n",
    "DEV_FILE_PATH = os.path.join(MODEL_PATH, 'structure_cf4_dev.tsv')\n",
    "TEST_FILE_PATH = os.path.join(MODEL_PATH, 'structure_cf4_test.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Generate train/test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.train_test_split import split_train_dev_test\n",
    "\n",
    "train, dev, test = split_train_dev_test('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 250\n",
    "MAX_DOCS = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "random_state = 45\n",
    "train_samples = []\n",
    "\n",
    "for file in tqdm(train):\n",
    "    gold = read_gold(file.replace('.edus', ''), features=True)\n",
    "    gold['relation'] = 1\n",
    "    gold['len_x'] = gold.tokens_x.map(len)\n",
    "    gold = gold[gold.len_x < MAX_LEN]\n",
    "    gold['len_y'] = gold.tokens_y.map(len)\n",
    "    gold = gold[gold.len_y < MAX_LEN]\n",
    "    gold['snippet_x'] = gold.tokens_x.map(lambda row: ' '.join(row))\n",
    "    gold['snippet_y'] = gold.tokens_y.map(lambda row: ' '.join(row))\n",
    "    gold = gold.apply(correct_samples, axis=1)\n",
    "    sample = gold[['relation', 'snippet_x', 'snippet_y', 'same_sentence']]\n",
    "    negative = read_negative(file.replace('.edus', ''), features=True)\n",
    "    negative['relation'] = 0\n",
    "    negative['len_x'] = negative.tokens_x.map(len)\n",
    "    negative = negative[negative.len_x < MAX_LEN]\n",
    "    negative['len_y'] = negative.tokens_y.map(len)\n",
    "    negative = negative[negative.len_y < MAX_LEN]\n",
    "    negative['snippet_x'] = negative.tokens_x.map(lambda row: ' '.join(row))\n",
    "    negative['snippet_y'] = negative.tokens_y.map(lambda row: ' '.join(row))\n",
    "    negative = negative.apply(correct_samples, axis=1)\n",
    "    sample = pd.concat([sample, negative[['relation', 'snippet_x', 'snippet_y', 'same_sentence']]])\n",
    "    sample = sample.sort_values(['relation'], ascending=True).drop_duplicates(['snippet_x', 'snippet_y'], keep='last')    \n",
    "    train_samples.append(sample)\n",
    "\n",
    "train_samples = pd.concat(train_samples).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "train_samples['snippet_x'] = train_samples.snippet_x.map(_prepare_sequence)\n",
    "train_samples['snippet_y'] = train_samples.snippet_y.map(_prepare_sequence)\n",
    "train_samples.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_samples.relation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_samples[['relation', 'snippet_x', 'snippet_y', 'same_sentence', 'index']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples[['relation', 'snippet_x', 'snippet_y', 'same_sentence', 'index']].to_csv(TRAIN_FILE_PATH, sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_state = 45\n",
    "dev_samples = []\n",
    "\n",
    "for file in tqdm(dev):\n",
    "    gold = read_gold(file.replace('.edus', ''), features=True)\n",
    "    gold['relation'] = 1\n",
    "    gold['len_x'] = gold.tokens_x.map(len)\n",
    "    gold = gold[gold.len_x < MAX_LEN]\n",
    "    gold['len_y'] = gold.tokens_y.map(len)\n",
    "    gold = gold[gold.len_y < MAX_LEN]\n",
    "    gold['snippet_x'] = gold.tokens_x.map(lambda row: ' '.join(row))\n",
    "    gold['snippet_y'] = gold.tokens_y.map(lambda row: ' '.join(row))\n",
    "    gold = gold.apply(correct_samples, axis=1)\n",
    "    sample = gold[['relation', 'snippet_x', 'snippet_y', 'same_sentence']]\n",
    "    negative = read_negative(file.replace('.edus', ''), features=True)\n",
    "    negative['relation'] = 0\n",
    "    negative['len_x'] = negative.tokens_x.map(len)\n",
    "    negative = negative[negative.len_x < MAX_LEN]\n",
    "    negative['len_y'] = negative.tokens_y.map(len)\n",
    "    negative = negative[negative.len_y < MAX_LEN]\n",
    "    negative['snippet_x'] = negative.tokens_x.map(lambda row: ' '.join(row))\n",
    "    negative['snippet_y'] = negative.tokens_y.map(lambda row: ' '.join(row))\n",
    "    negative = negative.apply(correct_samples, axis=1)\n",
    "    sample = pd.concat([sample, negative[['relation', 'snippet_x', 'snippet_y', 'same_sentence']]])\n",
    "    sample = sample.sort_values(['relation'], ascending=True).drop_duplicates(['snippet_x', 'snippet_y'], keep='last') \n",
    "    dev_samples.append(sample)\n",
    "\n",
    "dev_samples = pd.concat(dev_samples).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "dev_samples.reset_index(level=0, inplace=True)\n",
    "dev_samples['snippet_x'] = dev_samples.snippet_x.map(_prepare_sequence)\n",
    "dev_samples['snippet_y'] = dev_samples.snippet_y.map(_prepare_sequence)\n",
    "dev_samples[['relation', 'snippet_x', 'snippet_y', 'same_sentence', 'index']].to_csv(DEV_FILE_PATH, sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_state = 45\n",
    "test_samples = []\n",
    "\n",
    "for file in tqdm(test):\n",
    "    gold = read_gold(file.replace('.edus', ''), features=True)\n",
    "    gold['relation'] = 1\n",
    "    gold['len_x'] = gold.tokens_x.map(len)\n",
    "    gold = gold[gold.len_x < MAX_LEN]\n",
    "    gold['len_y'] = gold.tokens_y.map(len)\n",
    "    gold = gold[gold.len_y < MAX_LEN]\n",
    "    gold['snippet_x'] = gold.tokens_x.map(lambda row: ' '.join(row))\n",
    "    gold['snippet_y'] = gold.tokens_y.map(lambda row: ' '.join(row))\n",
    "    gold = gold.apply(correct_samples, axis=1)\n",
    "    sample = gold[['relation', 'snippet_x', 'snippet_y', 'same_sentence']]\n",
    "    negative = read_negative(file.replace('.edus', ''), features=True)\n",
    "    negative['relation'] = 0\n",
    "    negative['len_x'] = negative.tokens_x.map(len)\n",
    "    negative = negative[negative.len_x < MAX_LEN]\n",
    "    negative['len_y'] = negative.tokens_y.map(len)\n",
    "    negative = negative[negative.len_y < MAX_LEN]\n",
    "    negative['snippet_x'] = negative.tokens_x.map(lambda row: ' '.join(row))\n",
    "    negative['snippet_y'] = negative.tokens_y.map(lambda row: ' '.join(row))\n",
    "    negative = negative.apply(correct_samples, axis=1)\n",
    "    sample = pd.concat([sample, negative[['relation', 'snippet_x', 'snippet_y', 'same_sentence']]])\n",
    "    sample = sample.sort_values(['relation'], ascending=True).drop_duplicates(['snippet_x', 'snippet_y'], keep='last') \n",
    "    test_samples.append(sample)\n",
    "\n",
    "test_samples = pd.concat(test_samples).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "test_samples.reset_index(level=0, inplace=True)\n",
    "test_samples['snippet_x'] = test_samples.snippet_x.map(_prepare_sequence)\n",
    "test_samples['snippet_y'] = test_samples.snippet_y.map(_prepare_sequence)\n",
    "test_samples[['relation', 'snippet_x', 'snippet_y', 'same_sentence', 'index']].to_csv(TEST_FILE_PATH, sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize model with adding inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! rm -r models/customization_package\n",
    "! mkdir models/customization_package\n",
    "! touch models/customization_package/__init__.py\n",
    "! mkdir models/customization_package/dataset_readers\n",
    "! mkdir models/customization_package/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/customization_package/dataset_readers/__init__.py\n",
    "\n",
    "from customization_package.dataset_readers.custom_reader import CustomDataReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/customization_package/dataset_readers/custom_reader.py\n",
    "\n",
    "from typing import Dict, List\n",
    "import logging\n",
    "import csv\n",
    "\n",
    "from overrides import overrides\n",
    "\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "from allennlp.data.fields import LabelField, TextField, Field, ArrayField\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.tokenizers import Tokenizer\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token, Tokenizer, CharacterTokenizer, WordTokenizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@Tokenizer.register(\"simple\")\n",
    "class WhitespaceTokenizer(Tokenizer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        return [Token(token) for token in text.split()]\n",
    "\n",
    "    @overrides\n",
    "    def tokenize(self, text: str) -> List[Token]:\n",
    "        tokens = self._tokenize(text)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "@DatasetReader.register(\"custom_pairs_reader\")\n",
    "class CustomDataReader(DatasetReader):\n",
    "    \"\"\"\n",
    "    # Parameters\n",
    "    tokenizer : `Tokenizer`, optional\n",
    "        Tokenizer to use to split the premise and hypothesis into words or other kinds of tokens.\n",
    "        Defaults to `WhitespaceTokenizer`.\n",
    "    token_indexers : `Dict[str, TokenIndexer]`, optional\n",
    "        Indexers used to define input token representations. Defaults to `{\"tokens\":\n",
    "        SingleIdTokenIndexer()}`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, tokenizer: Tokenizer = None, token_indexers: Dict[str, TokenIndexer] = None, lazy: bool = True) -> None:\n",
    "        super().__init__(lazy)\n",
    "        self._tokenizer = tokenizer or WhitespaceTokenizer()\n",
    "        self._token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "\n",
    "    @overrides\n",
    "    def _read(self, file_path):\n",
    "        logger.info(\"Reading instances from lines in file at: %s\", file_path)\n",
    "        with open(cached_path(file_path), \"r\") as data_file:\n",
    "            tsv_in = csv.reader(data_file, delimiter=\"\\t\")\n",
    "            for row in tsv_in:\n",
    "                if len(row) == 5:\n",
    "                    yield self.text_to_instance(premise=row[1], hypothesis=row[2], label=row[0], same_sentence=row[3])\n",
    "\n",
    "    @overrides\n",
    "    def text_to_instance(\n",
    "        self,  # type: ignore\n",
    "        premise: str,\n",
    "        hypothesis: str,\n",
    "        label: str,\n",
    "        same_sentence: float,\n",
    "    ) -> Instance:\n",
    "\n",
    "        fields: Dict[str, Field] = {}\n",
    "        tokenized_premise = self._tokenizer.tokenize(premise)\n",
    "        tokenized_hypothesis = self._tokenizer.tokenize(hypothesis)\n",
    "        fields[\"premise\"] = TextField(tokenized_premise, self._token_indexers)\n",
    "        fields[\"hypothesis\"] = TextField(tokenized_hypothesis, self._token_indexers)\n",
    "        additional_features = list(map(list, zip(*same_sentence)))\n",
    "        fields[\"metadata\"] = ArrayField(np.array(additional_features))\n",
    "        if label is not None:\n",
    "            fields[\"label\"] = LabelField(label)\n",
    "\n",
    "        return Instance(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/customization_package/model/__init__.py\n",
    "\n",
    "from customization_package.model.custom_bimpm import BiMpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/customization_package/model/custom_bimpm.py\n",
    "\n",
    "\"\"\"\n",
    "BiMPM (Bilateral Multi-Perspective Matching) model implementation.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, Optional, List, Any\n",
    "\n",
    "from overrides import overrides\n",
    "import torch\n",
    "import numpy\n",
    "\n",
    "from allennlp.common.checks import check_dimensions_match\n",
    "from allennlp.data import Vocabulary\n",
    "from allennlp.modules import FeedForward, Seq2SeqEncoder, Seq2VecEncoder, TextFieldEmbedder\n",
    "from allennlp.models.model import Model\n",
    "from allennlp.nn import InitializerApplicator, RegularizerApplicator\n",
    "from allennlp.nn import util\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "\n",
    "from allennlp.modules.bimpm_matching import BiMpmMatching\n",
    "\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "@Model.register(\"custom_bimpm\")\n",
    "class BiMpm(Model):\n",
    "    \"\"\"\n",
    "    This ``Model`` augments with additional features the BiMPM model described in `Bilateral Multi-Perspective \n",
    "    Matching for Natural Language Sentences <https://arxiv.org/abs/1702.03814>`_ by Zhiguo Wang et al., 2017.\n",
    "    implemented in https://github.com/galsang/BIMPM-pytorch>`_.\n",
    "    Additional features are added before the feedforward classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab: Vocabulary,\n",
    "                 text_field_embedder: TextFieldEmbedder,\n",
    "                 matcher_word: BiMpmMatching,\n",
    "                 encoder1: Seq2SeqEncoder,\n",
    "                 matcher_forward1: BiMpmMatching,\n",
    "                 matcher_backward1: BiMpmMatching,\n",
    "                 encoder2: Seq2SeqEncoder,\n",
    "                 matcher_forward2: BiMpmMatching,\n",
    "                 matcher_backward2: BiMpmMatching,\n",
    "                 aggregator: Seq2VecEncoder,\n",
    "                 classifier_feedforward: FeedForward,\n",
    "                 dropout: float = 0.1,\n",
    "                 initializer: InitializerApplicator = InitializerApplicator(),\n",
    "                 regularizer: Optional[RegularizerApplicator] = None) -> None:\n",
    "        super(BiMpm, self).__init__(vocab, regularizer)\n",
    "\n",
    "        self.text_field_embedder = text_field_embedder\n",
    "\n",
    "        self.matcher_word = matcher_word\n",
    "\n",
    "        self.encoder1 = encoder1\n",
    "        self.matcher_forward1 = matcher_forward1\n",
    "        self.matcher_backward1 = matcher_backward1\n",
    "\n",
    "        self.encoder2 = encoder2\n",
    "        self.matcher_forward2 = matcher_forward2\n",
    "        self.matcher_backward2 = matcher_backward2\n",
    "\n",
    "        self.aggregator = aggregator\n",
    "\n",
    "        matching_dim = self.matcher_word.get_output_dim() + \\\n",
    "                       self.matcher_forward1.get_output_dim() + self.matcher_backward1.get_output_dim() + \\\n",
    "                       self.matcher_forward2.get_output_dim() + self.matcher_backward2.get_output_dim()\n",
    "\n",
    "        check_dimensions_match(matching_dim, self.aggregator.get_input_dim(),\n",
    "                               \"sum of dim of all matching layers\", \"aggregator input dim\")\n",
    "\n",
    "        self.classifier_feedforward = classifier_feedforward\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.metrics = {\"accuracy\": CategoricalAccuracy()}\n",
    "\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        initializer(self)\n",
    "\n",
    "    @overrides\n",
    "    def forward(self,  # type: ignore\n",
    "                premise: Dict[str, torch.LongTensor],\n",
    "                hypothesis: Dict[str, torch.LongTensor],\n",
    "                metadata: List[Dict[str, torch.FloatTensor]],\n",
    "                label: torch.LongTensor=None,# pylint:disable=unused-argument\n",
    "               ) -> Dict[str, torch.Tensor]:\n",
    "        # pylint: disable=arguments-differ\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        premise : Dict[str, torch.LongTensor]\n",
    "            The premise from a ``TextField``\n",
    "        hypothesis : Dict[str, torch.LongTensor]\n",
    "            The hypothesis from a ``TextField``\n",
    "        label : torch.LongTensor, optional (default = None)\n",
    "            The label for the pair of the premise and the hypothesis\n",
    "        metadata : ``List[Dict[str, Any]]``, optional, (default = None)\n",
    "            Additional information about the pair\n",
    "        Returns\n",
    "        -------\n",
    "        An output dictionary consisting of:\n",
    "        logits : torch.FloatTensor\n",
    "            A tensor of shape ``(batch_size, num_labels)`` representing unnormalised log\n",
    "            probabilities of the entailment label.\n",
    "        loss : torch.FloatTensor, optional\n",
    "            A scalar loss to be optimised.\n",
    "        \"\"\"\n",
    "\n",
    "        mask_premise = util.get_text_field_mask(premise)\n",
    "        mask_hypothesis = util.get_text_field_mask(hypothesis)\n",
    "\n",
    "        # embedding and encoding of the premise\n",
    "        embedded_premise = self.dropout(self.text_field_embedder(premise))\n",
    "        encoded_premise1 = self.dropout(self.encoder1(embedded_premise, mask_premise))\n",
    "        encoded_premise2 = self.dropout(self.encoder2(encoded_premise1, mask_premise))\n",
    "\n",
    "        # embedding and encoding of the hypothesis\n",
    "        embedded_hypothesis = self.dropout(self.text_field_embedder(hypothesis))\n",
    "        encoded_hypothesis1 = self.dropout(self.encoder1(embedded_hypothesis, mask_hypothesis))\n",
    "        encoded_hypothesis2 = self.dropout(self.encoder2(encoded_hypothesis1, mask_hypothesis))\n",
    "        \n",
    "        matching_vector_premise: List[torch.Tensor] = []\n",
    "        matching_vector_hypothesis: List[torch.Tensor] = []\n",
    "\n",
    "        def add_matching_result(matcher, encoded_premise, encoded_hypothesis):\n",
    "            # utility function to get matching result and add to the result list\n",
    "            matching_result = matcher(encoded_premise, mask_premise, encoded_hypothesis, mask_hypothesis)\n",
    "            matching_vector_premise.extend(matching_result[0])\n",
    "            matching_vector_hypothesis.extend(matching_result[1])\n",
    "\n",
    "        # calculate matching vectors from word embedding, first layer encoding, and second layer encoding\n",
    "        add_matching_result(self.matcher_word, embedded_premise, embedded_hypothesis)\n",
    "        half_hidden_size_1 = self.encoder1.get_output_dim() // 2\n",
    "        add_matching_result(self.matcher_forward1,\n",
    "                            encoded_premise1[:, :, :half_hidden_size_1],\n",
    "                            encoded_hypothesis1[:, :, :half_hidden_size_1])\n",
    "        add_matching_result(self.matcher_backward1,\n",
    "                            encoded_premise1[:, :, half_hidden_size_1:],\n",
    "                            encoded_hypothesis1[:, :, half_hidden_size_1:])\n",
    "\n",
    "        half_hidden_size_2 = self.encoder2.get_output_dim() // 2\n",
    "        add_matching_result(self.matcher_forward2,\n",
    "                            encoded_premise2[:, :, :half_hidden_size_2],\n",
    "                            encoded_hypothesis2[:, :, :half_hidden_size_2])\n",
    "        add_matching_result(self.matcher_backward2,\n",
    "                            encoded_premise2[:, :, half_hidden_size_2:],\n",
    "                            encoded_hypothesis2[:, :, half_hidden_size_2:])\n",
    "\n",
    "        # concat the matching vectors\n",
    "        matching_vector_cat_premise = self.dropout(torch.cat(matching_vector_premise, dim=2))\n",
    "        matching_vector_cat_hypothesis = self.dropout(torch.cat(matching_vector_hypothesis, dim=2))\n",
    "\n",
    "        # aggregate the matching vectors\n",
    "        aggregated_premise = self.dropout(self.aggregator(matching_vector_cat_premise, mask_premise))\n",
    "        aggregated_hypothesis = self.dropout(self.aggregator(matching_vector_cat_hypothesis, mask_hypothesis))\n",
    "\n",
    "        # encode additional information\n",
    "        batch_size, _ = aggregated_premise.size()\n",
    "        encoded_meta = metadata.float().view(batch_size, -1)\n",
    "        \n",
    "        # the final forward layer\n",
    "        logits = self.classifier_feedforward(torch.cat([aggregated_premise, aggregated_hypothesis, encoded_meta], dim=-1))\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        output_dict = {'logits': logits, \"probs\": probs}\n",
    "        if label is not None:\n",
    "            loss = self.loss(logits, label)\n",
    "            for metric in self.metrics.values():\n",
    "                metric(logits, label)\n",
    "            output_dict[\"loss\"] = loss\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "    @overrides\n",
    "    def decode(self, output_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Converts indices to string labels, and adds a ``\"label\"`` key to the result.\n",
    "        \"\"\"\n",
    "        predictions = output_dict[\"probs\"].cpu().data.numpy()\n",
    "        argmax_indices = numpy.argmax(predictions, axis=-1)\n",
    "        labels = [self.vocab.get_token_from_index(x, namespace=\"labels\")\n",
    "                  for x in argmax_indices]\n",
    "        output_dict['label'] = labels\n",
    "        return output_dict\n",
    "\n",
    "    @overrides\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {metric_name: metric.get_metric(reset) for metric_name, metric in self.metrics.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Generate config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile $MODEL_PATH/config4_bert.json\n",
    "\n",
    "// Configuration for a sentence matching model based on:\n",
    "//   Wang, Zhiguo, Wael Hamza, and Radu Florian. \"Bilateral multi-perspective matching for natural language sentences.\"\n",
    "//   Proceedings of the 26th International Joint Conference on Artificial Intelligence. 2017.\n",
    "\n",
    "{\n",
    "  \"dataset_reader\": {\n",
    "    \"type\": \"custom_pairs_reader\",\n",
    "    \"lazy\": false,\n",
    "    \"token_indexers\": {\n",
    "      \"bert\": {\n",
    "          \"type\": \"bert-pretrained\",\n",
    "          \"pretrained_model\": \"rubert_cased_L-12_H-768_A-12_pt\",\n",
    "          \"do_lowercase\": false,\n",
    "          \"use_starting_offsets\": true\n",
    "      },\n",
    "      \"token_characters\": {\n",
    "        \"type\": \"characters\",\n",
    "        \"min_padding_length\": 3\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"train_data_path\": \"structure_predictor_lstm/structure_cf4_train.tsv\",\n",
    "  \"validation_data_path\": \"structure_predictor_lstm/structure_cf4_dev.tsv\",\n",
    "  \"model\": {\n",
    "    \"type\": \"custom_bimpm\",\n",
    "    \"dropout\": 0.2,\n",
    "    \"text_field_embedder\": {\n",
    "        \"allow_unmatched_keys\": true,\n",
    "        \"embedder_to_indexer_map\": {\n",
    "            \"bert\": [\"bert\", \"bert-offsets\"],\n",
    "            \"token_characters\": [\"token_characters\"],\n",
    "        },\n",
    "        \"token_embedders\": {\n",
    "            \"bert\": {\n",
    "                \"type\": \"bert-pretrained\",\n",
    "                \"pretrained_model\": \"rubert_cased_L-12_H-768_A-12_pt\",\n",
    "            },\n",
    "            \"token_characters\": {\n",
    "                \"type\": \"character_encoding\",\n",
    "                \"embedding\": {\n",
    "                    \"embedding_dim\": 20,\n",
    "                    \"padding_index\": 0\n",
    "                },\n",
    "                \"encoder\": {\n",
    "                    \"type\": \"gru\",\n",
    "                    \"input_size\": 20,\n",
    "                    \"hidden_size\": 50,\n",
    "                    \"num_layers\": 1,\n",
    "                    \"bidirectional\": true\n",
    "              }\n",
    "            }\n",
    "      }\n",
    "    },\n",
    "    \"matcher_word\": {\n",
    "      \"is_forward\": true,\n",
    "      \"hidden_dim\": 768+100,\n",
    "      \"num_perspectives\": 10,\n",
    "      \"with_full_match\": false\n",
    "    },\n",
    "    \"encoder1\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": 768+100,\n",
    "      \"hidden_size\": 200,\n",
    "      \"num_layers\": 1\n",
    "    },\n",
    "    \"matcher_forward1\": {\n",
    "      \"is_forward\": true,\n",
    "      \"hidden_dim\": 200,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"matcher_backward1\": {\n",
    "      \"is_forward\": false,\n",
    "      \"hidden_dim\": 200,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"encoder2\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": 400,\n",
    "      \"hidden_size\": 200,\n",
    "      \"num_layers\": 1\n",
    "    },\n",
    "    \"matcher_forward2\": {\n",
    "      \"is_forward\": true,\n",
    "      \"hidden_dim\": 200,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"matcher_backward2\": {\n",
    "      \"is_forward\": false,\n",
    "      \"hidden_dim\": 200,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"aggregator\":{\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": 264,\n",
    "      \"hidden_size\": 100,\n",
    "      \"num_layers\": 2,\n",
    "      \"dropout\": 0.5\n",
    "    },\n",
    "    \"classifier_feedforward\": {\n",
    "      \"input_dim\": 200+200+1,\n",
    "      \"num_layers\": 1,\n",
    "      \"hidden_dims\": [200, 2],\n",
    "      \"activations\": [\"relu\", \"linear\"],\n",
    "      \"dropout\": [0.5, 0.0]\n",
    "    },\n",
    "    \"initializer\": [\n",
    "      [\".*linear_layers.*weight\", {\"type\": \"xavier_normal\"}],\n",
    "      [\".*linear_layers.*bias\", {\"type\": \"constant\", \"val\": 0}],\n",
    "      [\".*weight_ih.*\", {\"type\": \"xavier_normal\"}],\n",
    "      [\".*weight_hh.*\", {\"type\": \"orthogonal\"}],\n",
    "      [\".*bias.*\", {\"type\": \"constant\", \"val\": 0}],\n",
    "      [\".*matcher.*match_weights.*\", {\"type\": \"kaiming_normal\"}]\n",
    "    ]\n",
    "  },\n",
    "  \"iterator\": {\n",
    "    \"type\": \"basic\",\n",
    "    \"batch_size\": 2\n",
    "  },\n",
    "  \"trainer\": {\n",
    "    \"num_epochs\": 10,\n",
    "    \"patience\": 2,\n",
    "    \"cuda_device\": 0,\n",
    "    \"grad_norm\": 10.0,\n",
    "    \"validation_metric\": \"+accuracy\",\n",
    "    \"optimizer\": {\n",
    "      \"type\": \"bert_adam\",\n",
    "      \"lr\": 0.001\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $MODEL_PATH/config4_elmo.json\n",
    "\n",
    "// Configuration for a sentence matching model based on:\n",
    "//   Wang, Zhiguo, Wael Hamza, and Radu Florian. \"Bilateral multi-perspective matching for natural language sentences.\"\n",
    "//   Proceedings of the 26th International Joint Conference on Artificial Intelligence. 2017.\n",
    "\n",
    "{\n",
    "  \"dataset_reader\": {\n",
    "    \"type\": \"custom_pairs_reader\",\n",
    "    \"lazy\": false,\n",
    "    \"token_indexers\": {\n",
    "      \"token_characters\": {\n",
    "        \"type\": \"characters\",\n",
    "        \"min_padding_length\": 3\n",
    "      },\n",
    "      \"elmo\": {\n",
    "        \"type\": \"elmo_characters\"\n",
    "     }\n",
    "    }\n",
    "  },\n",
    "  \"train_data_path\": \"structure_predictor_lstm/structure_cf4_train.tsv\",\n",
    "  \"validation_data_path\": \"structure_predictor_lstm/structure_cf4_dev.tsv\",\n",
    "  \"model\": {\n",
    "    \"type\": \"custom_bimpm\",\n",
    "    \"dropout\": 0.2,\n",
    "    \"text_field_embedder\": {\n",
    "        \"token_embedders\": {\n",
    "            \"elmo\": {\n",
    "                    \"type\": \"elmo_token_embedder\",\n",
    "                    \"options_file\": \"rsv_elmo/options.json\",\n",
    "                    \"weight_file\": \"rsv_elmo/model.hdf5\",\n",
    "                    \"do_layer_norm\": false,\n",
    "                    \"dropout\": 0.0\n",
    "            },\n",
    "            \"token_characters\": {\n",
    "                \"type\": \"character_encoding\",\n",
    "                \"embedding\": {\n",
    "                    \"embedding_dim\": 20,\n",
    "                    \"padding_index\": 0\n",
    "                },\n",
    "                \"encoder\": {\n",
    "                    \"type\": \"gru\",\n",
    "                    \"input_size\": 20,\n",
    "                    \"hidden_size\": 50,\n",
    "                    \"num_layers\": 1,\n",
    "                    \"bidirectional\": true\n",
    "              }\n",
    "            }\n",
    "      }\n",
    "    },\n",
    "    \"matcher_word\": {\n",
    "      \"is_forward\": true,\n",
    "      \"hidden_dim\": 1024+100,\n",
    "      \"num_perspectives\": 10,\n",
    "      \"with_full_match\": false\n",
    "    },\n",
    "    \"encoder1\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": 1024+100,\n",
    "      \"hidden_size\": 200,\n",
    "      \"num_layers\": 1\n",
    "    },\n",
    "    \"matcher_forward1\": {\n",
    "      \"is_forward\": true,\n",
    "      \"hidden_dim\": 200,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"matcher_backward1\": {\n",
    "      \"is_forward\": false,\n",
    "      \"hidden_dim\": 200,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"encoder2\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": 400,\n",
    "      \"hidden_size\": 200,\n",
    "      \"num_layers\": 1\n",
    "    },\n",
    "    \"matcher_forward2\": {\n",
    "      \"is_forward\": true,\n",
    "      \"hidden_dim\": 200,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"matcher_backward2\": {\n",
    "      \"is_forward\": false,\n",
    "      \"hidden_dim\": 200,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"aggregator\":{\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": 264,\n",
    "      \"hidden_size\": 100,\n",
    "      \"num_layers\": 1,\n",
    "      \"dropout\": 0.2\n",
    "    },\n",
    "    \"classifier_feedforward\": {\n",
    "      \"input_dim\": 200+200+1,\n",
    "      \"num_layers\": 2,\n",
    "      \"hidden_dims\": [200, 2],\n",
    "      \"activations\": [\"relu\", \"linear\"],\n",
    "      \"dropout\": [0.5, 0.0]\n",
    "    },\n",
    "    \"initializer\": [\n",
    "      [\".*linear_layers.*weight\", {\"type\": \"xavier_normal\"}],\n",
    "      [\".*linear_layers.*bias\", {\"type\": \"constant\", \"val\": 0}],\n",
    "      [\".*weight_ih.*\", {\"type\": \"xavier_normal\"}],\n",
    "      [\".*weight_hh.*\", {\"type\": \"orthogonal\"}],\n",
    "      [\".*bias.*\", {\"type\": \"constant\", \"val\": 0}],\n",
    "      [\".*matcher.*match_weights.*\", {\"type\": \"kaiming_normal\"}]\n",
    "    ]\n",
    "  },\n",
    "  \"iterator\": {\n",
    "    \"type\": \"basic\",\n",
    "    \"batch_size\": 2\n",
    "  },\n",
    "  \"trainer\": {\n",
    "    \"num_epochs\": 50,\n",
    "    \"patience\": 5,\n",
    "    \"cuda_device\": 0,\n",
    "    \"grad_norm\": 10.0,\n",
    "    \"validation_metric\": \"+accuracy\",\n",
    "    \"optimizer\": {\n",
    "      \"type\": \"adam\",\n",
    "      \"lr\": 0.001\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/train_structure_predictor4.sh\n",
    "# usage:\n",
    "# $ cd models \n",
    "# $ sh train_structure_predictor4.sh {bert|elmo} result_4\n",
    "\n",
    "export METHOD=${1}\n",
    "export RESULT_DIR=${2}\n",
    "export DEV_FILE_PATH=\"structure_cf4_dev.tsv\"\n",
    "export TEST_FILE_PATH=\"structure_cf4_test.tsv\"\n",
    "\n",
    "rm -r structure_predictor_lstm/${RESULT_DIR}/\n",
    "allennlp train -s structure_predictor_lstm/${RESULT_DIR}/ structure_predictor_lstm/config4_${METHOD}.json --include-package customization_package\n",
    "allennlp predict --use-dataset-reader --silent --output-file structure_predictor_lstm/${RESULT_DIR}/predictions_dev.json structure_predictor_lstm/${RESULT_DIR}/model.tar.gz structure_predictor_lstm/${DEV_FILE_PATH}\n",
    "allennlp predict --use-dataset-reader --silent --output-file structure_predictor_lstm/${RESULT_DIR}/predictions_test.json structure_predictor_lstm/${RESULT_DIR}/model.tar.gz structure_predictor_lstm/${TEST_FILE_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Evaluate classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictions(path):\n",
    "    result = []\n",
    "    \n",
    "    with open(path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            result.append(json.loads(line)[\"label\"])\n",
    "            \n",
    "    result = list(map(int, result))\n",
    "    print('length of result:', len(result))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_DIR = 'results_4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "true = pd.read_csv(DEV_FILE_PATH, sep='\\t', header=None)[0].values.tolist()\n",
    "pred = load_predictions(f'{MODEL_PATH}/{RESULT_DIR}/predictions_dev.json')\n",
    "print('length of true labels:', len(true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(true[:len(pred)], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "print('f1: %.2f'%(f1_score(true[:len(pred)], pred)*100))\n",
    "print('pr: %.2f'%(precision_score(true[:len(pred)], pred)*100))\n",
    "print('re: %.2f'%(recall_score(true[:len(pred)], pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "true = pd.read_csv(TEST_FILE_PATH, sep='\\t', header=None)[0].values.tolist()\n",
    "pred = load_predictions(f'{MODEL_PATH}/{RESULT_DIR}/predictions_test.json')\n",
    "print('length of true labels:', len(true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(true[:len(pred)], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "print('f1: %.2f'%(f1_score(true[:len(pred)], pred)*100))\n",
    "print('pr: %.2f'%(precision_score(true[:len(pred)], pred)*100))\n",
    "print('re: %.2f'%(recall_score(true[:len(pred)], pred)*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
