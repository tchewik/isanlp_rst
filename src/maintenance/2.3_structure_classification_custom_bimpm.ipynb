{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary structure classification used in tree building: Step 3. BiMPM\n",
    "\n",
    "Prepare data and model-related scripts.\n",
    "\n",
    "Evaluate models.\n",
    "\n",
    "Output:\n",
    " - ``models/structure_predictor_bimpm/*``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from utils.file_reading import read_edus, read_gold, read_negative, read_annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‚Äòmodels/structure_predictor_bimpm‚Äô: File exists\r\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = 'models/structure_predictor_bimpm'\n",
    "! mkdir $MODEL_PATH\n",
    "\n",
    "TRAIN_FILE_PATH = os.path.join(MODEL_PATH, 'structure_cf_train.tsv')\n",
    "DEV_FILE_PATH = os.path.join(MODEL_PATH, 'structure_cf_dev.tsv')\n",
    "TEST_FILE_PATH = os.path.join(MODEL_PATH, 'structure_cf_test.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config_elmo.json  structure_cf_dev.tsv\t structure_cf_train.tsv\r\n",
      "strprdelmo\t  structure_cf_test.tsv\r\n"
     ]
    }
   ],
   "source": [
    "! ls models/structure_predictor_bimpm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_PATH = 'data_structure'\n",
    "\n",
    "train_samples = pd.read_pickle(os.path.join(IN_PATH, 'train_samples.pkl'))\n",
    "dev_samples = pd.read_pickle(os.path.join(IN_PATH, 'dev_samples.pkl'))\n",
    "test_samples = pd.read_pickle(os.path.join(IN_PATH, 'test_samples.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import razdel\n",
    "\n",
    "def tokenize(text):\n",
    "    result = ' '.join([tok.text for tok in razdel.tokenize(text)])\n",
    "    return result\n",
    "    \n",
    "train_samples['snippet_x'] = train_samples.snippet_x.map(tokenize)\n",
    "train_samples['snippet_y'] = train_samples.snippet_y.map(tokenize)\n",
    "\n",
    "dev_samples['snippet_x'] = dev_samples.snippet_x.map(tokenize)\n",
    "dev_samples['snippet_y'] = dev_samples.snippet_y.map(tokenize)\n",
    "\n",
    "test_samples['snippet_x'] = test_samples.snippet_x.map(tokenize)\n",
    "test_samples['snippet_y'] = test_samples.snippet_y.map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    33744\n",
       "1    16400\n",
       "Name: relation, dtype: int64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples.relation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>snippet_x</th>\n",
       "      <th>snippet_y</th>\n",
       "      <th>category_id</th>\n",
       "      <th>order</th>\n",
       "      <th>filename</th>\n",
       "      <th>is_broken</th>\n",
       "      <th>token_begin_x</th>\n",
       "      <th>token_begin_y</th>\n",
       "      <th>token_end_y</th>\n",
       "      <th>...</th>\n",
       "      <th>eucl_embed_dist</th>\n",
       "      <th>snippet_x_tmp</th>\n",
       "      <th>snippet_y_tmp</th>\n",
       "      <th>postags_x</th>\n",
       "      <th>postags_y</th>\n",
       "      <th>sm_x_positive</th>\n",
       "      <th>sm_x_negative</th>\n",
       "      <th>sm_y_positive</th>\n",
       "      <th>sm_y_negative</th>\n",
       "      <th>relation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>NaN</td>\n",
       "      <td>–≤—ã–ª–æ</td>\n",
       "      <td>–∏ —è—Ä–æ—Å—Ç–Ω–æ —Ç—Ä–µ—â–∞–ª–æ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>./data/blogs_16</td>\n",
       "      <td>False</td>\n",
       "      <td>0.935768</td>\n",
       "      <td>0.937028</td>\n",
       "      <td>0.942065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.917148</td>\n",
       "      <td>–≤—ã—Ç—å_VERB</td>\n",
       "      <td>–∏_CONJ —è—Ä–æ—Å—Ç–Ω–æ_ADV —Ç—Ä–µ—â–∞—Ç—å_VERB ,</td>\n",
       "      <td>VERB</td>\n",
       "      <td>CONJ ADV VERB</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.002126</td>\n",
       "      <td>0.341593</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>2405.0</td>\n",
       "      <td>–≠—Ç–æ</td>\n",
       "      <td>–ø–æ–º–∏–º–æ —è–≤–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω –≤ –≤–∏–¥–µ —Ç—É—Ç –∂–µ –ø–æ—è–≤–∏–≤—à–∏—Ö—Å—è...</td>\n",
       "      <td>elaboration</td>\n",
       "      <td>NS</td>\n",
       "      <td>news1_15</td>\n",
       "      <td>False</td>\n",
       "      <td>0.199614</td>\n",
       "      <td>0.200579</td>\n",
       "      <td>0.216972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.529479</td>\n",
       "      <td>—ç—Ç–æ_PRON</td>\n",
       "      <td>–ø–æ–º–∏–º–æ_ADP —è–≤–Ω—ã–π_ADJ –ø–µ—Ä–µ–º–µ–Ω–∞_NOUN –≤_ADP –≤–∏–¥_N...</td>\n",
       "      <td>PRON</td>\n",
       "      <td>ADP ADJ NOUN ADP NOUN ADV PART VERB ADJ NOUN P...</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.017452</td>\n",
       "      <td>0.065615</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>1316.0</td>\n",
       "      <td>–ë–æ–ª—å</td>\n",
       "      <td>–æ—Ç</td>\n",
       "      <td>cause</td>\n",
       "      <td>NS</td>\n",
       "      <td>blogs_65</td>\n",
       "      <td>False</td>\n",
       "      <td>0.128819</td>\n",
       "      <td>0.129328</td>\n",
       "      <td>0.130346</td>\n",
       "      <td>...</td>\n",
       "      <td>1.167933</td>\n",
       "      <td>–±–æ–ª—å_NOUN</td>\n",
       "      <td>–æ—Ç_ADP –ø–æ—Ç–µ—Ä—è_NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>ADP NOUN</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.384922</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.156115</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 2065 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     level_0 snippet_x                                          snippet_y  \\\n",
       "350      NaN      –≤—ã–ª–æ                                  –∏ —è—Ä–æ—Å—Ç–Ω–æ —Ç—Ä–µ—â–∞–ª–æ   \n",
       "580   2405.0       –≠—Ç–æ  –ø–æ–º–∏–º–æ —è–≤–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω –≤ –≤–∏–¥–µ —Ç—É—Ç –∂–µ –ø–æ—è–≤–∏–≤—à–∏—Ö—Å—è...   \n",
       "883   1316.0      –ë–æ–ª—å                                                 –æ—Ç   \n",
       "\n",
       "     category_id order         filename  is_broken  token_begin_x  \\\n",
       "350          NaN   NaN  ./data/blogs_16      False       0.935768   \n",
       "580  elaboration    NS         news1_15      False       0.199614   \n",
       "883        cause    NS         blogs_65      False       0.128819   \n",
       "\n",
       "     token_begin_y  token_end_y  ...  eucl_embed_dist  snippet_x_tmp  \\\n",
       "350       0.937028     0.942065  ...         0.917148      –≤—ã—Ç—å_VERB   \n",
       "580       0.200579     0.216972  ...         0.529479       —ç—Ç–æ_PRON   \n",
       "883       0.129328     0.130346  ...         1.167933      –±–æ–ª—å_NOUN   \n",
       "\n",
       "                                         snippet_y_tmp  postags_x  \\\n",
       "350                  –∏_CONJ —è—Ä–æ—Å—Ç–Ω–æ_ADV —Ç—Ä–µ—â–∞—Ç—å_VERB ,       VERB   \n",
       "580  –ø–æ–º–∏–º–æ_ADP —è–≤–Ω—ã–π_ADJ –ø–µ—Ä–µ–º–µ–Ω–∞_NOUN –≤_ADP –≤–∏–¥_N...       PRON   \n",
       "883                                 –æ—Ç_ADP –ø–æ—Ç–µ—Ä—è_NOUN       NOUN   \n",
       "\n",
       "                                             postags_y  sm_x_positive  \\\n",
       "350                                     CONJ ADV VERB         0.00001   \n",
       "580  ADP ADJ NOUN ADP NOUN ADV PART VERB ADJ NOUN P...        0.00001   \n",
       "883                                           ADP NOUN        0.00001   \n",
       "\n",
       "     sm_x_negative  sm_y_positive  sm_y_negative  relation  \n",
       "350       0.000010       0.002126       0.341593         0  \n",
       "580       0.000010       0.017452       0.065615         1  \n",
       "883       0.384922       0.000010       0.156115         1  \n",
       "\n",
       "[3 rows x 2065 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples[train_samples.snippet_x.map(len) < 5].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    50144.000000\n",
       "mean       104.064734\n",
       "std        100.394467\n",
       "min          3.000000\n",
       "25%         39.000000\n",
       "50%         70.000000\n",
       "75%        130.000000\n",
       "max        726.000000\n",
       "Name: snippet_x, dtype: float64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = train_samples.snippet_x.map(len)\n",
    "length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of classes: 2\n",
      "class weights: [0.486012 1.      ]\n"
     ]
    }
   ],
   "source": [
    "counts = train_samples['relation'].value_counts(normalize=False).values\n",
    "NUMBER_CLASSES = len(counts)\n",
    "print(\"number of classes:\", NUMBER_CLASSES)\n",
    "print(\"class weights:\", np.round(counts.min() / counts, decimals=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = train_samples.reset_index()\n",
    "train_samples[['relation', 'snippet_x', 'snippet_y', 'same_sentence', 'same_paragraph', 'index']].to_csv(\n",
    "    TRAIN_FILE_PATH, sep='\\t', header=False, index=False)\n",
    "\n",
    "dev_samples = dev_samples.reset_index()\n",
    "dev_samples[['relation', 'snippet_x', 'snippet_y', 'same_sentence', 'same_paragraph', 'index']].to_csv(\n",
    "    DEV_FILE_PATH, sep='\\t', header=False, index=False)\n",
    "\n",
    "test_samples = test_samples.reset_index()\n",
    "test_samples[['relation', 'snippet_x', 'snippet_y', 'same_sentence', 'same_paragraph', 'index']].to_csv(\n",
    "    TEST_FILE_PATH, sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50144, 2066)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize BiMPM model with adding inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! rm -r models/bimpm_custom_package\n",
    "! mkdir models/bimpm_custom_package\n",
    "! touch models/bimpm_custom_package/__init__.py\n",
    "! mkdir models/bimpm_custom_package/tokenizers\n",
    "! mkdir models/bimpm_custom_package/dataset_readers\n",
    "! mkdir models/bimpm_custom_package/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/bimpm_custom_package/dataset_readers/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/bimpm_custom_package/dataset_readers/__init__.py\n",
    "\n",
    "try:\n",
    "    from bimpm_custom_package.tokenizers.whitespace_tokenizer import WhitespaceTokenizer\n",
    "    from bimpm_custom_package.dataset_readers.custom_reader import CustomDataReader\n",
    "except ModuleNotFoundError:\n",
    "    from models.bimpm_custom_package.tokenizers.whitespace_tokenizer import WhitespaceTokenizer\n",
    "    from models.bimpm_custom_package.dataset_readers.custom_reader import CustomDataReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/bimpm_custom_package/tokenizers/whitespace_tokenizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/bimpm_custom_package/tokenizers/whitespace_tokenizer.py\n",
    "\n",
    "from allennlp.data.tokenizers import Token, Tokenizer\n",
    "from overrides import overrides\n",
    "from typing import List\n",
    "\n",
    "\n",
    "@Tokenizer.register(\"whitespace_tokenizer\")\n",
    "class WhitespaceTokenizer(Tokenizer):\n",
    "    def __init__(self, max_length=None) -> None:\n",
    "        super().__init__()\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        if self.max_length:\n",
    "            return [Token(token) for token in text.split()][:self.max_length]\n",
    "        return [Token(token) for token in text.split()]\n",
    "\n",
    "    @overrides\n",
    "    def tokenize(self, text: str) -> List[Token]:\n",
    "        tokens = self._tokenize(text)\n",
    "\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t—á—Ç–æ —Ä–µ—à–µ–Ω–∏–µ –≤–∏–∑–æ–≤–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ –∑–∞–π–º–µ—Ç –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª–µ–µ –¥–ª–∏—Ç–µ–ª—å–Ω—ã–π —Å—Ä–æ–∫ .\t\"\"\" –ü–æ—Å–∫–æ–ª—å–∫—É —ç—Ç–æ –∫–∞—Å–∞–µ—Ç—Å—è –®–µ–Ω–≥–µ–Ω—Å–∫–æ–π –∑–æ–Ω—ã –≤ —Ü–µ–ª–æ–º , –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≤–∏–∑–æ–≤–æ–≥–æ —Ä–µ–∂–∏–º–∞ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ–¥–æ–±—Ä–µ–Ω–æ –≤—Å–µ–º–∏ —Å—Ç—Ä–∞–Ω–∞–º–∏ , –≤—Ö–æ–¥—è—â–∏–º–∏ –≤ –µ–µ —Å–æ—Å—Ç–∞–≤\"\t0\t0\t0\r\n",
      "1\t–Ω–∞—á–∏–Ω–∞—è –∂–µ—Å—Ç–æ–∫–∏–º–∏ –∏–∑–±–∏–µ–Ω–∏—è–º–∏ –∂—É—Ä–Ω–∞–ª–∏—Å—Ç–æ–≤\t–∏ –∑–∞–∫–∞–Ω—á–∏–≤–∞—è –Ω–µ–∑–∞–∫–æ–Ω–Ω—ã–º –ª–∏—à–µ–Ω–∏–µ–º —Å–≤–æ–±–æ–¥—ã –¥–ª—è ¬´ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ ¬ª\t1\t1\t1\r\n",
      "0\t—É –∫–æ–≥–æ-—Ç–æ –æ—á–µ–Ω—å –ø–ª–æ—Ö–æ–π –æ–±–º–µ–Ω –≤–µ—â–µ—Å—Ç–≤ , —á—Ç–æ–±—ã –µ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∏ —Å–¥–µ–ª–∞—Ç—å –µ–≥–æ –±—ã—Å—Ç—Ä–µ–µ . –£ –∫–æ–≥–æ-—Ç–æ —É–∂–µ –ø–∞—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –Ω–∞—Ä—É—à–µ–Ω–∏—è –≤ —Ä–∞–±–æ—Ç–µ –≥–æ—Ä–º–æ–Ω–æ–≤ , –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –ø–æ–∑–≤–æ–ª—è—é—Ç —É–≤–∏–¥–µ—Ç—å –±—ã—Å—Ç—Ä—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç .\t–í–∞—à–µ —Ç–µ–ª–æ –Ω—É–∂–Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞—Ç—å\t0\t0\t2\r\n",
      "0\t–ü–æ –¥–æ—Ä–æ–≥–µ –æ–±—Ä–∞—Ç–Ω–æ –≤ –æ—Ç–µ–ª—å —Å—Ç–æ—è–ª–∏ –≤ –æ–≥—Ä–æ–º–Ω–æ–π –ø—Ä–æ–±–∫–µ –º–∏–Ω—É—Ç 40 .\tIMG –ö–æ–Ω–µ—á–Ω–æ , –Ω–∞–¥–æ –±—ã–ª–æ –∑–∞–π—Ç–∏ –Ω–∞ –±–∞–∑–∞—Ä\t0\t0\t3\r\n",
      "0\t–¢–∞–∫–∂–µ –∑–∞–¥–∞–≤–∞–ª —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å –≤ –ª–∏—á–Ω–æ–π –±–µ—Å–µ–¥–µ –æ–ø—ã—Ç–Ω—ã–º JavaScript-—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞–º , –≤—ã—Å—Ç—É–ø–∞—é—â–∏–º –Ω–∞ –º–∏—Ç–∞–ø–∞—Ö —Å –¥–æ–∫–ª–∞–¥–∞–º–∏ , –∏ –ª—é–¥—è–º –Ω–µ –∏–∑ –º–∏—Ä–∞ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞ , —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞–∑–≤–µ–¥-–æ–ø—Ä–æ—Å–∞ –±—ã–ª —Å–∏–ª—å–Ω–æ –ø–æ—Ö–æ–∂ –Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ—Ç–≤–µ—Ç–æ–≤ –≤ twitter .\t–Ø –∑–Ω–∞–ª –æ—Ç–≤–µ—Ç , —ç—Ç–æ –∂–µ\t0\t0\t4\r\n",
      "0\t–ø–æ—Å–∫–æ–ª—å–∫—É –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–π —Å–∏–ª—å–Ω–æ –Ω–µ–¥–æ–æ—Ü–µ–Ω–µ–Ω—ã –∏ –∏—Ö –∞–∫—Ü–∏–∏ –∏–º–µ—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —Ä–æ—Å—Ç–∞ .\t–ß—Ç–æ –∫–∞—Å–∞–µ—Ç—Å—è –¥–æ–ª–ª–∞—Ä–æ–≤—ã—Ö –¥–µ–ø–æ–∑–∏—Ç–æ–≤\t0\t0\t5\r\n",
      "0\t\"\"\" –≤ 2013 –≥–æ–¥—É –†–æ—Å—Å–∏—è –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–ª–∞ –∏–∑ –ï–° –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –ø–∏—Ç–∞–Ω–∏—è –Ω–∞ —Å—É–º–º—É –≤ 43 –º–ª—Ä–¥ . –¥–æ–ª–ª–∞—Ä–æ–≤ , –ø—Ä–∏ —ç—Ç–æ–º –Ω–∞ —Ç–æ–≤–∞—Ä—ã , [ –ø–æ–∑–¥–Ω–µ–µ ] –≤–∫–ª—é—á–µ–Ω–Ω—ã–µ –≤ —ç–º–±–∞—Ä–≥–æ , –ø—Ä–∏—Ö–æ–¥–∏–ª–æ—Å—å –ø–æ—á—Ç–∏ 9 –º–ª—Ä–¥ . –¥–æ–ª–ª–∞—Ä–æ–≤ \"\" .\"\t–í –∞–≤–≥—É—Å—Ç–µ 2014 –≥–æ–¥–∞ –≥–ª–∞–≤–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞ –ï–≤—Ä–æ—Å–æ—é–∑–∞ –≤ –†–§ [ –í–∏–≥–∞—É–¥–∞—Å –£—à–∞—Ü–∫–∞—Å ( V —É gaudas Usackas ) ] –∑–∞—è–≤–∏–ª\t0\t0\t6\r\n",
      "1\t–í–µ–¥—å –¥–ª—è —Ç–æ–≥–æ —á—Ç–æ–±—ã –ø–æ–∫—Ä—ã–≤–∞—Ç—å –¥–µ—Ñ–∏—Ü–∏—Ç –ø–ª–∞—Ç–µ–∂–Ω–æ–≥–æ –±–∞–ª–∞–Ω—Å–∞ , –°–®–ê –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–æ–ª—É—á–∞—Ç—å –æ—Ç –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω—ã—Ö –∏–Ω–≤–µ—Å—Ç–æ—Ä–æ–≤ 1,4 –º–∏–ª–ª–∏–∞—Ä–¥–∞ –¥–æ–ª–ª–∞—Ä–æ–≤ –µ–∂–µ–¥–Ω–µ–≤–Ω–æ .\t\"–ï—Å–ª–∏ –∂–µ –¥–µ–Ω—å–≥–∏ –ø–æ–∫–∏–¥–∞—é—Ç –ê–º–µ—Ä–∏–∫—É , —ç—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç , —á—Ç–æ –¥–ª—è –∏–Ω–≤–µ—Å—Ç–æ—Ä–æ–≤ —Ä–∏—Å–∫ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π –≤ —ç–∫–æ–Ω–æ–º–∏–∫—É –°–®–ê –ø–µ—Ä–µ–≤–µ—à–∏–≤–∞–µ—Ç –æ–∂–∏–¥–∞–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ–π –ø—Ä–∏–±—ã–ª–∏ . –ò —ç—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç , –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –Ω–µ–æ–±—ã—á–∞–π–Ω–æ —Å–ª–∞–±—ã–π —Ä–æ—Å—Ç –µ–≤—Ä–æ–ø–µ–π—Å–∫–æ–π —ç–∫–æ–Ω–æ–º–∏–∫–∏ ( 1-2 –ø—Ä–æ—Ü–µ–Ω—Ç–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å—Ç—Ä–∞–Ω—ã ) . –û—á–µ–≤–∏–¥–Ω–æ , –¥–µ–π—Å—Ç–≤–∏—è–º–∏ –∏–Ω–≤–µ—Å—Ç–æ—Ä–æ–≤ —Ä—É–∫–æ–≤–æ–¥–∏—Ç –ª–æ–≥–∏–∫–∞ \"\" –ø—É—Å—Ç—å –ø–æ–º–µ–Ω—å—à–µ , –Ω–æ –∑–∞—Ç–æ –Ω–∞–¥–µ–∂–Ω–æ \"\"\"\t0\t0\t7\r\n",
      "0\t–ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –≤–µ–≥–∞–Ω–æ–≤ –∏ –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–∞ –∫–∞–∫ cruelt —É-free . IMG –ú–∞—Å–∫–∞ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–∞ –¥–ª—è –æ—á–∏—â–µ–Ω–∏—è –∏ –ø–∏—Ç–∞–Ω–∏—è –∫–æ–∂–∏ , –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–µ —á–µ—Ä–µ–∑ –¥–µ–Ω—å . –ù–∞ –º–æ–π –≤–∑–≥–ª—è–¥ , —ç—Ç–æ –¥–æ–≤–æ–ª—å–Ω–æ —á–∞—Å—Ç–æ –¥–ª—è –º–∞—Å–∫–∏ , —Ç–∞–∫ –∫–∞–∫ —ç—Ç–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—è —É—Ö–æ–¥–æ–≤–æ–π –∫–æ—Å–º–µ—Ç–∏–∫–∏ , –ø–æ –∏–¥–µ–µ , –¥–æ–ª–∂–Ω–∞ –¥–∞–≤–∞—Ç—å –±—ã—Å—Ç—Ä—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞ –æ–¥–Ω–æ –Ω–∞–Ω–µ—Å–µ–Ω–∏–µ . üòÑ –ü–æ–ø—Ä–æ–±–æ–≤–∞–≤ –¥–µ–π—Å—Ç–≤–∏–µ –º–∞—Å–∫–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –¥–Ω–∏ , —è\t–¥–ª—è —á–∏—Å—Ç–æ—Ç—ã\t0\t0\t8\r\n",
      "0\t–ù–æ –≤–æ—Ç –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ —Ä–∞–∑–¥—É–º—ã–≤–∞—Ç—å –æ–± —ç—Ç–æ–º - –∫–∞–∫ –Ω–∞—Å –º–æ–∂–µ—Ç —Å–∂–∏—Ä–∞—Ç—å —Å—Ç—Ä–∞—Ö , —Å—Ç–æ–∏—Ç –º–∏—Ä—É –∫–∞–∫-—Ç–æ –ø–æ—à–∞—Ç–Ω—É—Ç—å—Å—è .\t–Ø - —Å–æ–≤—Å–µ–º –Ω–µ —Å–ø–æ—Ä—Ç—Å–º–µ–Ω–∫–∞\t0\t0\t9\r\n"
     ]
    }
   ],
   "source": [
    "! head $TRAIN_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# file_path = TRAIN_FILE_PATH\n",
    "# with open(file_path, \"r\") as data_file:\n",
    "#     tsv_in = csv.reader(data_file, delimiter=\"\\t\")\n",
    "#     for row in tsv_in:\n",
    "#         if len(row) == 6:\n",
    "#             print('+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/bimpm_custom_package/dataset_readers/custom_reader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/bimpm_custom_package/dataset_readers/custom_reader.py\n",
    "\n",
    "import csv\n",
    "import logging\n",
    "from typing import Optional, Dict\n",
    "\n",
    "import numpy as np\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "from allennlp.data.fields import LabelField, TextField, Field, ArrayField\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Tokenizer, SpacyTokenizer, PretrainedTransformerTokenizer\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@DatasetReader.register(\"custom_pairs_reader\")\n",
    "class CustomDataReader(DatasetReader):\n",
    "    \"\"\"\n",
    "    # Parameters\n",
    "    tokenizer : `Tokenizer`, optional\n",
    "        Tokenizer to use to split the premise and hypothesis into words or other kinds of tokens.\n",
    "        Defaults to `WhitespaceTokenizer`.\n",
    "    token_indexers : `Dict[str, TokenIndexer]`, optional\n",
    "        Indexers used to define input token representations. Defaults to `{\"tokens\":\n",
    "        SingleIdTokenIndexer()}`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            tokenizer: Tokenizer = None,\n",
    "            token_indexers: Dict[str, TokenIndexer] = None,\n",
    "            combine_input_fields: Optional[bool] = None,\n",
    "            **kwargs) -> None:\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self._tokenizer = tokenizer or SpacyTokenizer()\n",
    "        self._token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "\n",
    "        if isinstance(self._tokenizer, PretrainedTransformerTokenizer):\n",
    "            assert not self._tokenizer._add_special_tokens\n",
    "\n",
    "        if combine_input_fields is not None:\n",
    "            self._combine_input_fields = combine_input_fields\n",
    "        else:\n",
    "            self._combine_input_fields = isinstance(self._tokenizer, PretrainedTransformerTokenizer)\n",
    "\n",
    "    def _read(self, file_path):\n",
    "        logger.info(\"Reading instances from lines in file at: %s\", file_path)\n",
    "        file_path = cached_path(file_path)\n",
    "        with open(file_path, \"r\") as data_file:\n",
    "            tsv_in = csv.reader(data_file, delimiter=\"\\t\")\n",
    "            for row in tsv_in:\n",
    "                if len(row) == 6:\n",
    "                    yield self.text_to_instance(premise=row[1], hypothesis=row[2], label=row[0],\n",
    "                                                same_sentence=row[3], same_paragraph=row[4])\n",
    "\n",
    "    def text_to_instance(\n",
    "            self,  # type: ignore\n",
    "            premise: str,\n",
    "            hypothesis: str,\n",
    "            label: str,\n",
    "            same_sentence: str,\n",
    "            same_paragraph: str,\n",
    "    ) -> Instance:\n",
    "\n",
    "        fields: Dict[str, Field] = {}\n",
    "        tokenized_premise = self._tokenizer.tokenize(premise)\n",
    "        tokenized_hypothesis = self._tokenizer.tokenize(hypothesis)\n",
    "\n",
    "        if self._combine_input_fields:\n",
    "            tokens = self._tokenizer.add_special_tokens(tokenized_premise, tokenized_hypothesis)\n",
    "            fields[\"tokens\"] = TextField(tokens, self._token_indexers)\n",
    "        else:\n",
    "            tokenized_premise = self._tokenizer.add_special_tokens(tokenized_premise)\n",
    "            tokenized_hypothesis = self._tokenizer.add_special_tokens(tokenized_hypothesis)\n",
    "            fields[\"premise\"] = TextField(tokenized_premise, self._token_indexers)\n",
    "            fields[\"hypothesis\"] = TextField(tokenized_hypothesis, self._token_indexers)\n",
    "\n",
    "        _same_sentence = list(map(list, zip(*same_sentence)))\n",
    "        _same_paragraph = list(map(list, zip(*same_paragraph)))\n",
    "        fields[\"same_sentence\"] = ArrayField(np.array(_same_sentence).astype(np.float32))\n",
    "        fields[\"same_paragraph\"] = ArrayField(np.array(_same_paragraph).astype(np.float32))\n",
    "\n",
    "        if label is not None:\n",
    "            fields[\"label\"] = LabelField(label)\n",
    "\n",
    "        return Instance(fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/bimpm_custom_package/model/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/bimpm_custom_package/model/__init__.py\n",
    "\n",
    "try:\n",
    "    from bimpm_custom_package.tokenizers.whitespace_tokenizer import WhitespaceTokenizer\n",
    "    from bimpm_custom_package.model.custom_bimpm import BiMpm as CustomBiMpm\n",
    "    from bimpm_custom_package.model.multiclass_bimpm import BiMpm as MulticlassBiMpm\n",
    "    from bimpm_custom_package.model.custom_bimpm_predictor import CustomBiMPMPredictor\n",
    "except ModuleNotFoundError:\n",
    "    from models.bimpm_custom_package.tokenizers.whitespace_tokenizer import WhitespaceTokenizer\n",
    "    from models.bimpm_custom_package.model.custom_bimpm import BiMpm as CustomBiMpm\n",
    "    from models.bimpm_custom_package.model.multiclass_bimpm import BiMpm as MulticlassBiMpm\n",
    "    from models.bimpm_custom_package.model.custom_bimpm_predictor import CustomBiMPMPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/bimpm_custom_package/model/custom_bimpm.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/bimpm_custom_package/model/custom_bimpm.py\n",
    "\"\"\"\n",
    "BiMPM (Bilateral Multi-Perspective Matching) model implementation.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "import torch\n",
    "from allennlp.common.checks import check_dimensions_match\n",
    "from allennlp.data import Vocabulary\n",
    "from allennlp.models.model import Model\n",
    "from allennlp.modules import FeedForward, Seq2SeqEncoder, Seq2VecEncoder, TextFieldEmbedder\n",
    "from allennlp.modules.bimpm_matching import BiMpmMatching\n",
    "from allennlp.nn import InitializerApplicator\n",
    "from allennlp.nn import util\n",
    "from allennlp.training.metrics import CategoricalAccuracy, F1Measure\n",
    "from overrides import overrides\n",
    "\n",
    "\n",
    "@Model.register(\"custom_bimpm\")\n",
    "class BiMpm(Model):\n",
    "    \"\"\"\n",
    "    This ``Model`` augments with additional features the BiMPM model described in `Bilateral Multi-Perspective \n",
    "    Matching for Natural Language Sentences <https://arxiv.org/abs/1702.03814>`_ by Zhiguo Wang et al., 2017.\n",
    "    implemented in https://github.com/galsang/BIMPM-pytorch>`_.\n",
    "    Additional features are added before the feedforward classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab: Vocabulary,\n",
    "                 text_field_embedder: TextFieldEmbedder,\n",
    "                 matcher_word: BiMpmMatching,\n",
    "                 encoder1: Seq2SeqEncoder,\n",
    "                 matcher_forward1: BiMpmMatching,\n",
    "                 matcher_backward1: BiMpmMatching,\n",
    "                 encoder2: Seq2SeqEncoder,\n",
    "                 matcher_forward2: BiMpmMatching,\n",
    "                 matcher_backward2: BiMpmMatching,\n",
    "                 aggregator: Seq2VecEncoder,\n",
    "                 classifier_feedforward: FeedForward,\n",
    "                 encode_together: bool = False,\n",
    "                 encode_lstm: bool = True,\n",
    "                 dropout: float = 0.1,\n",
    "                 class_weights: list = [],\n",
    "                 initializer: InitializerApplicator = InitializerApplicator(),\n",
    "                 **kwargs) -> None:\n",
    "        super().__init__(vocab, **kwargs)\n",
    "\n",
    "        self.text_field_embedder = text_field_embedder\n",
    "\n",
    "        self.matcher_word = matcher_word\n",
    "\n",
    "        self.encoder1 = encoder1\n",
    "        self.matcher_forward1 = matcher_forward1\n",
    "        self.matcher_backward1 = matcher_backward1\n",
    "\n",
    "        self.encoder2 = encoder2\n",
    "        self.matcher_forward2 = matcher_forward2\n",
    "        self.matcher_backward2 = matcher_backward2\n",
    "\n",
    "        self.aggregator = aggregator\n",
    "\n",
    "        self.encode_together = encode_together\n",
    "        self.encode_lstm = encode_lstm\n",
    "\n",
    "        matching_dim = self.matcher_word.get_output_dim()\n",
    "\n",
    "        if self.encode_lstm:\n",
    "            matching_dim += self.matcher_forward1.get_output_dim(\n",
    "            ) + self.matcher_backward1.get_output_dim(\n",
    "            ) + self.matcher_forward2.get_output_dim(\n",
    "            ) + self.matcher_backward2.get_output_dim(\n",
    "            )\n",
    "\n",
    "        check_dimensions_match(matching_dim, self.aggregator.get_input_dim(),\n",
    "                               \"sum of dim of all matching layers\", \"aggregator input dim\")\n",
    "\n",
    "        self.classifier_feedforward = classifier_feedforward\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        if class_weights:\n",
    "            self.class_weights = class_weights\n",
    "        else:\n",
    "            self.class_weights = [1.] * self.classifier_feedforward.get_output_dim()\n",
    "\n",
    "        self.metrics = {\"accuracy\": CategoricalAccuracy(),\n",
    "                        \"f1\": F1Measure(1)}\n",
    "\n",
    "        self.loss = torch.nn.CrossEntropyLoss(weight=torch.FloatTensor(self.class_weights))\n",
    "\n",
    "        initializer(self)\n",
    "\n",
    "    @overrides\n",
    "    def forward(self,\n",
    "                premise: Dict[str, torch.LongTensor],\n",
    "                hypothesis: Dict[str, torch.LongTensor],\n",
    "                same_sentence: List[Dict[str, torch.IntTensor]],\n",
    "                same_paragraph: List[Dict[str, torch.IntTensor]],\n",
    "                label: torch.LongTensor = None,  # pylint:disable=unused-argument\n",
    "                ) -> Dict[str, torch.Tensor]:\n",
    "        # pylint: disable=arguments-differ\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        premise : Dict[str, torch.LongTensor]\n",
    "            The premise from a ``TextField``\n",
    "        hypothesis : Dict[str, torch.LongTensor]\n",
    "            The hypothesis from a ``TextField``\n",
    "        label : torch.LongTensor, optional (default = None)\n",
    "            The label for the pair of the premise and the hypothesis\n",
    "        metadata : ``List[Dict[str, Any]]``, optional, (default = None)\n",
    "            Additional information about the pair\n",
    "        Returns\n",
    "        -------\n",
    "        An output dictionary consisting of:\n",
    "        logits : torch.FloatTensor\n",
    "            A tensor of shape ``(batch_size, num_labels)`` representing unnormalised log\n",
    "            probabilities of the entailment label.\n",
    "        loss : torch.FloatTensor, optional\n",
    "            A scalar loss to be optimised.\n",
    "        \"\"\"\n",
    "\n",
    "        def encode_pair(x1, x2, mask1=None, mask2=None):\n",
    "            _joined_pair: Dict[str, torch.LongTensor] = {}\n",
    "\n",
    "            for key in premise.keys():\n",
    "                bsz = premise[key].size(0)\n",
    "                x1_len, x2_len = premise[key].size(1), hypothesis[key].size(1)\n",
    "                sep = torch.empty([bsz, 1], dtype=torch.long, device=premise[key].device)\n",
    "                sep.data.fill_(0)  # 2 is the id for </s>\n",
    "\n",
    "                x = torch.cat([premise[key], hypothesis[key]], dim=1)\n",
    "                _joined_pair[key] = x\n",
    "\n",
    "            x_output = self.dropout(self.text_field_embedder(_joined_pair))\n",
    "            return x_output[:, :x1_len], x_output[:, -x2_len:], mask1, mask2\n",
    "\n",
    "        mask_premise = util.get_text_field_mask(premise)\n",
    "        mask_hypothesis = util.get_text_field_mask(hypothesis)\n",
    "\n",
    "        if self.encode_together:\n",
    "            embedded_premise, embedded_hypothesis, _, _ = encode_pair(premise, hypothesis)\n",
    "        else:\n",
    "            embedded_premise = self.dropout(self.text_field_embedder(premise))\n",
    "            embedded_hypothesis = self.dropout(self.text_field_embedder(hypothesis))\n",
    "\n",
    "        # embedding and encoding of the premise\n",
    "        encoded_premise1 = self.dropout(self.encoder1(embedded_premise, mask_premise))\n",
    "        encoded_premise2 = self.dropout(self.encoder2(encoded_premise1, mask_premise))\n",
    "\n",
    "        # embedding and encoding of the hypothesis\n",
    "        encoded_hypothesis1 = self.dropout(self.encoder1(embedded_hypothesis, mask_hypothesis))\n",
    "        encoded_hypothesis2 = self.dropout(self.encoder2(encoded_hypothesis1, mask_hypothesis))\n",
    "\n",
    "        matching_vector_premise: List[torch.Tensor] = []\n",
    "        matching_vector_hypothesis: List[torch.Tensor] = []\n",
    "\n",
    "        def add_matching_result(matcher, encoded_premise, encoded_hypothesis):\n",
    "            # utility function to get matching result and add to the result list\n",
    "            matching_result = matcher(encoded_premise, mask_premise, encoded_hypothesis, mask_hypothesis)\n",
    "            matching_vector_premise.extend(matching_result[0])\n",
    "            matching_vector_hypothesis.extend(matching_result[1])\n",
    "\n",
    "        # calculate matching vectors from word embedding, first layer encoding, and second layer encoding\n",
    "        add_matching_result(self.matcher_word, embedded_premise, embedded_hypothesis)\n",
    "        half_hidden_size_1 = self.encoder1.get_output_dim() // 2\n",
    "        add_matching_result(self.matcher_forward1,\n",
    "                            encoded_premise1[:, :, :half_hidden_size_1],\n",
    "                            encoded_hypothesis1[:, :, :half_hidden_size_1])\n",
    "        add_matching_result(self.matcher_backward1,\n",
    "                            encoded_premise1[:, :, half_hidden_size_1:],\n",
    "                            encoded_hypothesis1[:, :, half_hidden_size_1:])\n",
    "\n",
    "        half_hidden_size_2 = self.encoder2.get_output_dim() // 2\n",
    "        add_matching_result(self.matcher_forward2,\n",
    "                            encoded_premise2[:, :, :half_hidden_size_2],\n",
    "                            encoded_hypothesis2[:, :, :half_hidden_size_2])\n",
    "        add_matching_result(self.matcher_backward2,\n",
    "                            encoded_premise2[:, :, half_hidden_size_2:],\n",
    "                            encoded_hypothesis2[:, :, half_hidden_size_2:])\n",
    "\n",
    "        # concat the matching vectors\n",
    "        matching_vector_cat_premise = self.dropout(torch.cat(matching_vector_premise, dim=2))\n",
    "        matching_vector_cat_hypothesis = self.dropout(torch.cat(matching_vector_hypothesis, dim=2))\n",
    "\n",
    "        # aggregate the matching vectors\n",
    "        aggregated_premise = self.dropout(self.aggregator(matching_vector_cat_premise, mask_premise))\n",
    "        aggregated_hypothesis = self.dropout(self.aggregator(matching_vector_cat_hypothesis, mask_hypothesis))\n",
    "\n",
    "        # encode additional information\n",
    "        batch_size, _ = aggregated_premise.size()\n",
    "        encoded_same_sentence = same_sentence.float().view(batch_size, -1)\n",
    "        encoded_same_paragraph = same_paragraph.float().view(batch_size, -1)\n",
    "\n",
    "        # the final forward layer\n",
    "        logits = self.classifier_feedforward(\n",
    "            torch.cat([aggregated_premise,\n",
    "                       aggregated_hypothesis,\n",
    "                       encoded_same_sentence,\n",
    "                       encoded_same_paragraph], dim=-1))\n",
    "\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        output_dict = {'logits': logits, \"probs\": probs}\n",
    "\n",
    "        if label is not None:\n",
    "            loss = self.loss(logits, label)\n",
    "            output_dict[\"loss\"] = loss\n",
    "\n",
    "            for metric in self.metrics.values():\n",
    "                metric(logits, label)\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "    def make_output_human_readable(self, output_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Does a simple argmax over the probabilities, converts index to string label, and\n",
    "        add `\"label\"` key to the dictionary with the result.\n",
    "        \"\"\"\n",
    "        predictions = output_dict[\"label_probs\"]\n",
    "        if predictions.dim() == 2:\n",
    "            predictions_list = [predictions[i] for i in range(predictions.shape[0])]\n",
    "        else:\n",
    "            predictions_list = [predictions]\n",
    "        classes = []\n",
    "        for prediction in predictions_list:\n",
    "            label_idx = prediction.argmax(dim=-1).item()\n",
    "            label_str = self.vocab.get_index_to_token_vocabulary(\"labels\").get(\n",
    "                label_idx, str(label_idx)\n",
    "            )\n",
    "            classes.append(label_str)\n",
    "        output_dict[\"label\"] = classes\n",
    "        return output_dict\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\n",
    "            \"f1\": self.metrics[\"f1\"].get_metric(reset)['f1'],\n",
    "            \"accuracy\": self.metrics[\"accuracy\"].get_metric(reset)\n",
    "        }\n",
    "\n",
    "    default_predictor = 'custom_bimpm_predictor'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/bimpm_custom_package/model/custom_bimpm_predictor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/bimpm_custom_package/model/custom_bimpm_predictor.py\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy\n",
    "from allennlp.common import JsonDict\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields.label_field import LabelField\n",
    "from allennlp.data.tokenizers.spacy_tokenizer import SpacyTokenizer\n",
    "from allennlp.predictors import Predictor\n",
    "from overrides import overrides\n",
    "\n",
    "\n",
    "# You need to name your predictor and register so that `allennlp` command can recognize it\n",
    "# Note that you need to use \"@Predictor.register\", not \"@Model.register\"!\n",
    "@Predictor.register(\"custom_bimpm_predictor\")\n",
    "class CustomBiMPMPredictor(Predictor):\n",
    "\n",
    "    def predict(self, premise: str, hypothesis: str, same_sentence: str, same_paragraph: str) -> JsonDict:\n",
    "        return self.predict_json({\"premise\": premise, \"hypothesis\": hypothesis,\n",
    "                                  \"same_sentence\": same_sentence, \"same_paragraph\": same_paragraph})\n",
    "\n",
    "    @overrides\n",
    "    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n",
    "        \"\"\"\n",
    "        Expects JSON that looks like `{\"premise\": \"...\", \"hypothesis\": \"...\", \"metadata\": \"...\"}`.\n",
    "        \"\"\"\n",
    "        premise_text = json_dict[\"premise\"]\n",
    "        hypothesis_text = json_dict[\"hypothesis\"]\n",
    "        same_sentence = json_dict[\"same_sentence\"]\n",
    "        same_paragraph = json_dict[\"same_paragraph\"]\n",
    "        reader_has_tokenizer = (\n",
    "                getattr(self._dataset_reader, \"tokenizer\", None) is not None\n",
    "                or getattr(self._dataset_reader, \"_tokenizer\", None) is not None\n",
    "        )\n",
    "        if not reader_has_tokenizer:\n",
    "            tokenizer = SpacyTokenizer()\n",
    "            premise_text = tokenizer.tokenize(premise_text)\n",
    "            hypothesis_text = tokenizer.tokenize(hypothesis_text)\n",
    "\n",
    "        return self._dataset_reader.text_to_instance(premise_text,\n",
    "                                                     hypothesis_text,\n",
    "                                                     label=None,\n",
    "                                                     same_sentence=same_sentence,\n",
    "                                                     same_paragraph=same_paragraph)\n",
    "\n",
    "    def predictions_to_labeled_instances(\n",
    "            self, instance: Instance, outputs: Dict[str, numpy.ndarray]\n",
    "    ) -> List[Instance]:\n",
    "        new_instance = instance.duplicate()\n",
    "        label = numpy.argmax(outputs[\"label_logits\"])\n",
    "        # Skip indexing, we have integer representations of the strings \"entailment\", etc.\n",
    "        new_instance.add_field(\"label\", LabelField(int(label), skip_indexing=True))\n",
    "        return [new_instance]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Generate config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/structure_predictor_bimpm/structure_cf_train.tsv\n",
      "models/structure_predictor_bimpm/structure_cf_dev.tsv\n",
      "models/structure_predictor_bimpm/structure_cf_test.tsv\n"
     ]
    }
   ],
   "source": [
    "print(TRAIN_FILE_PATH)\n",
    "print(DEV_FILE_PATH)\n",
    "print(TEST_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t—á—Ç–æ —Ä–µ—à–µ–Ω–∏–µ –≤–∏–∑–æ–≤–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ –∑–∞–π–º–µ—Ç –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª–µ–µ –¥–ª–∏—Ç–µ–ª—å–Ω—ã–π —Å—Ä–æ–∫ .\t\"\"\" –ü–æ—Å–∫–æ–ª—å–∫—É —ç—Ç–æ –∫–∞—Å–∞–µ—Ç—Å—è –®–µ–Ω–≥–µ–Ω—Å–∫–æ–π –∑–æ–Ω—ã –≤ —Ü–µ–ª–æ–º , –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≤–∏–∑–æ–≤–æ–≥–æ —Ä–µ–∂–∏–º–∞ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ–¥–æ–±—Ä–µ–Ω–æ –≤—Å–µ–º–∏ —Å—Ç—Ä–∞–Ω–∞–º–∏ , –≤—Ö–æ–¥—è—â–∏–º–∏ –≤ –µ–µ —Å–æ—Å—Ç–∞–≤\"\t0\t0\t0\r\n",
      "1\t–Ω–∞—á–∏–Ω–∞—è –∂–µ—Å—Ç–æ–∫–∏–º–∏ –∏–∑–±–∏–µ–Ω–∏—è–º–∏ –∂—É—Ä–Ω–∞–ª–∏—Å—Ç–æ–≤\t–∏ –∑–∞–∫–∞–Ω—á–∏–≤–∞—è –Ω–µ–∑–∞–∫–æ–Ω–Ω—ã–º –ª–∏—à–µ–Ω–∏–µ–º —Å–≤–æ–±–æ–¥—ã –¥–ª—è ¬´ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ ¬ª\t1\t1\t1\r\n",
      "0\t—É –∫–æ–≥–æ-—Ç–æ –æ—á–µ–Ω—å –ø–ª–æ—Ö–æ–π –æ–±–º–µ–Ω –≤–µ—â–µ—Å—Ç–≤ , —á—Ç–æ–±—ã –µ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∏ —Å–¥–µ–ª–∞—Ç—å –µ–≥–æ –±—ã—Å—Ç—Ä–µ–µ . –£ –∫–æ–≥–æ-—Ç–æ —É–∂–µ –ø–∞—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –Ω–∞—Ä—É—à–µ–Ω–∏—è –≤ —Ä–∞–±–æ—Ç–µ –≥–æ—Ä–º–æ–Ω–æ–≤ , –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –ø–æ–∑–≤–æ–ª—è—é—Ç —É–≤–∏–¥–µ—Ç—å –±—ã—Å—Ç—Ä—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç .\t–í–∞—à–µ —Ç–µ–ª–æ –Ω—É–∂–Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞—Ç—å\t0\t0\t2\r\n",
      "0\t–ü–æ –¥–æ—Ä–æ–≥–µ –æ–±—Ä–∞—Ç–Ω–æ –≤ –æ—Ç–µ–ª—å —Å—Ç–æ—è–ª–∏ –≤ –æ–≥—Ä–æ–º–Ω–æ–π –ø—Ä–æ–±–∫–µ –º–∏–Ω—É—Ç 40 .\tIMG –ö–æ–Ω–µ—á–Ω–æ , –Ω–∞–¥–æ –±—ã–ª–æ –∑–∞–π—Ç–∏ –Ω–∞ –±–∞–∑–∞—Ä\t0\t0\t3\r\n",
      "0\t–¢–∞–∫–∂–µ –∑–∞–¥–∞–≤–∞–ª —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å –≤ –ª–∏—á–Ω–æ–π –±–µ—Å–µ–¥–µ –æ–ø—ã—Ç–Ω—ã–º JavaScript-—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞–º , –≤—ã—Å—Ç—É–ø–∞—é—â–∏–º –Ω–∞ –º–∏—Ç–∞–ø–∞—Ö —Å –¥–æ–∫–ª–∞–¥–∞–º–∏ , –∏ –ª—é–¥—è–º –Ω–µ –∏–∑ –º–∏—Ä–∞ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞ , —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞–∑–≤–µ–¥-–æ–ø—Ä–æ—Å–∞ –±—ã–ª —Å–∏–ª—å–Ω–æ –ø–æ—Ö–æ–∂ –Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ—Ç–≤–µ—Ç–æ–≤ –≤ twitter .\t–Ø –∑–Ω–∞–ª –æ—Ç–≤–µ—Ç , —ç—Ç–æ –∂–µ\t0\t0\t4\r\n",
      "0\t–ø–æ—Å–∫–æ–ª—å–∫—É –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–π —Å–∏–ª—å–Ω–æ –Ω–µ–¥–æ–æ—Ü–µ–Ω–µ–Ω—ã –∏ –∏—Ö –∞–∫—Ü–∏–∏ –∏–º–µ—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —Ä–æ—Å—Ç–∞ .\t–ß—Ç–æ –∫–∞—Å–∞–µ—Ç—Å—è –¥–æ–ª–ª–∞—Ä–æ–≤—ã—Ö –¥–µ–ø–æ–∑–∏—Ç–æ–≤\t0\t0\t5\r\n",
      "0\t\"\"\" –≤ 2013 –≥–æ–¥—É –†–æ—Å—Å–∏—è –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–ª–∞ –∏–∑ –ï–° –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –ø–∏—Ç–∞–Ω–∏—è –Ω–∞ —Å—É–º–º—É –≤ 43 –º–ª—Ä–¥ . –¥–æ–ª–ª–∞—Ä–æ–≤ , –ø—Ä–∏ —ç—Ç–æ–º –Ω–∞ —Ç–æ–≤–∞—Ä—ã , [ –ø–æ–∑–¥–Ω–µ–µ ] –≤–∫–ª—é—á–µ–Ω–Ω—ã–µ –≤ —ç–º–±–∞—Ä–≥–æ , –ø—Ä–∏—Ö–æ–¥–∏–ª–æ—Å—å –ø–æ—á—Ç–∏ 9 –º–ª—Ä–¥ . –¥–æ–ª–ª–∞—Ä–æ–≤ \"\" .\"\t–í –∞–≤–≥—É—Å—Ç–µ 2014 –≥–æ–¥–∞ –≥–ª–∞–≤–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞ –ï–≤—Ä–æ—Å–æ—é–∑–∞ –≤ –†–§ [ –í–∏–≥–∞—É–¥–∞—Å –£—à–∞—Ü–∫–∞—Å ( V —É gaudas Usackas ) ] –∑–∞—è–≤–∏–ª\t0\t0\t6\r\n",
      "1\t–í–µ–¥—å –¥–ª—è —Ç–æ–≥–æ —á—Ç–æ–±—ã –ø–æ–∫—Ä—ã–≤–∞—Ç—å –¥–µ—Ñ–∏—Ü–∏—Ç –ø–ª–∞—Ç–µ–∂–Ω–æ–≥–æ –±–∞–ª–∞–Ω—Å–∞ , –°–®–ê –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–æ–ª—É—á–∞—Ç—å –æ—Ç –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω—ã—Ö –∏–Ω–≤–µ—Å—Ç–æ—Ä–æ–≤ 1,4 –º–∏–ª–ª–∏–∞—Ä–¥–∞ –¥–æ–ª–ª–∞—Ä–æ–≤ –µ–∂–µ–¥–Ω–µ–≤–Ω–æ .\t\"–ï—Å–ª–∏ –∂–µ –¥–µ–Ω—å–≥–∏ –ø–æ–∫–∏–¥–∞—é—Ç –ê–º–µ—Ä–∏–∫—É , —ç—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç , —á—Ç–æ –¥–ª—è –∏–Ω–≤–µ—Å—Ç–æ—Ä–æ–≤ —Ä–∏—Å–∫ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π –≤ —ç–∫–æ–Ω–æ–º–∏–∫—É –°–®–ê –ø–µ—Ä–µ–≤–µ—à–∏–≤–∞–µ—Ç –æ–∂–∏–¥–∞–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ–π –ø—Ä–∏–±—ã–ª–∏ . –ò —ç—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç , –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –Ω–µ–æ–±—ã—á–∞–π–Ω–æ —Å–ª–∞–±—ã–π —Ä–æ—Å—Ç –µ–≤—Ä–æ–ø–µ–π—Å–∫–æ–π —ç–∫–æ–Ω–æ–º–∏–∫–∏ ( 1-2 –ø—Ä–æ—Ü–µ–Ω—Ç–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å—Ç—Ä–∞–Ω—ã ) . –û—á–µ–≤–∏–¥–Ω–æ , –¥–µ–π—Å—Ç–≤–∏—è–º–∏ –∏–Ω–≤–µ—Å—Ç–æ—Ä–æ–≤ —Ä—É–∫–æ–≤–æ–¥–∏—Ç –ª–æ–≥–∏–∫–∞ \"\" –ø—É—Å—Ç—å –ø–æ–º–µ–Ω—å—à–µ , –Ω–æ –∑–∞—Ç–æ –Ω–∞–¥–µ–∂–Ω–æ \"\"\"\t0\t0\t7\r\n",
      "0\t–ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –≤–µ–≥–∞–Ω–æ–≤ –∏ –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–∞ –∫–∞–∫ cruelt —É-free . IMG –ú–∞—Å–∫–∞ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–∞ –¥–ª—è –æ—á–∏—â–µ–Ω–∏—è –∏ –ø–∏—Ç–∞–Ω–∏—è –∫–æ–∂–∏ , –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–µ —á–µ—Ä–µ–∑ –¥–µ–Ω—å . –ù–∞ –º–æ–π –≤–∑–≥–ª—è–¥ , —ç—Ç–æ –¥–æ–≤–æ–ª—å–Ω–æ —á–∞—Å—Ç–æ –¥–ª—è –º–∞—Å–∫–∏ , —Ç–∞–∫ –∫–∞–∫ —ç—Ç–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—è —É—Ö–æ–¥–æ–≤–æ–π –∫–æ—Å–º–µ—Ç–∏–∫–∏ , –ø–æ –∏–¥–µ–µ , –¥–æ–ª–∂–Ω–∞ –¥–∞–≤–∞—Ç—å –±—ã—Å—Ç—Ä—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞ –æ–¥–Ω–æ –Ω–∞–Ω–µ—Å–µ–Ω–∏–µ . üòÑ –ü–æ–ø—Ä–æ–±–æ–≤–∞–≤ –¥–µ–π—Å—Ç–≤–∏–µ –º–∞—Å–∫–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –¥–Ω–∏ , —è\t–¥–ª—è —á–∏—Å—Ç–æ—Ç—ã\t0\t0\t8\r\n",
      "0\t–ù–æ –≤–æ—Ç –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ —Ä–∞–∑–¥—É–º—ã–≤–∞—Ç—å –æ–± —ç—Ç–æ–º - –∫–∞–∫ –Ω–∞—Å –º–æ–∂–µ—Ç —Å–∂–∏—Ä–∞—Ç—å —Å—Ç—Ä–∞—Ö , —Å—Ç–æ–∏—Ç –º–∏—Ä—É –∫–∞–∫-—Ç–æ –ø–æ—à–∞—Ç–Ω—É—Ç—å—Å—è .\t–Ø - —Å–æ–≤—Å–µ–º –Ω–µ —Å–ø–æ—Ä—Ç—Å–º–µ–Ω–∫–∞\t0\t0\t9\r\n"
     ]
    }
   ],
   "source": [
    "! head $TRAIN_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‚Äòmodels/structure_predictor_bimpm‚Äô: File exists\r\n"
     ]
    }
   ],
   "source": [
    "! mkdir models/structure_predictor_bimpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1074-1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1124-1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/structure_predictor_bimpm'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/structure_predictor_bimpm/config_elmo.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile $MODEL_PATH/config_elmo.json\n",
    "\n",
    "// Configuration for a sentence matching model based on:\n",
    "//   Wang, Zhiguo, Wael Hamza, and Radu Florian. \"Bilateral multi-perspective matching for natural language sentences.\"\n",
    "//   Proceedings of the 26th International Joint Conference on Artificial Intelligence. 2017.\n",
    "// (Augmented with additional granularity related features)\n",
    "\n",
    "local NUM_EPOCHS = 50;\n",
    "local LR = 1e-5;\n",
    "local MAX_LEN = 1000;\n",
    "local LSTM_ENCODER_HIDDEN = 25;\n",
    "\n",
    "{\n",
    "  \"dataset_reader\": {\n",
    "    \"type\": \"custom_pairs_reader\",\n",
    "    \"tokenizer\": {\n",
    "      \"type\": \"just_spaces\"\n",
    "    },\n",
    "#     \"token_indexers\": {\n",
    "#       \"tokens\": {\n",
    "#         \"type\": \"single_id\",\n",
    "#         \"lowercase_tokens\": true\n",
    "#       },\n",
    "#       \"elmo\": {\n",
    "#         \"type\": \"elmo_characters\"\n",
    "#       }\n",
    "#     }\n",
    "    \"token_indexers\": {\n",
    "      \"token_characters\": {\n",
    "        \"type\": \"characters\",\n",
    "        \"min_padding_length\": 3\n",
    "      },\n",
    "      \"elmo\": {\n",
    "        \"type\": \"elmo_characters\",\n",
    "      },\n",
    "    }\n",
    "  },\n",
    "  \"train_data_path\": \"structure_predictor_bimpm/structure_cf_train.tsv\",\n",
    "  \"validation_data_path\": \"structure_predictor_bimpm/structure_cf_dev.tsv\",\n",
    "  \"model\": {\n",
    "    \"type\": \"custom_bimpm\",\n",
    "    \"dropout\": 0.5,\n",
    "    \"class_weights\": [0.5, 1.0],\n",
    "    \"encode_together\": false,\n",
    "    \"text_field_embedder\": {\n",
    "        \"token_embedders\": {\n",
    "            \"elmo\": {\n",
    "                    \"type\": \"elmo_token_embedder\",\n",
    "                    \"options_file\": \"rsv_elmo/options.json\",\n",
    "                    \"weight_file\": \"rsv_elmo/model.hdf5\",\n",
    "                    \"do_layer_norm\": true,\n",
    "                    \"dropout\": 0.2\n",
    "            },\n",
    "            \"token_characters\": {\n",
    "                \"type\": \"character_encoding\",\n",
    "                \"dropout\": 0.2,\n",
    "                \"embedding\": {\n",
    "                    \"embedding_dim\": 20,\n",
    "                    \"sparse\": false,\n",
    "                    \"vocab_namespace\": \"token_characters\"\n",
    "                },\n",
    "                \"encoder\": {\n",
    "                    \"type\": \"lstm\",\n",
    "                    \"input_size\": $.model.text_field_embedder.token_embedders.token_characters.embedding.embedding_dim,\n",
    "                    \"hidden_size\": LSTM_ENCODER_HIDDEN,\n",
    "                    \"num_layers\": 1,\n",
    "                    \"bidirectional\": true,\n",
    "                    \"dropout\": 0.4\n",
    "              },\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"matcher_word\": {\n",
    "      \"is_forward\": true,\n",
    "      \"hidden_dim\": 1024+LSTM_ENCODER_HIDDEN+LSTM_ENCODER_HIDDEN,\n",
    "      \"num_perspectives\": 10,\n",
    "      \"with_full_match\": false\n",
    "    },\n",
    "    \"encoder1\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": 1024+LSTM_ENCODER_HIDDEN+LSTM_ENCODER_HIDDEN,\n",
    "      \"hidden_size\": 200,\n",
    "      \"num_layers\": 1\n",
    "    },\n",
    "    \"matcher_forward1\": {\n",
    "      \"is_forward\": true,\n",
    "      \"hidden_dim\": 200,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"matcher_backward1\": {\n",
    "      \"is_forward\": false,\n",
    "      \"hidden_dim\": 200,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"encoder2\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": $.model.matcher_forward1.hidden_dim+$.model.matcher_backward1.hidden_dim,\n",
    "      \"hidden_size\": 200,\n",
    "      \"num_layers\": 1\n",
    "    },\n",
    "    \"matcher_forward2\": {\n",
    "      \"is_forward\": true,\n",
    "      \"hidden_dim\": 200,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"matcher_backward2\": {\n",
    "      \"is_forward\": false,\n",
    "      \"hidden_dim\": 200,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"aggregator\":{\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": 264,\n",
    "      \"hidden_size\": 100,\n",
    "      \"num_layers\": 1,\n",
    "      \"dropout\": 0.1,\n",
    "    },\n",
    "    \"classifier_feedforward\": {\n",
    "      \"input_dim\": $.model.matcher_forward2.hidden_dim+$.model.matcher_backward2.hidden_dim+1+1,\n",
    "      \"num_layers\": 2,\n",
    "      \"hidden_dims\": [200, 2],\n",
    "      \"activations\": [\"mish\", \"mish\"],\n",
    "      \"dropout\": [0.5, 0.0]\n",
    "    },\n",
    "    \"initializer\": {\n",
    "      \"regexes\": [\n",
    "        [\".*linear_layers.*weight\", {\"type\": \"xavier_normal\"}],\n",
    "        [\".*linear_layers.*bias\", {\"type\": \"constant\", \"val\": 0}],\n",
    "        [\".*weight_ih.*\", {\"type\": \"xavier_normal\"}],\n",
    "        [\".*weight_hh.*\", {\"type\": \"orthogonal\"}],\n",
    "        [\".*bias.*\", {\"type\": \"constant\", \"val\": 0}],\n",
    "        [\".*matcher.*match_weights.*\", {\"type\": \"kaiming_normal\"}]\n",
    "      ]\n",
    "    }\n",
    "  },\n",
    "  \"data_loader\": {\n",
    "    \"batch_sampler\": {\n",
    "        \"type\": \"bucket\",\n",
    "        \"batch_size\": 20,\n",
    "        \"padding_noise\": 0.0,\n",
    "        \"sorting_keys\": [\"premise\"],\n",
    "    },\n",
    "  },\n",
    "  \"trainer\": {\n",
    "    \"num_epochs\": NUM_EPOCHS,\n",
    "    \"patience\": 5,\n",
    "    \"grad_clipping\": 5.0,\n",
    "    \"validation_metric\": \"+f1\",\n",
    "    \"cuda_device\": 0,\n",
    "    \"optimizer\": {\n",
    "      \"type\": \"adam\",\n",
    "      \"lr\": LR\n",
    "    },\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! mv ../../../maintenance_rst/models/structure_predictor_bimpm ../../../maintenance_rst/models/structure_predictor_bimpm_OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp -r models/structure_predictor_bimpm ../../../maintenance_rst/models/structure_predictor_bimpm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Scripts for training/prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1. Directly from the config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/train_structure_predictor.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/train_structure_predictor.sh\n",
    "# usage:\n",
    "# $ cd models \n",
    "# $ sh train_structure_predictor.sh {bert|elmo} result_directory\n",
    "\n",
    "export METHOD=${1}\n",
    "export RESULT_DIR=${2}\n",
    "export DEV_FILE_PATH=\"structure_cf_dev.tsv\"\n",
    "export TEST_FILE_PATH=\"structure_cf_test.tsv\"\n",
    "\n",
    "rm -r structure_predictor_bimpm/${RESULT_DIR}/\n",
    "allennlp train -s structure_predictor_bimpm/${RESULT_DIR}/ structure_predictor_bimpm/config_${METHOD}.json \\\n",
    "   --include-package bimpm_custom_package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on dev&test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/eval_structure_predictor.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/eval_structure_predictor.sh\n",
    "# usage:\n",
    "# $ cd models \n",
    "# $ sh eval_structure_predictor.sh {bert|elmo} result_directory\n",
    "\n",
    "export METHOD=${1}\n",
    "export RESULT_DIR=${2}\n",
    "export DEV_FILE_PATH=\"structure_cf_dev.tsv\"\n",
    "export TEST_FILE_PATH=\"structure_cf_test.tsv\"\n",
    "\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "    --output-file structure_predictor_bimpm/${RESULT_DIR}/predictions_dev.json \\\n",
    "    structure_predictor_bimpm/${RESULT_DIR}/model.tar.gz structure_predictor_bimpm/${DEV_FILE_PATH} \\\n",
    "    --include-package bimpm_custom_package \\\n",
    "    --predictor custom_bimpm_predictor\n",
    "\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "    --output-file structure_predictor_bimpm/${RESULT_DIR}/predictions_test.json \\\n",
    "    structure_predictor_bimpm/${RESULT_DIR}/model.tar.gz structure_predictor_bimpm/${TEST_FILE_PATH} \\\n",
    "    --include-package bimpm_custom_package \\\n",
    "    --predictor custom_bimpm_predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2. Using wandb for parameters adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/wandb_structure_predictor.yaml\n",
    "# usage:\n",
    "# $ cd models\n",
    "# wandb sweep wandb_structure_predictor.yaml\n",
    "\n",
    "name: structure_predictor_stacked\n",
    "program: wandb_allennlp # this is a wrapper console script around allennlp commands. It is part of wandb-allennlp\n",
    "method: bayes\n",
    "## Do not for get to use the command keyword to specify the following command structure\n",
    "command:\n",
    "  - ${program} #omit the interpreter as we use allennlp train command directly\n",
    "  - \"--subcommand=train\"\n",
    "  - \"--include-package=bimpm_custom_package\" # add all packages containing your registered classes here\n",
    "  - \"--config_file=structure_predictor_bimpm/config_elmo.json\"\n",
    "  - ${args}\n",
    "metric:\n",
    "    name: best_f1\n",
    "    goal: maximize\n",
    "parameters:\n",
    "    model.type:\n",
    "        values: [\"custom_bimpm\",]\n",
    "    iterator.batch_size:\n",
    "        values: [20,]\n",
    "    model.encode_together:\n",
    "        values: [\"false\",]\n",
    "    trainer.optimizer.lr:\n",
    "        values: [0.001,]\n",
    "    model.dropout:\n",
    "        values: [0.5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Run training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``wandb sweep wandb_structure_predictor.yaml``\n",
    "\n",
    "(returns %sweepname)\n",
    "\n",
    "``wandb agent --count 1 %sweepname``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the best model in structure_predictor_bimpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp -r models/wandb/run-20200720_203050-84hl3zwy/training_dumps models/structure_predictor_bimpm/snowy-sweep-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mv models/wandb/run-20200929_034343-5tmisocu models/structure_predictor_bimpm/colorful-sweep-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Evaluate classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictions(path):\n",
    "    result = []\n",
    "    \n",
    "    with open(path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            result.append(json.loads(line)[\"label\"])\n",
    "            \n",
    "    result = list(map(int, result))\n",
    "    print('length of result:', len(result))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp ../../../maintenance_rst/models/structure_predictor_bimpm/colorful-sweep-1-dumps/*.json models/structure_predictor_bimpm/colorful-sweep-1-dumps/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_DIR = 'colorful-sweep-1-dumps'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "true = pd.read_csv(DEV_FILE_PATH, sep='\\t', header=None)[0].values.tolist()\n",
    "pred = load_predictions(f'{MODEL_PATH}/{RESULT_DIR}/predictions_dev.json')\n",
    "print('length of true labels:', len(true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "\n",
    "print('f1: %.2f'%(f1_score(true[:len(pred)], pred)*100))\n",
    "print('pr: %.2f'%(precision_score(true[:len(pred)], pred)*100))\n",
    "print('re: %.2f'%(recall_score(true[:len(pred)], pred)*100))\n",
    "\n",
    "print(classification_report(true[:len(pred)], pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "true = pd.read_csv(TEST_FILE_PATH, sep='\\t', header=None)[0].values.tolist()\n",
    "pred = load_predictions(f'{MODEL_PATH}/{RESULT_DIR}/predictions_test.json')\n",
    "print('length of true labels:', len(true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('f1: %.2f'%(f1_score(true[:len(pred)], pred)*100))\n",
    "print('pr: %.2f'%(precision_score(true[:len(pred)], pred)*100))\n",
    "print('re: %.2f'%(recall_score(true[:len(pred)], pred)*100))\n",
    "\n",
    "print(classification_report(true[:len(pred)], pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vocab = [0, 1]\n",
    "catboost_vocab = [0, 1]\n",
    "\n",
    "def load_neural_predictions(path):\n",
    "    result = []\n",
    "    \n",
    "    with open(path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            probs = json.loads(line)['probs']\n",
    "            probs = {model_vocab[i]: probs[i] for i in range(len(model_vocab))}\n",
    "            result.append(probs)\n",
    "            \n",
    "    return result\n",
    "\n",
    "def load_scikit_predictions(model, X):\n",
    "    result = []\n",
    "    \n",
    "    try:\n",
    "        predictions = model.predict_proba(X)\n",
    "    except AttributeError:\n",
    "        predictions = model._predict_proba_lr(X)\n",
    "    \n",
    "    for prediction in predictions:\n",
    "        probs = {catboost_vocab[j]: prediction[j] for j in range(len(catboost_vocab))}\n",
    "        result.append(probs)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def vote_predictions(pred1, pred2, soft=True):\n",
    "    assert len(pred1) == len(pred2)\n",
    "    result = []\n",
    "    \n",
    "    for i in range(len(pred1)):\n",
    "        sample_result = {}\n",
    "        for key in pred1[i].keys():\n",
    "            if soft:\n",
    "                sample_result[key] = pred1[i][key] + pred2[i][key]\n",
    "            else:\n",
    "                sample_result[key] = max(pred1[i][key], pred2[i][key])\n",
    "        \n",
    "        result.append(sample_result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def probs_to_classes(pred):\n",
    "    result = []\n",
    "    \n",
    "    for sample in pred:\n",
    "        best_class = ''\n",
    "        best_prob = 0.\n",
    "        for key in sample.keys():\n",
    "            if sample[key] > best_prob:\n",
    "                best_prob = sample[key]\n",
    "                best_class = key\n",
    "        \n",
    "        result.append(best_class)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model = pickle.load(open('models/structure_predictor_baseline/model.pkl', 'rb'))\n",
    "scaler = pickle.load(open('models/structure_predictor_baseline/scaler.pkl', 'rb'))\n",
    "drop_columns = pickle.load(open('models/structure_predictor_baseline/drop_columns.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_PATH = 'data_structure'\n",
    "\n",
    "train_samples = pd.read_pickle(os.path.join(IN_PATH, 'train_samples.pkl'))\n",
    "dev_samples = pd.read_pickle(os.path.join(IN_PATH, 'dev_samples.pkl'))\n",
    "test_samples = pd.read_pickle(os.path.join(IN_PATH, 'test_samples.pkl'))\n",
    "\n",
    "y_train, X_train = train_samples['relation'].to_frame(), train_samples.drop('relation', axis=1).drop(\n",
    "    columns=drop_columns + ['category_id'])\n",
    "y_dev, X_dev = dev_samples['relation'].to_frame(), dev_samples.drop('relation', axis=1).drop(\n",
    "    columns=drop_columns + ['category_id'])\n",
    "y_test, X_test = test_samples['relation'].to_frame(), test_samples.drop('relation', axis=1).drop(\n",
    "    columns=drop_columns + ['category_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_np = scaler.transform(X_dev)\n",
    "X_dev = pd.DataFrame(X_scaled_np, index=X_dev.index)\n",
    "\n",
    "X_scaled_np = scaler.transform(X_test)\n",
    "X_test = pd.DataFrame(X_scaled_np, index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "TARGET = 'relation'\n",
    "svm_predictions = load_scikit_predictions(model, X_dev)\n",
    "neural_predictions = load_neural_predictions(f'{MODEL_PATH}/{RESULT_DIR}/predictions_dev.json')\n",
    "\n",
    "tmp = vote_predictions(neural_predictions, svm_predictions, soft=True)\n",
    "ensemble_pred = probs_to_classes(tmp)\n",
    "\n",
    "print('f1: %.2f'%(metrics.f1_score(y_dev, ensemble_pred)*100.))\n",
    "print('pr: %.2f'%(metrics.precision_score(y_dev, ensemble_pred)*100.))\n",
    "print('re: %.2f'%(metrics.recall_score(y_dev, ensemble_pred)*100.))\n",
    "print()\n",
    "print(metrics.classification_report(y_dev, ensemble_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_predictions = load_scikit_predictions(model, X_test)\n",
    "neural_predictions = load_neural_predictions(f'{MODEL_PATH}/{RESULT_DIR}/predictions_test.json')\n",
    "\n",
    "tmp = vote_predictions(neural_predictions, svm_predictions, soft=True)\n",
    "ensemble_pred = probs_to_classes(tmp)\n",
    "\n",
    "print('f1: %.2f'%(metrics.f1_score(y_test, ensemble_pred)*100.))\n",
    "print('pr: %.2f'%(metrics.precision_score(y_test, ensemble_pred)*100.))\n",
    "print('re: %.2f'%(metrics.recall_score(y_test, ensemble_pred)*100.))\n",
    "print()\n",
    "print(metrics.classification_report(y_test, ensemble_pred, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
