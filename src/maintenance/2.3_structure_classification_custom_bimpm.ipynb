{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary structure classification used in tree building: Step 3. BiMPM\n",
    "\n",
    "Prepare data and model-related scripts.\n",
    "\n",
    "Evaluate models.\n",
    "\n",
    "Output:\n",
    " - ``models/structure_predictor_bimpm/*``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from utils.file_reading import read_edus, read_gold, read_negative, read_annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘models/structure_predictor_bimpm’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = 'models/structure_predictor_bimpm'\n",
    "! mkdir $MODEL_PATH\n",
    "\n",
    "TRAIN_FILE_PATH = os.path.join(MODEL_PATH, 'structure_cf_train.tsv')\n",
    "DEV_FILE_PATH = os.path.join(MODEL_PATH, 'structure_cf_dev.tsv')\n",
    "TEST_FILE_PATH = os.path.join(MODEL_PATH, 'structure_cf_test.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_PATH = 'data_structure'\n",
    "\n",
    "train_samples = pd.read_pickle(os.path.join(IN_PATH, 'train_samples.pkl'))\n",
    "dev_samples = pd.read_pickle(os.path.join(IN_PATH, 'dev_samples.pkl'))\n",
    "test_samples = pd.read_pickle(os.path.join(IN_PATH, 'test_samples.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of classes: 2\n",
      "class weights: [0.396912 1.      ]\n"
     ]
    }
   ],
   "source": [
    "counts = train_samples['relation'].value_counts(normalize=False).values\n",
    "NUMBER_CLASSES = len(counts)\n",
    "print(\"number of classes:\", NUMBER_CLASSES)\n",
    "print(\"class weights:\", np.round(counts.min() / counts, decimals=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = train_samples.reset_index()\n",
    "train_samples[['relation', 'snippet_x', 'snippet_y', 'same_sentence', 'same_paragraph', 'index']].to_csv(\n",
    "    TRAIN_FILE_PATH, sep='\\t', header=False, index=False)\n",
    "\n",
    "dev_samples = dev_samples.reset_index()\n",
    "dev_samples[['relation', 'snippet_x', 'snippet_y', 'same_sentence', 'same_paragraph', 'index']].to_csv(\n",
    "    DEV_FILE_PATH, sep='\\t', header=False, index=False)\n",
    "\n",
    "test_samples = test_samples.reset_index()\n",
    "test_samples[['relation', 'snippet_x', 'snippet_y', 'same_sentence', 'same_paragraph', 'index']].to_csv(\n",
    "    TEST_FILE_PATH, sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize BiMPM model with adding inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! rm -r models/bimpm_custom_package\n",
    "! mkdir models/bimpm_custom_package\n",
    "! touch models/bimpm_custom_package/__init__.py\n",
    "! mkdir models/bimpm_custom_package/tokenizers\n",
    "! mkdir models/bimpm_custom_package/dataset_readers\n",
    "! mkdir models/bimpm_custom_package/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile models/bimpm_custom_package/dataset_readers/__init__.py\n",
    "\n",
    "try:\n",
    "    from bimpm_custom_package.tokenizers.whitespace_tokenizer import WhitespaceTokenizer\n",
    "    from bimpm_custom_package.dataset_readers.custom_reader import CustomDataReader\n",
    "except ModuleNotFoundError:\n",
    "    from models.bimpm_custom_package.tokenizers.whitespace_tokenizer import WhitespaceTokenizer\n",
    "    from models.bimpm_custom_package.dataset_readers.custom_reader import CustomDataReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/bimpm_custom_package/tokenizers/whitespace_tokenizer.py\n",
    "\n",
    "from allennlp.data.tokenizers import Tokenizer\n",
    "from allennlp.data.tokenizers import Token, Tokenizer, CharacterTokenizer, WordTokenizer\n",
    "from overrides import overrides\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "@Tokenizer.register(\"whitespace_tokenizer\")\n",
    "class WhitespaceTokenizer(Tokenizer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        return [Token(token) for token in text.split()]\n",
    "\n",
    "    @overrides\n",
    "    def tokenize(self, text: str) -> List[Token]:\n",
    "        tokens = self._tokenize(text)\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/bimpm_custom_package/dataset_readers/custom_reader.py\n",
    "\n",
    "import csv\n",
    "import logging\n",
    "from typing import Dict\n",
    "\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "from allennlp.data.fields import LabelField, TextField, Field, ArrayField\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Tokenizer\n",
    "from overrides import overrides\n",
    "\n",
    "try:\n",
    "    from bimpm_custom_package.tokenizers.whitespace_tokenizer import WhitespaceTokenizer\n",
    "except ModuleNotFoundError:\n",
    "    from models.bimpm_custom_package.tokenizers.whitespace_tokenizer import WhitespaceTokenizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@DatasetReader.register(\"custom_pairs_reader\")\n",
    "class CustomDataReader(DatasetReader):\n",
    "    \"\"\"\n",
    "    # Parameters\n",
    "    tokenizer : `Tokenizer`, optional\n",
    "        Tokenizer to use to split the premise and hypothesis into words or other kinds of tokens.\n",
    "        Defaults to `WhitespaceTokenizer`.\n",
    "    token_indexers : `Dict[str, TokenIndexer]`, optional\n",
    "        Indexers used to define input token representations. Defaults to `{\"tokens\":\n",
    "        SingleIdTokenIndexer()}`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, tokenizer: Tokenizer = None, token_indexers: Dict[str, TokenIndexer] = None,\n",
    "            lazy: bool = True) -> None:\n",
    "        super().__init__(lazy)\n",
    "        self._tokenizer = tokenizer or WhitespaceTokenizer()\n",
    "        self._token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "\n",
    "    @overrides\n",
    "    def _read(self, file_path):\n",
    "        logger.info(\"Reading instances from lines in file at: %s\", file_path)\n",
    "        with open(cached_path(file_path), \"r\") as data_file:\n",
    "            tsv_in = csv.reader(data_file, delimiter=\"\\t\")\n",
    "            for row in tsv_in:\n",
    "                if len(row) == 6:\n",
    "                    yield self.text_to_instance(premise=row[1], hypothesis=row[2], label=row[0], \n",
    "                                                same_sentence=row[3], same_paragraph=row[4])\n",
    "\n",
    "    @overrides\n",
    "    def text_to_instance(\n",
    "            self,  # type: ignore\n",
    "            premise: str,\n",
    "            hypothesis: str,\n",
    "            label: str,\n",
    "            same_sentence: str,\n",
    "            same_paragraph: str,\n",
    "    ) -> Instance:\n",
    "\n",
    "        fields: Dict[str, Field] = {}\n",
    "        tokenized_premise = self._tokenizer.tokenize(premise)\n",
    "        tokenized_hypothesis = self._tokenizer.tokenize(hypothesis)\n",
    "        fields[\"premise\"] = TextField(tokenized_premise, self._token_indexers)\n",
    "        fields[\"hypothesis\"] = TextField(tokenized_hypothesis, self._token_indexers)\n",
    "        _same_sentence = list(map(list, zip(*same_sentence)))\n",
    "        _same_paragraph = list(map(list, zip(*same_paragraph)))\n",
    "        #additional_features = list(map(list, zip(*same_sentence)))\n",
    "        fields[\"same_sentence\"] = ArrayField(np.array(_same_sentence))\n",
    "        fields[\"same_paragraph\"] = ArrayField(np.array(_same_paragraph))\n",
    "        if label is not None:\n",
    "            fields[\"label\"] = LabelField(label)\n",
    "\n",
    "        return Instance(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/bimpm_custom_package/model/__init__.py\n",
    "\n",
    "try:\n",
    "    from bimpm_custom_package.tokenizers.whitespace_tokenizer import WhitespaceTokenizer\n",
    "    from bimpm_custom_package.model.custom_bimpm import BiMpm as CustomBiMpm\n",
    "    from bimpm_custom_package.model.multiclass_bimpm import BiMpm as MulticlassBiMpm\n",
    "    from bimpm_custom_package.model.custom_bimpm_predictor import CustomBiMPMPredictor\n",
    "except ModuleNotFoundError:\n",
    "    from models.bimpm_custom_package.tokenizers.whitespace_tokenizer import WhitespaceTokenizer\n",
    "    from models.bimpm_custom_package.model.custom_bimpm import BiMpm as CustomBiMpm\n",
    "    from models.bimpm_custom_package.model.multiclass_bimpm import BiMpm as MulticlassBiMpm\n",
    "    from models.bimpm_custom_package.model.custom_bimpm_predictor import CustomBiMPMPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/bimpm_custom_package/model/custom_bimpm.py\n",
    "\n",
    "\"\"\"\n",
    "BiMPM (Bilateral Multi-Perspective Matching) model implementation.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, Optional, List, Any\n",
    "\n",
    "from overrides import overrides\n",
    "import torch\n",
    "import numpy\n",
    "\n",
    "from allennlp.common.checks import check_dimensions_match\n",
    "from allennlp.data import Vocabulary\n",
    "from allennlp.modules import FeedForward, Seq2SeqEncoder, Seq2VecEncoder, TextFieldEmbedder\n",
    "from allennlp.models.model import Model\n",
    "from allennlp.nn import InitializerApplicator, RegularizerApplicator\n",
    "from allennlp.nn import util\n",
    "from allennlp.training.metrics import CategoricalAccuracy, F1Measure\n",
    "\n",
    "from allennlp.modules.bimpm_matching import BiMpmMatching\n",
    "\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "@Model.register(\"custom_bimpm\")\n",
    "class BiMpm(Model):\n",
    "    \"\"\"\n",
    "    This ``Model`` augments with additional features the BiMPM model described in `Bilateral Multi-Perspective \n",
    "    Matching for Natural Language Sentences <https://arxiv.org/abs/1702.03814>`_ by Zhiguo Wang et al., 2017.\n",
    "    implemented in https://github.com/galsang/BIMPM-pytorch>`_.\n",
    "    Additional features are added before the feedforward classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab: Vocabulary,\n",
    "                 text_field_embedder: TextFieldEmbedder,\n",
    "                 matcher_word: BiMpmMatching,\n",
    "                 encoder1: Seq2SeqEncoder,\n",
    "                 matcher_forward1: BiMpmMatching,\n",
    "                 matcher_backward1: BiMpmMatching,\n",
    "                 encoder2: Seq2SeqEncoder,\n",
    "                 matcher_forward2: BiMpmMatching,\n",
    "                 matcher_backward2: BiMpmMatching,\n",
    "                 aggregator: Seq2VecEncoder,\n",
    "                 classifier_feedforward: FeedForward,\n",
    "                 encode_together: bool = False,\n",
    "                 encode_lstm: bool = True,\n",
    "                 dropout: float = 0.1,\n",
    "                 class_weights: list = [],\n",
    "                 initializer: InitializerApplicator = InitializerApplicator(),\n",
    "                 regularizer: Optional[RegularizerApplicator] = None) -> None:\n",
    "        super(BiMpm, self).__init__(vocab, regularizer)\n",
    "\n",
    "        self.text_field_embedder = text_field_embedder\n",
    "\n",
    "        self.matcher_word = matcher_word\n",
    "\n",
    "        self.encoder1 = encoder1\n",
    "        self.matcher_forward1 = matcher_forward1\n",
    "        self.matcher_backward1 = matcher_backward1\n",
    "\n",
    "        self.encoder2 = encoder2\n",
    "        self.matcher_forward2 = matcher_forward2\n",
    "        self.matcher_backward2 = matcher_backward2\n",
    "\n",
    "        self.aggregator = aggregator\n",
    "\n",
    "        self.encode_together = encode_together\n",
    "        self.encode_lstm = encode_lstm\n",
    "\n",
    "        matching_dim = self.matcher_word.get_output_dim()\n",
    "\n",
    "        if self.encode_lstm:\n",
    "            matching_dim += self.matcher_forward1.get_output_dim(\n",
    "            ) + self.matcher_backward1.get_output_dim(\n",
    "            ) + self.matcher_forward2.get_output_dim(\n",
    "            ) + self.matcher_backward2.get_output_dim(\n",
    "            )\n",
    "\n",
    "        check_dimensions_match(matching_dim, self.aggregator.get_input_dim(),\n",
    "                               \"sum of dim of all matching layers\", \"aggregator input dim\")\n",
    "\n",
    "        self.classifier_feedforward = classifier_feedforward\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        if class_weights:\n",
    "            self.class_weights = class_weights\n",
    "        else:\n",
    "            self.class_weights = [1.] * self.classifier_feedforward.get_output_dim()\n",
    "\n",
    "        self.metrics = {\"accuracy\": CategoricalAccuracy(),\n",
    "                        \"f1\": F1Measure(1)}\n",
    "\n",
    "        self.loss = torch.nn.CrossEntropyLoss(weight=torch.FloatTensor(self.class_weights))\n",
    "\n",
    "        initializer(self)\n",
    "\n",
    "    @overrides\n",
    "    def forward(self,  # type: ignore\n",
    "                premise: Dict[str, torch.LongTensor],\n",
    "                hypothesis: Dict[str, torch.LongTensor],\n",
    "                same_sentence: List[Dict[str, torch.FloatTensor]],\n",
    "                same_paragraph: List[Dict[str, torch.FloatTensor]],\n",
    "                #metadata: List[Dict[str, torch.FloatTensor]],\n",
    "                label: torch.LongTensor = None,  # pylint:disable=unused-argument\n",
    "                ) -> Dict[str, torch.Tensor]:\n",
    "        # pylint: disable=arguments-differ\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        premise : Dict[str, torch.LongTensor]\n",
    "            The premise from a ``TextField``\n",
    "        hypothesis : Dict[str, torch.LongTensor]\n",
    "            The hypothesis from a ``TextField``\n",
    "        label : torch.LongTensor, optional (default = None)\n",
    "            The label for the pair of the premise and the hypothesis\n",
    "        metadata : ``List[Dict[str, Any]]``, optional, (default = None)\n",
    "            Additional information about the pair\n",
    "        Returns\n",
    "        -------\n",
    "        An output dictionary consisting of:\n",
    "        logits : torch.FloatTensor\n",
    "            A tensor of shape ``(batch_size, num_labels)`` representing unnormalised log\n",
    "            probabilities of the entailment label.\n",
    "        loss : torch.FloatTensor, optional\n",
    "            A scalar loss to be optimised.\n",
    "        \"\"\"\n",
    "\n",
    "        def encode_pair(x1, x2, mask1=None, mask2=None):\n",
    "            _joined_pair: Dict[str, torch.LongTensor] = {}\n",
    "\n",
    "            for key in premise.keys():\n",
    "                bsz = premise[key].size(0)\n",
    "                x1_len, x2_len = premise[key].size(1), hypothesis[key].size(1)\n",
    "                sep = torch.empty([bsz, 1], dtype=torch.long, device=premise[key].device)\n",
    "                sep.data.fill_(0)  # 2 is the id for </s>\n",
    "\n",
    "                x = torch.cat([premise[key], hypothesis[key]], dim=1)\n",
    "                _joined_pair[key] = x\n",
    "\n",
    "            x_output = self.dropout(self.text_field_embedder(_joined_pair))\n",
    "            return x_output[:, :x1_len], x_output[:, -x2_len:], mask1, mask2\n",
    "\n",
    "        mask_premise = util.get_text_field_mask(premise)\n",
    "        mask_hypothesis = util.get_text_field_mask(hypothesis)\n",
    "\n",
    "        if self.encode_together:\n",
    "            embedded_premise, embedded_hypothesis, _, _ = encode_pair(premise, hypothesis)\n",
    "        else:\n",
    "            embedded_premise = self.dropout(self.text_field_embedder(premise))\n",
    "            embedded_hypothesis = self.dropout(self.text_field_embedder(hypothesis))\n",
    "\n",
    "        # embedding and encoding of the premise\n",
    "        encoded_premise1 = self.dropout(self.encoder1(embedded_premise, mask_premise))\n",
    "        encoded_premise2 = self.dropout(self.encoder2(encoded_premise1, mask_premise))\n",
    "\n",
    "        # embedding and encoding of the hypothesis\n",
    "        encoded_hypothesis1 = self.dropout(self.encoder1(embedded_hypothesis, mask_hypothesis))\n",
    "        encoded_hypothesis2 = self.dropout(self.encoder2(encoded_hypothesis1, mask_hypothesis))\n",
    "\n",
    "        matching_vector_premise: List[torch.Tensor] = []\n",
    "        matching_vector_hypothesis: List[torch.Tensor] = []\n",
    "\n",
    "        def add_matching_result(matcher, encoded_premise, encoded_hypothesis):\n",
    "            # utility function to get matching result and add to the result list\n",
    "            matching_result = matcher(encoded_premise, mask_premise, encoded_hypothesis, mask_hypothesis)\n",
    "            matching_vector_premise.extend(matching_result[0])\n",
    "            matching_vector_hypothesis.extend(matching_result[1])\n",
    "\n",
    "        # calculate matching vectors from word embedding, first layer encoding, and second layer encoding\n",
    "        add_matching_result(self.matcher_word, embedded_premise, embedded_hypothesis)\n",
    "        half_hidden_size_1 = self.encoder1.get_output_dim() // 2\n",
    "        add_matching_result(self.matcher_forward1,\n",
    "                            encoded_premise1[:, :, :half_hidden_size_1],\n",
    "                            encoded_hypothesis1[:, :, :half_hidden_size_1])\n",
    "        add_matching_result(self.matcher_backward1,\n",
    "                            encoded_premise1[:, :, half_hidden_size_1:],\n",
    "                            encoded_hypothesis1[:, :, half_hidden_size_1:])\n",
    "\n",
    "        half_hidden_size_2 = self.encoder2.get_output_dim() // 2\n",
    "        add_matching_result(self.matcher_forward2,\n",
    "                            encoded_premise2[:, :, :half_hidden_size_2],\n",
    "                            encoded_hypothesis2[:, :, :half_hidden_size_2])\n",
    "        add_matching_result(self.matcher_backward2,\n",
    "                            encoded_premise2[:, :, half_hidden_size_2:],\n",
    "                            encoded_hypothesis2[:, :, half_hidden_size_2:])\n",
    "\n",
    "        # concat the matching vectors\n",
    "        matching_vector_cat_premise = self.dropout(torch.cat(matching_vector_premise, dim=2))\n",
    "        matching_vector_cat_hypothesis = self.dropout(torch.cat(matching_vector_hypothesis, dim=2))\n",
    "\n",
    "        # aggregate the matching vectors\n",
    "        aggregated_premise = self.dropout(self.aggregator(matching_vector_cat_premise, mask_premise))\n",
    "        aggregated_hypothesis = self.dropout(self.aggregator(matching_vector_cat_hypothesis, mask_hypothesis))\n",
    "\n",
    "        # encode additional information\n",
    "        batch_size, _ = aggregated_premise.size()\n",
    "        encoded_same_sentence = same_sentence.float().view(batch_size, -1)\n",
    "        encoded_same_paragraph = same_paragraph.float().view(batch_size, -1)\n",
    "\n",
    "        # the final forward layer\n",
    "        logits = self.classifier_feedforward(\n",
    "            torch.cat([aggregated_premise, \n",
    "                       aggregated_hypothesis, \n",
    "                       encoded_same_sentence, \n",
    "                       encoded_same_paragraph], dim=-1))\n",
    "        \n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        output_dict = {'logits': logits, \"probs\": probs}\n",
    "\n",
    "        if label is not None:\n",
    "            loss = self.loss(logits, label)\n",
    "            for metric in self.metrics.values():\n",
    "                metric(logits, label)\n",
    "            output_dict[\"loss\"] = loss\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "    @overrides\n",
    "    def decode(self, output_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Converts indices to string labels, and adds a ``\"label\"`` key to the result.\n",
    "        \"\"\"\n",
    "        predictions = output_dict[\"probs\"].cpu().data.numpy()\n",
    "        argmax_indices = numpy.argmax(predictions, axis=-1)\n",
    "        labels = [self.vocab.get_token_from_index(x, namespace=\"labels\")\n",
    "                  for x in argmax_indices]\n",
    "        output_dict['label'] = labels\n",
    "        return output_dict\n",
    "\n",
    "    @overrides\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\n",
    "            \"f1\": self.metrics[\"f1\"].get_metric(reset=reset)[2],\n",
    "            \"accuracy\": self.metrics[\"accuracy\"].get_metric(reset=reset)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/bimpm_custom_package/model/custom_bimpm_predictor.py\n",
    "\n",
    "from allennlp.common import JsonDict\n",
    "from allennlp.data import DatasetReader, Instance\n",
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "from allennlp.models import Model\n",
    "from allennlp.predictors import Predictor\n",
    "from allennlp.predictors.decomposable_attention import DecomposableAttentionPredictor\n",
    "from overrides import overrides\n",
    "\n",
    "from allennlp.data.tokenizers import Tokenizer\n",
    "from allennlp.data.tokenizers import Token, Tokenizer, CharacterTokenizer, WordTokenizer\n",
    "from overrides import overrides\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "try:\n",
    "    from bimpm_custom_package.tokenizers.whitespace_tokenizer import WhitespaceTokenizer\n",
    "except ModuleNotFoundError:\n",
    "    from models.bimpm_custom_package.tokenizers.whitespace_tokenizer import WhitespaceTokenizer\n",
    "\n",
    "# You need to name your predictor and register so that `allennlp` command can recognize it\n",
    "# Note that you need to use \"@Predictor.register\", not \"@Model.register\"!\n",
    "@Predictor.register(\"custom_bimpm_predictor\")\n",
    "class CustomBiMPMPredictor(DecomposableAttentionPredictor):\n",
    "    def __init__(self, model: Model, dataset_reader: DatasetReader) -> None:\n",
    "        super().__init__(model, dataset_reader)\n",
    "        self._tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "    def predict(self, premise: str, hypothesis: str, same_sentence: str, same_paragraph: str) -> JsonDict:\n",
    "        return self.predict_json({\"premise\": premise, \"hypothesis\": hypothesis, \n",
    "                                  \"same_sentence\": same_sentence, \"same_paragraph\": same_paragraph})\n",
    "    \n",
    "    @overrides\n",
    "    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n",
    "        \"\"\"\n",
    "        Expects JSON that looks like `{\"premise\": \"...\", \"hypothesis\": \"...\", \"metadata\": \"...\"}`.\n",
    "        \"\"\"\n",
    "        premise_text = json_dict[\"premise\"]\n",
    "        hypothesis_text = json_dict[\"hypothesis\"]\n",
    "        same_sentence = json_dict[\"same_sentence\"]\n",
    "        same_paragraph = json_dict[\"same_paragraph\"]\n",
    "        #metadata = json_dict[\"metadata\"]\n",
    "        #same_sentence, same_paragraph = metadata.split('\\t')\n",
    "        return self._dataset_reader.text_to_instance(premise_text, \n",
    "                                                     hypothesis_text, \n",
    "                                                     label=None, \n",
    "                                                     same_sentence=same_sentence,\n",
    "                                                     same_paragraph=same_paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Generate config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/structure_predictor_bimpm/structure_cf_train.tsv\n",
      "models/structure_predictor_bimpm/structure_cf_dev.tsv\n",
      "models/structure_predictor_bimpm/structure_cf_test.tsv\n"
     ]
    }
   ],
   "source": [
    "print(TRAIN_FILE_PATH)\n",
    "print(DEV_FILE_PATH)\n",
    "print(TEST_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/structure_predictor_bimpm/config_elmo.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile $MODEL_PATH/config_elmo.json\n",
    "\n",
    "// Configuration for a sentence matching model based on:\n",
    "//   Wang, Zhiguo, Wael Hamza, and Radu Florian. \"Bilateral multi-perspective matching for natural language sentences.\"\n",
    "//   Proceedings of the 26th International Joint Conference on Artificial Intelligence. 2017.\n",
    "// (Augmented with additional granularity related features)\n",
    "\n",
    "local NUM_EPOCHS = 50;\n",
    "local LR = 1e-3;\n",
    "\n",
    "{\n",
    "  \"dataset_reader\": {\n",
    "    \"type\": \"custom_pairs_reader\",\n",
    "    \"lazy\": false,\n",
    "    \"token_indexers\": {\n",
    "      \"token_characters\": {\n",
    "        \"type\": \"characters\",\n",
    "        \"min_padding_length\": 3\n",
    "      },\n",
    "      \"elmo\": {\n",
    "        \"type\": \"elmo_characters\"\n",
    "     }\n",
    "    }\n",
    "  },\n",
    "  \"train_data_path\": \"structure_predictor_bimpm/structure_cf_train.tsv\",\n",
    "  \"validation_data_path\": \"structure_predictor_bimpm/structure_cf_dev.tsv\",\n",
    "  \"model\": {\n",
    "    \"type\": \"custom_bimpm\",\n",
    "    \"dropout\": 0.5,\n",
    "    \"class_weights\": [0.3, 1.0],\n",
    "    \"encode_together\": false,\n",
    "    \"text_field_embedder\": {\n",
    "        \"token_embedders\": {\n",
    "            \"elmo\": {\n",
    "                    \"type\": \"elmo_token_embedder\",\n",
    "                    \"options_file\": \"rsv_elmo/options.json\",\n",
    "                    \"weight_file\": \"rsv_elmo/model.hdf5\",\n",
    "                    \"do_layer_norm\": false,\n",
    "                    \"dropout\": 0.0\n",
    "            },\n",
    "            \"token_characters\": {\n",
    "                \"type\": \"character_encoding\",\n",
    "                \"dropout\": 0.2,\n",
    "                \"embedding\": {\n",
    "                    \"embedding_dim\": 20,\n",
    "                    \"padding_index\": 0\n",
    "                },\n",
    "                \"encoder\": {\n",
    "                    \"type\": \"gru\",\n",
    "                    \"input_size\": 20,\n",
    "                    \"hidden_size\": 50,\n",
    "                    \"num_layers\": 1,\n",
    "                    \"bidirectional\": true\n",
    "              },\n",
    "            }\n",
    "      }\n",
    "    },\n",
    "    \"matcher_word\": {\n",
    "      \"is_forward\": true,\n",
    "      \"hidden_dim\": 1024+100,\n",
    "      \"num_perspectives\": 10,\n",
    "      \"with_full_match\": false\n",
    "    },\n",
    "    \"encoder1\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": 1024+100,\n",
    "      \"hidden_size\": 200,\n",
    "      \"num_layers\": 1\n",
    "    },\n",
    "    \"matcher_forward1\": {\n",
    "      \"is_forward\": true,\n",
    "      \"hidden_dim\": 200,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"matcher_backward1\": {\n",
    "      \"is_forward\": false,\n",
    "      \"hidden_dim\": 200,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"encoder2\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": 400,\n",
    "      \"hidden_size\": 200,\n",
    "      \"num_layers\": 1\n",
    "    },\n",
    "    \"matcher_forward2\": {\n",
    "      \"is_forward\": true,\n",
    "      \"hidden_dim\": 200,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"matcher_backward2\": {\n",
    "      \"is_forward\": false,\n",
    "      \"hidden_dim\": 200,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"aggregator\":{\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": 264,\n",
    "      \"hidden_size\": 100,\n",
    "      \"num_layers\": 1,\n",
    "    },\n",
    "    \"classifier_feedforward\": {\n",
    "      \"input_dim\": 200+200+1+1,\n",
    "      \"num_layers\": 2,\n",
    "      \"hidden_dims\": [200, 2],\n",
    "      \"activations\": [\"relu\", \"linear\"],\n",
    "      \"dropout\": [0.5, 0.0]\n",
    "    },\n",
    "    \"initializer\": [\n",
    "      [\".*linear_layers.*weight\", {\"type\": \"xavier_normal\"}],\n",
    "      [\".*linear_layers.*bias\", {\"type\": \"constant\", \"val\": 0}],\n",
    "      [\".*weight_ih.*\", {\"type\": \"xavier_normal\"}],\n",
    "      [\".*weight_hh.*\", {\"type\": \"orthogonal\"}],\n",
    "      [\".*bias.*\", {\"type\": \"constant\", \"val\": 0}],\n",
    "      [\".*matcher.*match_weights.*\", {\"type\": \"kaiming_normal\"}]\n",
    "    ]\n",
    "  },\n",
    "  \"iterator\": {\n",
    "        \"type\": \"bucket\",\n",
    "        \"batch_size\": 20,\n",
    "        \"padding_noise\": 0,\n",
    "        \"sorting_keys\": [\n",
    "            [\n",
    "                \"premise\",\n",
    "                \"num_tokens\"\n",
    "            ],\n",
    "            [\n",
    "                \"hypothesis\",\n",
    "                \"num_tokens\"\n",
    "            ]\n",
    "        ]\n",
    "  },\n",
    "  \"trainer\": {\n",
    "    \"num_epochs\": NUM_EPOCHS,\n",
    "    \"cuda_device\": 1,\n",
    "    \"optimizer\": {\n",
    "      \"type\": \"adam\",\n",
    "      \"lr\": LR\n",
    "    },\n",
    "    \"type\":\"callback\",\n",
    "    \"callbacks\": [\n",
    "        {\n",
    "            \"type\": \"validate\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"checkpoint\",\n",
    "            \"checkpointer\":{\n",
    "                \"num_serialized_models_to_keep\": 1\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"gradient_norm_and_clip\", \n",
    "            \"grad_norm\": 10.0\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"track_metrics\",\n",
    "            \"patience\": 7,\n",
    "            \"validation_metric\": \"+f1\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"log_metrics_to_wandb\"\n",
    "        }\n",
    "    ],\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Scripts for training/prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1. Directly from the config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/train_structure_predictor.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/train_structure_predictor.sh\n",
    "# usage:\n",
    "# $ cd models \n",
    "# $ sh train_structure_predictor.sh {bert|elmo} result_directory\n",
    "\n",
    "export METHOD=${1}\n",
    "export RESULT_DIR=${2}\n",
    "export DEV_FILE_PATH=\"structure_cf_dev.tsv\"\n",
    "export TEST_FILE_PATH=\"structure_cf_test.tsv\"\n",
    "\n",
    "rm -r structure_predictor_bimpm/${RESULT_DIR}/\n",
    "allennlp train -s structure_predictor_bimpm/${RESULT_DIR}/ structure_predictor_bimpm/config_${METHOD}.json \\\n",
    "   --include-package bimpm_custom_package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on dev&test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/eval_structure_predictor.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/eval_structure_predictor.sh\n",
    "# usage:\n",
    "# $ cd models \n",
    "# $ sh eval_structure_predictor.sh {bert|elmo} result_directory\n",
    "\n",
    "export METHOD=${1}\n",
    "export RESULT_DIR=${2}\n",
    "export DEV_FILE_PATH=\"structure_cf_dev.tsv\"\n",
    "export TEST_FILE_PATH=\"structure_cf_test.tsv\"\n",
    "\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "    --output-file structure_predictor_bimpm/${RESULT_DIR}/predictions_dev.json \\\n",
    "    structure_predictor_bimpm/${RESULT_DIR}/model.tar.gz structure_predictor_bimpm/${DEV_FILE_PATH} \\\n",
    "    --include-package bimpm_custom_package \\\n",
    "    --predictor custom_bimpm_predictor\n",
    "\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "    --output-file structure_predictor_bimpm/${RESULT_DIR}/predictions_test.json \\\n",
    "    structure_predictor_bimpm/${RESULT_DIR}/model.tar.gz structure_predictor_bimpm/${TEST_FILE_PATH} \\\n",
    "    --include-package bimpm_custom_package \\\n",
    "    --predictor custom_bimpm_predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2. Using wandb for parameters adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/wandb_structure_predictor.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/wandb_structure_predictor.yaml\n",
    "# usage:\n",
    "# $ cd models\n",
    "# wandb sweep wandb_structure_predictor.yaml\n",
    "\n",
    "name: structure_predictor_stacked\n",
    "program: wandb_allennlp # this is a wrapper console script around allennlp commands. It is part of wandb-allennlp\n",
    "method: bayes\n",
    "## Do not for get to use the command keyword to specify the following command structure\n",
    "command:\n",
    "  - ${program} #omit the interpreter as we use allennlp train command directly\n",
    "  - \"--subcommand=train\"\n",
    "  - \"--include-package=bimpm_custom_package\" # add all packages containing your registered classes here\n",
    "  - \"--config_file=structure_predictor_bimpm/config_elmo.json\"\n",
    "  - ${args}\n",
    "metric:\n",
    "    name: best_f1\n",
    "    goal: maximize\n",
    "parameters:\n",
    "    model.type:\n",
    "        values: [\"custom_bimpm\",]\n",
    "    iterator.batch_size:\n",
    "        values: [8,]\n",
    "    model.encode_together:\n",
    "        values: [\"false\",]\n",
    "    trainer.optimizer.lr:\n",
    "        values: [0.001,]\n",
    "    model.dropout:\n",
    "        values: [0.5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Run training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``wandb sweep wandb_structure_predictor.yaml``\n",
    "\n",
    "(returns %sweepname)\n",
    "\n",
    "``wandb agent --count 1 %sweepname``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the best model in structure_predictor_bimpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! ls -laht models/wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp -r models/wandb/run-20200712_185220-estrcbze/training_dumps models/structure_predictor_bimpm/fresh-sweep-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Evaluate classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictions(path):\n",
    "    result = []\n",
    "    \n",
    "    with open(path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            result.append(json.loads(line)[\"label\"])\n",
    "            \n",
    "    result = list(map(int, result))\n",
    "    print('length of result:', len(result))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_DIR = 'fresh-sweep-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "true = pd.read_csv(DEV_FILE_PATH, sep='\\t', header=None)[0].values.tolist()\n",
    "pred = load_predictions(f'{MODEL_PATH}/{RESULT_DIR}/predictions_dev.json')\n",
    "print('length of true labels:', len(true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "\n",
    "print('f1: %.2f'%(f1_score(true[:len(pred)], pred)*100))\n",
    "print('pr: %.2f'%(precision_score(true[:len(pred)], pred)*100))\n",
    "print('re: %.2f'%(recall_score(true[:len(pred)], pred)*100))\n",
    "\n",
    "print(classification_report(true[:len(pred)], pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "true = pd.read_csv(TEST_FILE_PATH, sep='\\t', header=None)[0].values.tolist()\n",
    "pred = load_predictions(f'{MODEL_PATH}/{RESULT_DIR}/predictions_test.json')\n",
    "print('length of true labels:', len(true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('f1: %.2f'%(f1_score(true[:len(pred)], pred)*100))\n",
    "print('pr: %.2f'%(precision_score(true[:len(pred)], pred)*100))\n",
    "print('re: %.2f'%(recall_score(true[:len(pred)], pred)*100))\n",
    "\n",
    "print(classification_report(true[:len(pred)], pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "print('f1: %.2f'%(f1_score(true[:len(pred)], pred)*100))\n",
    "print('pr: %.2f'%(precision_score(true[:len(pred)], pred)*100))\n",
    "print('re: %.2f'%(recall_score(true[:len(pred)], pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vocab = [0, 1]\n",
    "catboost_vocab = [0, 1]\n",
    "\n",
    "def load_neural_predictions(path):\n",
    "    result = []\n",
    "    \n",
    "    with open(path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            probs = json.loads(line)['probs']\n",
    "            probs = {model_vocab[i]: probs[i] for i in range(len(model_vocab))}\n",
    "            result.append(probs)\n",
    "            \n",
    "    return result\n",
    "\n",
    "def load_scikit_predictions(model, X):\n",
    "    result = []\n",
    "    \n",
    "    try:\n",
    "        predictions = model.predict_proba(X)\n",
    "    except AttributeError:\n",
    "        predictions = model._predict_proba_lr(X)\n",
    "    \n",
    "    for prediction in predictions:\n",
    "        probs = {catboost_vocab[j]: prediction[j] for j in range(len(catboost_vocab))}\n",
    "        result.append(probs)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def vote_predictions(pred1, pred2, soft=True):\n",
    "    assert len(pred1) == len(pred2)\n",
    "    result = []\n",
    "    \n",
    "    for i in range(len(pred1)):\n",
    "        sample_result = {}\n",
    "        for key in pred1[i].keys():\n",
    "            if soft:\n",
    "                sample_result[key] = pred1[i][key] + pred2[i][key]\n",
    "            else:\n",
    "                sample_result[key] = max(pred1[i][key], pred2[i][key])\n",
    "        \n",
    "        result.append(sample_result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def probs_to_classes(pred):\n",
    "    result = []\n",
    "    \n",
    "    for sample in pred:\n",
    "        best_class = ''\n",
    "        best_prob = 0.\n",
    "        for key in sample.keys():\n",
    "            if sample[key] > best_prob:\n",
    "                best_prob = sample[key]\n",
    "                best_class = key\n",
    "        \n",
    "        result.append(best_class)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model = pickle.load(open('models/structure_predictor_baseline/model.pkl', 'rb'))\n",
    "scaler = pickle.load(open('models/structure_predictor_baseline/scaler.pkl', 'rb'))\n",
    "drop_columns = pickle.load(open('models/structure_predictor_baseline/drop_columns.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_PATH = 'data_structure'\n",
    "\n",
    "train_samples = pd.read_pickle(os.path.join(IN_PATH, 'train_samples.pkl'))\n",
    "dev_samples = pd.read_pickle(os.path.join(IN_PATH, 'dev_samples.pkl'))\n",
    "test_samples = pd.read_pickle(os.path.join(IN_PATH, 'test_samples.pkl'))\n",
    "\n",
    "y_train, X_train = train_samples['relation'].to_frame(), train_samples.drop('relation', axis=1).drop(\n",
    "    columns=drop_columns + ['category_id'])\n",
    "y_dev, X_dev = dev_samples['relation'].to_frame(), dev_samples.drop('relation', axis=1).drop(\n",
    "    columns=drop_columns + ['category_id'])\n",
    "y_test, X_test = test_samples['relation'].to_frame(), test_samples.drop('relation', axis=1).drop(\n",
    "    columns=drop_columns + ['category_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_np = scaler.transform(X_dev)\n",
    "X_dev = pd.DataFrame(X_scaled_np, index=X_dev.index)\n",
    "\n",
    "X_scaled_np = scaler.transform(X_test)\n",
    "X_test = pd.DataFrame(X_scaled_np, index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "TARGET = 'relation'\n",
    "svm_predictions = load_scikit_predictions(model, X_dev)\n",
    "neural_predictions = load_neural_predictions(f'{MODEL_PATH}/{RESULT_DIR}/predictions_dev.json')\n",
    "\n",
    "tmp = vote_predictions(neural_predictions, svm_predictions, soft=True)\n",
    "ensemble_pred = probs_to_classes(tmp)\n",
    "\n",
    "print('f1: %.2f'%(metrics.f1_score(y_dev, ensemble_pred)*100.))\n",
    "print('pr: %.2f'%(metrics.precision_score(y_dev, ensemble_pred)*100.))\n",
    "print('re: %.2f'%(metrics.recall_score(y_dev, ensemble_pred)*100.))\n",
    "print()\n",
    "print(metrics.classification_report(y_dev, ensemble_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_predictions = load_scikit_predictions(model, X_test)\n",
    "neural_predictions = load_neural_predictions(f'{MODEL_PATH}/{RESULT_DIR}/predictions_test.json')\n",
    "\n",
    "tmp = vote_predictions(neural_predictions, svm_predictions, soft=True)\n",
    "ensemble_pred = probs_to_classes(tmp)\n",
    "\n",
    "print('f1: %.2f'%(metrics.f1_score(y_test, ensemble_pred)*100.))\n",
    "print('pr: %.2f'%(metrics.precision_score(y_test, ensemble_pred)*100.))\n",
    "print('re: %.2f'%(metrics.recall_score(y_test, ensemble_pred)*100.))\n",
    "print()\n",
    "print(metrics.classification_report(y_test, ensemble_pred, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
