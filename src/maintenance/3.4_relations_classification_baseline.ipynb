{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments with the label classification method presented in \"CLASSIFICATION MODELS FOR RST DISCOURSE PARSING OF TEXTS IN RUSSIAN\"\n",
    "http://www.dialog-21.ru/media/4595/chistovaevplusetal-076.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get code for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "rm -r rurst2019\n",
    "mkdir rurst2019\n",
    "cd rurst2019\n",
    "wget -q http://nlp.isa.ru/paper_dialog2019/utils/meaningfulwords_v3.py\n",
    "wget -q http://nlp.isa.ru/paper_dialog2019/utils/language_features.py\n",
    "wget -q http://nlp.isa.ru/paper_dialog2019/utils/features_processor.py\n",
    "\n",
    "# some external modules are structurally the same but have other paths\n",
    "sed -i \"s|utils/tf_idf_pipeline.save|models/tf_idf/pipeline.pkl|g\" features_processor.py  # tf-idf pipeline\n",
    "sed -i \"s|models_w2v/model2_tokenized|models/w2v/segmentator/model2_tokenized|g\" features_processor.py  # w2v model\n",
    "\n",
    "# also some fixes of the feature extractor\n",
    "sed -i \"s|'common_root_fpos',|\\n|g\" features_processor.py\n",
    "sed -i \"s|'common_root_att',|\\n|g\" features_processor.py\n",
    "sed -i \"s|'common_root'|\\n|g\" features_processor.py\n",
    "sed -i \"s|/ len(row))|/ (len(row) + 1e-8))|g\" features_processor.py\n",
    "sed -i \"s|'tokens_x', 'tokens_y',|\\n|g\" features_processor.py\n",
    "#sed -i \"s|return [self.annotations['tokens'][i].text for i in range(begin, end)]|result = [self.annotations['tokens'][i].text for i in range(begin, end)]\\n        if result:\\n            return result\\n        return ['_']|g\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extract features \n",
    "Same way as in ``1_data_extraction.ipynb`` but with another interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from rurst2019.features_processor import FeaturesProcessor\n",
    "\n",
    "features_processor = FeaturesProcessor(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb58bb9f22e4a5cb89f3f83df77a120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=233), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm.autonotebook import tqdm\n",
    "from utils.file_reading import read_gold, read_annotation\n",
    "\n",
    "\n",
    "IN_PATH = 'data/'\n",
    "for file in tqdm(glob.glob(\"%s*.json\" % IN_PATH)):\n",
    "    # print(file)\n",
    "    table = read_gold(file.replace('.json', ''))\n",
    "    table = table[table.snippet_x.map(len) > 0]\n",
    "    table = table[table.snippet_y.map(len) > 0]\n",
    "    annot = read_annotation(file.replace('.json', ''))\n",
    "    features = features_processor(table, annot)\n",
    "    features.to_pickle(file.replace('.json', '.gold.pkl.oldf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Classification model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from catboost import Pool\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "random_state = 45\n",
    "TARGET = 'relation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news in train: 0.5344827586206896,\tin dev: 0.6470588235294118,\tin test: 0.6086956521739131\n",
      "ling in train: 0.0,\tin dev: 0.0,\tin test: 0.0\n",
      "comp in train: 0.0,\tin dev: 0.0,\tin test: 0.0\n",
      "blog in train: 0.43103448275862066,\tin dev: 0.5294117647058824,\tin test: 0.4782608695652174\n"
     ]
    }
   ],
   "source": [
    "from utils.train_test_split import split_train_dev_test\n",
    "\n",
    "train, dev, test = split_train_dev_test('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.file_reading import read_gold\n",
    "\n",
    "\n",
    "train_samples = []\n",
    "test_samples = []\n",
    "dev_samples = []\n",
    "\n",
    "for file in train:\n",
    "    train_samples.append(pd.read_pickle(file.replace('.edus', '.gold.pkl.oldf')))\n",
    "\n",
    "for file in dev:\n",
    "    dev_samples.append(pd.read_pickle(file.replace('.edus', '.gold.pkl.oldf')))\n",
    "    \n",
    "for file in test:\n",
    "    test_samples.append(pd.read_pickle(file.replace('.edus', '.gold.pkl.oldf')))\n",
    "\n",
    "train_samples = pd.concat(train_samples).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "dev_samples = pd.concat(dev_samples).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "test_samples = pd.concat(test_samples).sample(frac=1, random_state=random_state).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_PATH = 'data_labeling'\n",
    "\n",
    "train_samples = pd.read_pickle(os.path.join(IN_PATH, 'train_samples.pkl'))\n",
    "dev_samples = pd.read_pickle(os.path.join(IN_PATH, 'dev_samples.pkl'))\n",
    "test_samples = pd.read_pickle(os.path.join(IN_PATH, 'test_samples.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data, max_len=100):\n",
    "    target_map = {\n",
    "        'relation': 'joint',\n",
    "        'antithesis': 'contrast',\n",
    "        'cause': 'cause-effect',\n",
    "        'effect': 'cause-effect',\n",
    "        'conclusion': 'restatement',\n",
    "        'evaluation': 'interpretation-evaluation',\n",
    "        'motivation': 'condition',\n",
    "    }\n",
    "\n",
    "    relation_map = {\n",
    "        'restatement_SN': 'restatement_NN',\n",
    "        'restatement_NS': 'restatement_NN',\n",
    "        'contrast_SN': 'contrast_NN',\n",
    "        'contrast_NS': 'contrast_NN',\n",
    "        'solutionhood_NS': 'elaboration_NS',\n",
    "        'preparation_NS': 'elaboration_NS',\n",
    "        'concession_SN': 'preparation_SN',\n",
    "        'evaluation_SN': 'preparation_SN',\n",
    "        'elaboration_SN': 'preparation_SN',\n",
    "        'evidence_SN': 'preparation_SN',\n",
    "    }\n",
    "\n",
    "    data['snippet_x'] = data.tokens_x.map(lambda row: ' '.join(row))\n",
    "    data['snippet_y'] = data.tokens_y.map(lambda row: ' '.join(row))\n",
    "    \n",
    "    data = data[data.snippet_x.map(len) > 0]\n",
    "    data = data[data.snippet_y.map(len) > 0]\n",
    "\n",
    "    data['category_id'] = data['category_id'].map(lambda row: row.split('_')[0])\n",
    "    data['category_id'] = data['category_id'].replace([0.0], 'same-unit')\n",
    "    data['order'] = data['order'].replace([0.0], 'NN')\n",
    "    data['category_id'] = data['category_id'].replace(target_map, regex=False)\n",
    "\n",
    "    data['relation'] = data['category_id'].map(lambda row: row) + '_' + data['order']\n",
    "    data['relation'] = data['relation'].replace(relation_map, regex=False)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = prepare_data(train_samples)\n",
    "dev_samples = prepare_data(dev_samples)\n",
    "test_samples = prepare_data(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3674, 3204, 1241,  823,  744,  726,  690,  597,  546,  544,  509,\n",
       "        491,  393,  390,  280,  271,  259,  178,  159,  130,  107,  105,\n",
       "         92])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = train_samples[TARGET].value_counts(normalize=False).values\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['snippet_x', 'snippet_y', 'category_id', \n",
    "                'snippet_x_tmp', 'snippet_y_tmp', \n",
    "                'filename', 'order', 'postags_x', 'postags_y',\n",
    "                'is_broken', 'tokens_x', 'tokens_y']\n",
    "y_train, X_train = train_samples[TARGET].to_frame(), train_samples.drop(TARGET, axis=1).drop(\n",
    "    columns=drop_columns + ['category_id'])\n",
    "y_dev, X_dev = dev_samples[TARGET].to_frame(), dev_samples.drop(TARGET, axis=1).drop(\n",
    "    columns=drop_columns + ['category_id'])\n",
    "y_test, X_test = test_samples[TARGET].to_frame(), test_samples.drop(TARGET, axis=1).drop(\n",
    "    columns=drop_columns + ['category_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "X_scaled_np = scaler.transform(X_train)\n",
    "X_train = pd.DataFrame(X_scaled_np, index=X_train.index)\n",
    "\n",
    "X_scaled_np = scaler.transform(X_dev)\n",
    "X_dev = pd.DataFrame(X_scaled_np, index=X_dev.index)\n",
    "\n",
    "X_scaled_np = scaler.transform(X_test)\n",
    "X_test = pd.DataFrame(X_scaled_np, index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "lab_encoder = LabelEncoder()\n",
    "y_train = lab_encoder.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=random_state,\n",
    "                            solver='lbfgs',\n",
    "                            n_jobs=8,\n",
    "                            C=0.002,\n",
    "                            multi_class='multinomial',\n",
    "                            class_weight='balanced')\n",
    "\n",
    "eval_dataset = Pool(data=X_dev,\n",
    "                    label=y_dev)\n",
    "\n",
    "catboost = CatBoostClassifier(\n",
    "    iterations=2000,\n",
    "    learning_rate=0.1,\n",
    "    custom_loss=['F1'],\n",
    "    random_seed=random_state,\n",
    "    verbose=0,\n",
    "    loss_function='MultiClass',\n",
    "    class_weights=counts / counts[-1]\n",
    ")\n",
    "\n",
    "fs_catboost = Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LogisticRegression(solver='saga', penalty='l1', C=1., n_jobs=-1))),\n",
    "  ('classification', catboost)\n",
    "])\n",
    "\n",
    "logreg = LogisticRegression(random_state=random_state,\n",
    "                            solver='lbfgs',\n",
    "                            n_jobs=-1,\n",
    "                            C=0.002,\n",
    "                            multi_class='multinomial',\n",
    "                            class_weight='balanced')\n",
    "\n",
    "fs_catboost_plus_logreg = VotingClassifier([('fs_catboost', fs_catboost), ('logreg', logreg)], voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_catboost_plus_logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "predicted = lab_encoder.inverse_transform(fs_catboost_plus_logreg.predict(X_dev))\n",
    "\n",
    "print('weighted f1: ', metrics.f1_score(y_dev.values, predicted, average='weighted'))\n",
    "print('macro f1: ', metrics.f1_score(y_dev.values, predicted, average='macro'))\n",
    "print('accuracy: ', metrics.accuracy_score(y_dev.values, predicted))\n",
    "print()\n",
    "print(metrics.classification_report(y_dev, predicted, digits=4))\n",
    "print('macro precision: %.2f'%(metrics.precision_score(y_dev, predicted, average='macro')*100.))\n",
    "print('macro recall: %.2f'%(metrics.recall_score(y_dev, predicted, average='macro')*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "predicted = lab_encoder.inverse_transform(fs_catboost_plus_logreg.predict(X_test))\n",
    "\n",
    "print('weighted f1: ', metrics.f1_score(y_test.values, predicted, average='weighted'))\n",
    "print('macro f1: ', metrics.f1_score(y_test.values, predicted, average='macro'))\n",
    "print('accuracy: ', metrics.accuracy_score(y_test.values, predicted))\n",
    "print()\n",
    "print(metrics.classification_report(y_test, predicted, digits=4))\n",
    "print('macro precision: %.2f'%(metrics.precision_score(y_test, predicted, average='macro')*100.))\n",
    "print('macro recall: %.2f'%(metrics.recall_score(y_test, predicted, average='macro')*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(fs_catboost_plus_logreg, open('models/dialog_model.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
