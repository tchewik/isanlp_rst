{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discourse segmentation\n",
    "\n",
    "(Customizes the multilingual RST segmentation system introduced in https://www.aclweb.org/anthology/W19-2715/ )\n",
    "\n",
    "Create new train and test sets for Russian; make up new models configs and modify train/evaluation scripts.\n",
    "\n",
    "Requires:\n",
    "\n",
    " - code for the original paper in ``../tony/``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U git+https://github.com/IINemo/isanlp.git@discourse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare dataset for model training and evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_reading import read_edus\n",
    "from utils.file_reading import SYMBOL_MAP\n",
    "\n",
    "def prepare_token(token):\n",
    "    for key, value in SYMBOL_MAP.items():\n",
    "        token = token.replace(key, value)\n",
    "        \n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annot2tags(annot, edus):\n",
    "    tags = []\n",
    "    cursor = 0\n",
    "\n",
    "#     for i, edu in enumerate(edus):\n",
    "#         if prepare_token(edu).find(prepare_token(annot['text'][annot['tokens'][0].begin:annot['tokens'][0].end])) != -1:\n",
    "#             cursor = i         \n",
    "    \n",
    "    for sentence in range(len(annot['sentences'])):\n",
    "        sentence_tags = []\n",
    "        previous_first_token = 0\n",
    "        previous_edu = ''\n",
    "\n",
    "        for token in range(annot['sentences'][sentence].begin, annot['sentences'][sentence].end):\n",
    "\n",
    "            if cursor == len(edus):\n",
    "                is_first_token = False\n",
    "\n",
    "            else:\n",
    "                is_first_token = False\n",
    "                \n",
    "                tmp_edu = prepare_token(edus[cursor])\n",
    "                annot['text'] = annot['text'].replace('билие ', 'Обилие ')\n",
    "                original_text = annot['text'][annot['tokens'][token].begin:annot['tokens'][token].end]\n",
    "                original_text = prepare_token(original_text).strip()\n",
    "\n",
    "                if tmp_edu.startswith(original_text):\n",
    "                    if previous_edu:\n",
    "                        if prepare_token(annot['text'][annot['tokens'][previous_first_token].begin:annot['tokens'][\n",
    "                        token].begin].strip()) == previous_edu or original_text.lower() in [\"сначала\", \"кватахеви\", \n",
    "                                                                                           \"целый\", \"для\", \n",
    "                                                                                           \"максимальный\", \"тогда\",\n",
    "                                                                                           \"два\", \"исследованием\",\n",
    "                                                                                            \"хотя\", \"хоть\",\n",
    "                                                                                            \"обилие\", \"активное\",\n",
    "                                                                                            \"менее\"\n",
    "                                                                                           ] or tmp_edu in [\n",
    "                            \"и два внешних кольца (α, α).\",\n",
    "                            \"У этого кольца самый высокий эксцентриситет из всех,\",\n",
    "                            prepare_token(\"Средний размер частичек в этом кольце 0,2— 20 метров,\"),\n",
    "                            \"(Перевод Нгуен Тхи Тхуи Чам))\",\n",
    "                            \"в Чувашии состоялся необъявленный праздник саха культуры.\",\n",
    "                            \"и выпустила сборник якутской поэзии «Жемчужина Сахи» (Саха ахахӗ, 1996),\",\n",
    "                            \"Однако и песня ему кажется какой-то бесцветной. Видимо, потому, что она творится только голосом, а не душой [Там же];\",\n",
    "                            \"и может использоваться, как и все конструкции, содержащие турцизмы,\",\n",
    "                            prepare_token(\"Новобранщине - солдатчине, Эполетщине - бобёрщине, Всей пехотщине, поморщине, менее Хлеборобщине, военщине, Что с армейской долей венчаны более \"),\n",
    "                            \"который фильтрует вредные ультрафиолетовые лучи Солнца.\",\n",
    "                            \"а также крем-уход для волос «Пептиды шелка и иранская хна».\",\n",
    "                            \"К 2010 году около 100 озоноразрушающих веществ, включая ХФУ, будут сняты с производства повсеместно.\"\n",
    "                        ]:\n",
    "                            is_first_token = True\n",
    "                            previous_first_token = token\n",
    "                            previous_edu = tmp_edu\n",
    "                            cursor += 1\n",
    "                    else:\n",
    "                        is_first_token = True\n",
    "                        previous_first_token = token\n",
    "                        previous_edu = tmp_edu\n",
    "                        cursor += 1\n",
    "\n",
    "            tag = 'BeginSeg=Yes' if is_first_token else '_'\n",
    "            sentence_tags.append(tag)\n",
    "\n",
    "        tags.append(sentence_tags)\n",
    "\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isanlp.utils.annotation_conll_converter import AnnotationCONLLConverter\n",
    "\n",
    "converter = AnnotationCONLLConverter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isanlp.annotation import Token, Sentence\n",
    "\n",
    "\n",
    "def split_by_paragraphs(annot_text, annot_tokens, annot_sentences, annot_lemma, annot_morph, annot_postag, annot_ud_postag,\n",
    "                 annot_syntax_dep_tree):\n",
    "\n",
    "        def split_on_two(sents, boundary):\n",
    "            list_sum = lambda l: sum([len(sublist) for sublist in l])\n",
    "\n",
    "            i = 1\n",
    "            while list_sum(sents[:i]) < boundary and i < len(sents):\n",
    "                i += 1\n",
    "\n",
    "            intersentence_boundary = min(len(sents[i - 1]), boundary - list_sum(sents[:i - 1]))\n",
    "            return (sents[:i - 1] + [sents[i - 1][:intersentence_boundary]], \n",
    "                    [sents[i - 1][intersentence_boundary:]] + sents[i:])\n",
    "        \n",
    "        def recount_sentences(chunk):\n",
    "            sentences = []\n",
    "            lemma = []\n",
    "            morph = []\n",
    "            postag = []\n",
    "            ud_postag = []\n",
    "            syntax_dep_tree = []\n",
    "            tokens_cursor = 0\n",
    "            local_cursor = 0\n",
    "\n",
    "            for i, sent in enumerate(chunk['syntax_dep_tree']):\n",
    "                if len(sent) > 0:\n",
    "                    sentences.append(Sentence(tokens_cursor, tokens_cursor + len(sent)))\n",
    "                    lemma.append(chunk['lemma'][i])\n",
    "                    morph.append(chunk['morph'][i])\n",
    "                    postag.append(chunk['postag'][i])\n",
    "                    ud_postag.append(chunk['ud_postag'][i])\n",
    "                    syntax_dep_tree.append(chunk['syntax_dep_tree'][i])\n",
    "                    tokens_cursor += len(sent)\n",
    "\n",
    "            chunk['sentences'] = sentences\n",
    "            chunk['lemma'] = lemma\n",
    "            chunk['morph'] = morph\n",
    "            chunk['postag'] = postag\n",
    "            chunk['ud_postag'] = ud_postag\n",
    "            chunk['syntax_dep_tree'] = syntax_dep_tree\n",
    "            \n",
    "            return chunk\n",
    "\n",
    "        chunks = []\n",
    "        prev_right_boundary = -1\n",
    "\n",
    "        for i, token in enumerate(annot_tokens[:-1]):\n",
    "\n",
    "            if '\\n' in annot_text[token.end:annot_tokens[i + 1].begin]:\n",
    "                if prev_right_boundary > -1:\n",
    "                    chunk = {\n",
    "                        'text': annot_text[annot_tokens[prev_right_boundary].end:token.end + 1].strip(),\n",
    "                        'tokens': annot_tokens[prev_right_boundary + 1:i + 1]\n",
    "                    }\n",
    "                else:\n",
    "                    chunk = {\n",
    "                        'text': annot_text[:token.end + 1].strip(),\n",
    "                        'tokens': annot_tokens[:i + 1]\n",
    "                    }\n",
    "\n",
    "                lemma, annot_lemma = split_on_two(annot_lemma, i - prev_right_boundary)\n",
    "                morph, annot_morph = split_on_two(annot_morph, i - prev_right_boundary)\n",
    "                postag, annot_postag = split_on_two(annot_postag, i - prev_right_boundary)\n",
    "                ud_postag, annot_ud_postag = split_on_two(annot_ud_postag, i - prev_right_boundary)\n",
    "                syntax_dep_tree, annot_syntax_dep_tree = split_on_two(annot_syntax_dep_tree, i - prev_right_boundary)\n",
    "\n",
    "                chunk.update({\n",
    "                    'lemma': lemma,\n",
    "                    'morph': morph,\n",
    "                    'postag': postag,\n",
    "                    'ud_postag': ud_postag,\n",
    "                    'syntax_dep_tree': syntax_dep_tree,\n",
    "                })\n",
    "                chunks.append(recount_sentences(chunk))\n",
    "\n",
    "                prev_right_boundary = i  # number of last token in the last chunk\n",
    "\n",
    "        chunk = {\n",
    "            'text': annot_text[annot_tokens[prev_right_boundary].end:].strip(),\n",
    "            'tokens': annot_tokens[prev_right_boundary + 1:],\n",
    "            'lemma' : annot_lemma,\n",
    "            'morph': annot_morph,\n",
    "            'postag': annot_postag,\n",
    "            'ud_postag': annot_ud_postag,\n",
    "            'syntax_dep_tree': annot_syntax_dep_tree,\n",
    "        }\n",
    "        \n",
    "        chunks.append(recount_sentences(chunk))\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm.autonotebook import tqdm\n",
    "from utils.file_reading import read_annotation, read_edus\n",
    "import re\n",
    "from utils.train_test_split import split_train_dev_test\n",
    "\n",
    "train, dev, test = split_train_dev_test('./data')\n",
    "TRAIN_FILE = 'rus.rst.rrt_train.conll'\n",
    "DEV_FILE = 'rus.rst.rrt_dev.conll'\n",
    "TEST_FILE = 'rus.rst.rrt_test.conll'\n",
    "MAX_LEN = 230\n",
    "\n",
    "\n",
    "def preprocess(files, train=True, dev=False):\n",
    "    print(f'preprocess {\"train\" if train else \"test\"} set')\n",
    "\n",
    "    output_file = DEV_FILE if dev else TRAIN_FILE if train else TEST_FILE\n",
    "    with open(output_file, 'w') as fo:\n",
    "        for filename in tqdm(files):\n",
    "            filename = filename.replace('.edus', '')\n",
    "            annot = read_annotation(filename)  # split as well  ToDO:\n",
    "            edus = read_edus(filename)\n",
    "            last_edu = 0\n",
    "            # tags = annot2tags(annot, edus)\n",
    "\n",
    "            for i, chunk in enumerate(split_by_paragraphs(  # self,\n",
    "                    annot['text'],\n",
    "                    annot['tokens'],\n",
    "                    annot['sentences'],\n",
    "                    annot['lemma'],\n",
    "                    annot['morph'],\n",
    "                    annot['postag'],\n",
    "                    annot['ud_postag'],\n",
    "                    annot['syntax_dep_tree'])):\n",
    "\n",
    "                sentence = 0\n",
    "                token = 0\n",
    "                chunk['text'] = annot['text']\n",
    "                #edus = \n",
    "                tags = annot2tags(chunk, edus[last_edu:])\n",
    "                \n",
    "                for string in converter(filename.replace('data/', ''), chunk):\n",
    "                    #print(string)\n",
    "                    if string.startswith('# newdoc id ='):\n",
    "                        sentence = 0\n",
    "                        token = 0\n",
    "                        fo.write(string + '\\n')\n",
    "\n",
    "                    elif string == '\\n':\n",
    "                        fo.write(string)\n",
    "                        sentence += 1\n",
    "                        token = 0\n",
    "\n",
    "                    else:\n",
    "                        if ' ' in string:\n",
    "                            string = re.sub(r' .*\\t', '\\t', string)\n",
    "                        if 'www' in string:\n",
    "                            string = re.sub(r'www[^\\t]*', '_html_', string)\n",
    "                        if 'http' in string:\n",
    "                            string = re.sub(r'http[^ \\t]*', '_html_', string)\n",
    "                            \n",
    "                        string = prepare_token(string)                        \n",
    "                        fo.write(string + '\\t' + tags[sentence][token] + '\\n')\n",
    "                        \n",
    "                        if tags[sentence][token] != '_':\n",
    "                            last_edu += 1\n",
    "                        \n",
    "                        token += 1\n",
    "\n",
    "                    if token == MAX_LEN:\n",
    "                        print(filename + ' ::: occured very long sentence; truncate to ' + str(MAX_LEN) + ' tokens.')\n",
    "                        fo.write('\\n')\n",
    "                        sentence += 1\n",
    "                        token = 0\n",
    "                        break\n",
    "\n",
    "\n",
    "preprocess(train)\n",
    "preprocess(dev, dev=True)\n",
    "preprocess(test, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$TRAIN_FILE\" \"$DEV_FILE\" \"$TEST_FILE\"\n",
    "\n",
    "export TONY_PATH=\"../tony/\"\n",
    "\n",
    "cp ${1} ${TONY_PATH}/data/rus.rst.rrt/${1}\n",
    "cp ${2} ${TONY_PATH}/data/rus.rst.rrt/${2}\n",
    "cp ${3} ${TONY_PATH}/data/rus.rst.rrt/${3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Baseline model (BERT-M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../tony/code/contextual_embeddings/configs/bertM.jsonnet\n",
    "\n",
    "\n",
    "// Configuration for a named entity recognization model based on:\n",
    "//   Peters, Matthew E. et al. “Deep contextualized word representations.” NAACL-HLT (2018).\n",
    "{\n",
    "  \"dataset_reader\": {\n",
    "    \"type\": \"conll2003\",\n",
    "    \"tag_label\": \"ner\",\n",
    "    \"coding_scheme\": \"BIOUL\",\n",
    "    \"token_indexers\": {\n",
    "      \"bert\": {\n",
    "          \"type\": \"bert-pretrained\",\n",
    "          \"pretrained_model\": std.extVar(\"BERT_VOCAB\"),\n",
    "          \"do_lowercase\": false,\n",
    "          \"use_starting_offsets\": true\n",
    "      },\n",
    "      \"token_characters\": {\n",
    "        \"type\": \"characters\",\n",
    "        \"min_padding_length\": 3\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"train_data_path\": std.extVar(\"TRAIN_DATA_PATH\"),\n",
    "  \"validation_data_path\": std.extVar(\"TEST_A_PATH\"),\n",
    "  \"model\": {\n",
    "    \"type\": \"simple_tagger\",\n",
    "    \"text_field_embedder\": {\n",
    "        \"allow_unmatched_keys\": true,\n",
    "        \"embedder_to_indexer_map\": {\n",
    "            \"bert\": [\"bert\", \"bert-offsets\"],\n",
    "            \"token_characters\": [\"token_characters\"],\n",
    "        },\n",
    "        \"token_embedders\": {\n",
    "            \"bert\": {\n",
    "                \"type\": \"bert-pretrained\",\n",
    "                \"pretrained_model\": std.extVar(\"BERT_WEIGHTS\")\n",
    "            },\n",
    "            \"token_characters\": {\n",
    "                \"type\": \"character_encoding\",\n",
    "                \"embedding\": {\n",
    "                    \"embedding_dim\": 16\n",
    "                },\n",
    "                \"encoder\": {\n",
    "                    \"type\": \"cnn\",\n",
    "                    \"embedding_dim\": 16,\n",
    "                    \"num_filters\": 128,\n",
    "                    \"ngram_filter_sizes\": [3],\n",
    "                    \"conv_layer_activation\": \"relu\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"encoder\": {\n",
    "        \"type\": \"lstm\",\n",
    "        \"input_size\": 768 + 128,\n",
    "        \"hidden_size\": 100,\n",
    "        \"num_layers\": 1,\n",
    "        \"dropout\": 0.5,\n",
    "        \"bidirectional\": true\n",
    "    },\n",
    "  },\n",
    "  \"iterator\": {\n",
    "    \"type\": \"basic\",\n",
    "    \"batch_size\": 2\n",
    "  },\n",
    "  \"trainer\": {\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"bert_adam\",\n",
    "        \"lr\": 0.001\n",
    "    },\n",
    "    \"num_serialized_models_to_keep\": 3,\n",
    "    \"num_epochs\": 10,\n",
    "    \"grad_norm\": 5.0,\n",
    "    \"patience\": 7,\n",
    "    \"cuda_device\": 0\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. CRF model (BERT-M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../tony/code/contextual_embeddings/configs/bertM_crf.jsonnet\n",
    "\n",
    "// Configuration for a named entity recognization model based on:\n",
    "//   Peters, Matthew E. et al. “Deep contextualized word representations.” NAACL-HLT (2018).\n",
    "{\n",
    "  \"dataset_reader\": {\n",
    "    \"type\": \"conll2003\",\n",
    "    \"tag_label\": \"ner\",\n",
    "    \"coding_scheme\": \"BIOUL\",\n",
    "    \"token_indexers\": {\n",
    "      \"bert\": {\n",
    "          \"type\": \"bert-pretrained\",\n",
    "          \"pretrained_model\": \"bert-base-multilingual-cased\",\n",
    "          \"do_lowercase\": false,\n",
    "          \"use_starting_offsets\": true\n",
    "      },\n",
    "      \"token_characters\": {\n",
    "        \"type\": \"characters\",\n",
    "        \"min_padding_length\": 3\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"train_data_path\": std.extVar(\"TRAIN_DATA_PATH\"),\n",
    "  \"validation_data_path\": std.extVar(\"TEST_A_PATH\"),\n",
    "  \"model\": {\n",
    "    \"type\": \"crf_tagger\",\n",
    "    \"dropout\": 0.2,\n",
    "    \"calculate_span_f1\": true,\n",
    "    \"label_encoding\": \"BIOUL\",\n",
    "    \"text_field_embedder\": {\n",
    "        \"allow_unmatched_keys\": true,\n",
    "        \"embedder_to_indexer_map\": {\n",
    "            \"bert\": [\"bert\", \"bert-offsets\"],\n",
    "            \"token_characters\": [\"token_characters\"],\n",
    "        },\n",
    "        \"token_embedders\": {\n",
    "            \"bert\": {\n",
    "                \"type\": \"bert-pretrained\",\n",
    "                \"pretrained_model\": \"bert-base-multilingual-cased\",\n",
    "            },\n",
    "            \"token_characters\": {\n",
    "                \"type\": \"character_encoding\",\n",
    "                \"embedding\": {\n",
    "                    \"embedding_dim\": 16\n",
    "                },\n",
    "                \"encoder\": {\n",
    "                    \"type\": \"cnn\",\n",
    "                    \"embedding_dim\": 16,\n",
    "                    \"num_filters\": 128,\n",
    "                    \"ngram_filter_sizes\": [3],\n",
    "                    \"conv_layer_activation\": \"relu\"\n",
    "                },\n",
    "                \"dropout\": 0.2\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    \"encoder\": {\n",
    "        \"type\": \"lstm\",\n",
    "        \"input_size\": 768 + 128,\n",
    "        \"hidden_size\": 200,\n",
    "        \"num_layers\": 1,\n",
    "        \"dropout\": 0.5,\n",
    "        \"bidirectional\": true\n",
    "    },\n",
    "  },\n",
    "  \"iterator\": {\n",
    "    \"type\": \"basic\",\n",
    "    \"batch_size\": 2\n",
    "  },\n",
    "  \"trainer\": {\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"bert_adam\",\n",
    "        \"lr\": 0.001\n",
    "    },\n",
    "    \"validation_metric\": \"+f1-measure-overall\",\n",
    "    \"num_serialized_models_to_keep\": 3,\n",
    "    \"num_epochs\": 10,\n",
    "    \"grad_norm\": 5.0,\n",
    "    \"patience\": 2,\n",
    "    \"cuda_device\": 1\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. CRF model (ELMo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../tony/code/contextual_embeddings/configs/elmo.jsonnet\n",
    "\n",
    "// Configuration for the NER model with ELMo, modified slightly from\n",
    "// the version included in \"Deep Contextualized Word Representations\",\n",
    "// taken from AllenNLP examples\n",
    "// modified for the disrpt discourse segmentation shared task -- 2019 \n",
    "{\n",
    "\n",
    "  \"dataset_reader\": {\n",
    "    \"type\": \"conll2003\",\n",
    "    \"tag_label\": \"ner\",\n",
    "    \"coding_scheme\": \"BIOUL\",\n",
    "    \"token_indexers\": {\n",
    "      \"tokens\": {\n",
    "        \"type\": \"single_id\",\n",
    "        \"lowercase_tokens\": true\n",
    "      },\n",
    "      \"token_characters\": {\n",
    "        \"type\": \"characters\",\n",
    "        \"min_padding_length\": 3\n",
    "      },\n",
    "      \"elmo\": {\n",
    "        \"type\": \"elmo_characters\"\n",
    "     }\n",
    "    }\n",
    "  },\n",
    "  \"train_data_path\": std.extVar(\"TRAIN_DATA_PATH\"),\n",
    "  \"validation_data_path\": std.extVar(\"TEST_A_PATH\"),\n",
    "  \"model\": {\n",
    "    \"type\": \"crf_tagger\",\n",
    "    \"dropout\": 0.5,\n",
    "    \"calculate_span_f1\": true,\n",
    "    \"label_encoding\": \"BIOUL\",\n",
    "    \"text_field_embedder\": {\n",
    "      \"token_embedders\": {\n",
    "        \"tokens\": {\n",
    "            \"type\": \"embedding\",\n",
    "            \"embedding_dim\": 300,\n",
    "            \"pretrained_file\": \"ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize.vec\",\n",
    "            \"trainable\": true\n",
    "        },\n",
    "        \"elmo\":{\n",
    "            \"type\": \"elmo_token_embedder\",\n",
    "            \"options_file\": \"rsv_elmo/options.json\",\n",
    "            \"weight_file\": \"rsv_elmo/model.hdf5\",\n",
    "            \"do_layer_norm\": false,\n",
    "            \"dropout\": 0.1\n",
    "        },\n",
    "        \"token_characters\": {\n",
    "            \"type\": \"character_encoding\",\n",
    "            \"embedding\": {\n",
    "                \"embedding_dim\": 16\n",
    "            },\n",
    "            \"encoder\": {\n",
    "                \"type\": \"cnn\",\n",
    "                \"embedding_dim\": 16,\n",
    "                \"num_filters\": 128,\n",
    "                \"ngram_filter_sizes\": [3],\n",
    "                \"conv_layer_activation\": \"relu\"\n",
    "            },\n",
    "            \"dropout\": 0.25\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"encoder\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"input_size\": 1024+128+300,\n",
    "      \"hidden_size\": 200,\n",
    "      \"num_layers\": 2,\n",
    "      \"dropout\": 0.5,\n",
    "      \"bidirectional\": true\n",
    "    },\n",
    "    \"regularizer\": [\n",
    "      [\n",
    "        \"scalar_parameters\",\n",
    "        {\n",
    "          \"type\": \"l2\",\n",
    "          \"alpha\": 0.01,\n",
    "        }\n",
    "      ]\n",
    "    ]\n",
    "  },\n",
    "  \"iterator\": {\n",
    "    \"type\": \"basic\",\n",
    "    \"batch_size\": 2\n",
    "  },\n",
    "  \"trainer\": {\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"adam\",\n",
    "        //\"lr\": 0.001\n",
    "    },\n",
    "    \"validation_metric\": \"+f1-measure-overall\",\n",
    "    \"num_serialized_models_to_keep\": 3,\n",
    "    \"num_epochs\": 10,\n",
    "    \"grad_norm\": 5.0,\n",
    "    \"patience\": 3,\n",
    "    \"cuda_device\": 0\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. CRF model (ELMo + RuBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../tony/code/contextual_embeddings/configs/stacked.jsonnet\n",
    "\n",
    "// Configuration for the NER model with ELMo, modified slightly from\n",
    "// the version included in \"Deep Contextualized Word Representations\",\n",
    "// taken from AllenNLP examples\n",
    "// modified for the disrpt discourse segmentation shared task -- 2019 \n",
    "{\n",
    "\n",
    "  \"dataset_reader\": {\n",
    "    \"type\": \"conll2003\",\n",
    "    \"tag_label\": \"ner\",\n",
    "    \"coding_scheme\": \"BIOUL\",\n",
    "    \"token_indexers\": {\n",
    "      //\"tokens\": {\n",
    "      //  \"type\": \"single_id\",\n",
    "      //  \"lowercase_tokens\": true\n",
    "      //},\n",
    "      \"token_characters\": {\n",
    "        \"type\": \"characters\",\n",
    "        \"min_padding_length\": 3\n",
    "      },\n",
    "      \"elmo\": {\n",
    "        \"type\": \"elmo_characters\"\n",
    "     },\n",
    "      \"bert\": {\n",
    "          \"type\": \"bert-pretrained\",\n",
    "          \"pretrained_model\": std.extVar(\"BERT_VOCAB\"),\n",
    "          \"do_lowercase\": false,\n",
    "          \"use_starting_offsets\": true\n",
    "      },\n",
    "    }\n",
    "  },\n",
    "  \"train_data_path\": std.extVar(\"TRAIN_DATA_PATH\"),\n",
    "  \"validation_data_path\": std.extVar(\"TEST_A_PATH\"),\n",
    "  \"model\": {\n",
    "    \"type\": \"crf_tagger\",\n",
    "    \"dropout\": 0.5,\n",
    "    \"calculate_span_f1\": true,\n",
    "    \"label_encoding\": \"BIOUL\",\n",
    "    \"text_field_embedder\": {\n",
    "        \"allow_unmatched_keys\": true,\n",
    "        \"embedder_to_indexer_map\": {\n",
    "            \"bert\": [\"bert\", \"bert-offsets\"],\n",
    "            \"token_characters\": [\"token_characters\"],\n",
    "            \"elmo\": [\"elmo\"],\n",
    "            \"tokens\": [\"tokens\"],\n",
    "        },\n",
    "      \"token_embedders\": {\n",
    "        //\"tokens\": {\n",
    "        //    \"type\": \"embedding\",\n",
    "        //    \"embedding_dim\": 300,\n",
    "        //    \"pretrained_file\": \"ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize.vec\",\n",
    "        //    \"trainable\": true\n",
    "        //},\n",
    "        \"elmo\":{\n",
    "            \"type\": \"elmo_token_embedder\",\n",
    "            \"options_file\": \"rsv_elmo/options.json\",\n",
    "            \"weight_file\": \"rsv_elmo/model.hdf5\",\n",
    "            \"do_layer_norm\": false,\n",
    "            \"dropout\": 0.0\n",
    "        },\n",
    "        \"bert\": {\n",
    "                \"type\": \"bert-pretrained\",\n",
    "                \"pretrained_model\": std.extVar(\"BERT_WEIGHTS\"),\n",
    "                \"requires_grad\": true,\n",
    "                \"top_layer_only\": false\n",
    "            },\n",
    "        \"token_characters\": {\n",
    "            \"type\": \"character_encoding\",\n",
    "            \"embedding\": {\n",
    "                \"embedding_dim\": 16\n",
    "            },\n",
    "            \"encoder\": {\n",
    "                \"type\": \"cnn\",\n",
    "                \"embedding_dim\": 16,\n",
    "                \"num_filters\": 128,\n",
    "                \"ngram_filter_sizes\": [3],\n",
    "                \"conv_layer_activation\": \"relu\"\n",
    "            },\n",
    "            \"dropout\": 0.25\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"encoder\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"input_size\": 1024+128+768,\n",
    "      \"hidden_size\": 200,\n",
    "      \"num_layers\": 2,\n",
    "      \"dropout\": 0.5,\n",
    "      \"bidirectional\": true\n",
    "    },\n",
    "    \"regularizer\": [\n",
    "            [\n",
    "                \"scalar_parameters\",\n",
    "                {\n",
    "                    \"alpha\": 0.01,\n",
    "                    \"type\": \"l2\"\n",
    "                }\n",
    "            ]\n",
    "    ]\n",
    "  },\n",
    "  \"iterator\": {\n",
    "    \"type\": \"basic\",\n",
    "    \"batch_size\": 2\n",
    "  },\n",
    "  \"trainer\": {\n",
    "        \"optimizer\": {\n",
    "            \"type\": \"bert_adam\",\n",
    "            \"lr\": 0.001\n",
    "        },\n",
    "    \"validation_metric\": \"+f1-measure-overall\",\n",
    "    \"num_serialized_models_to_keep\": 3,\n",
    "    \"num_epochs\": 20,\n",
    "    \"grad_norm\": 5.0,\n",
    "    \"patience\": 3,\n",
    "    \"cuda_device\": 0\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../tony/code/contextual_embeddings/expes.sh\n",
    "\n",
    "# usage\n",
    "#  sh expes.sh dataset config model\n",
    "\n",
    "echo \"data=$1, config=$2, model=$3\"\n",
    "   \n",
    "export DATASET=${1}\n",
    "# eg \"eng.rst.gum\"\n",
    "\n",
    "export CONFIG=${2}\n",
    "# options: conll tok split.tok wend.tok\n",
    "#\n",
    "export MODEL=${3}\n",
    "#options: bert elmo bertM\n",
    "\n",
    "if [ \"$MODEL\"=\"bert\" ] ; \n",
    "then \n",
    "    export BERT_VOCAB=\"bert-base-multilingual-cased\"\n",
    "    export BERT_WEIGHTS=\"bert-base-multilingual-cased\"\n",
    "# else\n",
    "#     # english models\n",
    "#     #export BERT_VOCAB=\"bert-base-cased\"\n",
    "#     #export BERT_WEIGHTS=\"bert-base-cased\"   \n",
    "    \n",
    "fi\n",
    "\n",
    "if [ \"$MODEL\"=\"bertM_crf\" ] ; \n",
    "then \n",
    "    export BERT_VOCAB=\"bert-base-multilingual-cased\"\n",
    "    export BERT_WEIGHTS=\"bert-base-multilingual-cased\"    \n",
    "fi\n",
    "\n",
    "if [ \"$MODEL\"=\"rubert\" ] ;\n",
    "then\n",
    "    #export BERT_VOCAB=\"http://files.deeppavlov.ai/deeppavlov_data/bert/rubert_cased_L-12_H-768_A-12_pt.tar.gz\"\n",
    "    #export BERT_WEIGHTS=\"http://files.deeppavlov.ai/deeppavlov_data/bert/rubert_cased_L-12_H-768_A-12_pt.tar.gz\"\n",
    "    export BERT_VOCAB=\"rubert_cased_L-12_H-768_A-12_pt\"\n",
    "    export BERT_WEIGHTS=\"rubert_cased_L-12_H-768_A-12_pt\"\n",
    "fi\n",
    "\n",
    "# dev ou test\n",
    "export EVAL=dev\n",
    "#export EVAL=test\n",
    "\n",
    "export GOLD_BASE=\"../../data/\"\n",
    "export CONV=\"data_converted/\"\n",
    "export TRAIN_DATA_PATH=${CONV}${DATASET}\"_train.ner.\"${CONFIG}\n",
    "export TEST_A_PATH=${CONV}${DATASET}\"_\"${EVAL}\".ner.\"${CONFIG}\n",
    "export OUTPUT=${DATASET}\"_\"${MODEL}\n",
    "export GOLD=${GOLD_BASE}${DATASET}\"/\"${DATASET}\"_\"${EVAL}\".\"${CONFIG}\n",
    "\n",
    "# conversion des datasets au format NER/BIO  en testant d'abord existence fichiers pour pas le refaire à chaque fois\n",
    "if [ ! -f ${CONV}${DATASET}\"_train.ner.\"${CONFIG} ]; then\n",
    "    echo \"converting to ner format -> in data_converted ...\"\n",
    "    python conv2ner.py \"../../data/\"${DATASET}\"/\"${DATASET}\"_train.\"${CONFIG} > ${CONV}/${DATASET}\"_train.ner.\"${CONFIG}\n",
    "    python conv2ner.py \"../../data/\"${DATASET}\"/\"${DATASET}\"_test.\"${CONFIG} > ${CONV}/${DATASET}\"_test.ner.\"${CONFIG}\n",
    "    python conv2ner.py \"../../data/\"${DATASET}\"/\"${DATASET}\"_dev.\"${CONFIG} > ${CONV}/${DATASET}\"_dev.ner.\"${CONFIG}\n",
    "fi\n",
    "\n",
    "#python conv2ner.py \"../../data/\"${DATASET}\"/\"${DATASET}\"_\"${EVAL}\".\"${CONFIG} > ${CONV}/${DATASET}\"_\"${EVAL}\".ner.\"${CONFIG}\n",
    "# train with config in ner_elmo ou ner_bert.jsonnet; the config references explicitely variables TRAIN_DATA_PATH and TEST_A_PATH\n",
    "allennlp train -s Results_${CONFIG}/results_${OUTPUT} configs/${MODEL}.jsonnet\n",
    "\n",
    "# predict with model -> outputs json\n",
    "allennlp predict --use-dataset-reader --silent --output-file Results_${CONFIG}/results_${OUTPUT}/${DATASET}_${EVAL}.predictions.json Results_${CONFIG}/results_${OUTPUT}/model.tar.gz ${TEST_A_PATH}\n",
    "# convert to disrpt format \n",
    "python json2conll.py Results_${CONFIG}/results_${OUTPUT}/${DATASET}_${EVAL}.predictions.json ${CONFIG} > Results_${CONFIG}/results_${OUTPUT}/${DATASET}_${EVAL}.predictions.${CONFIG}\n",
    "# eval with disrpt script\n",
    "python ../utils/seg_eval.py $GOLD Results_${CONFIG}/results_${OUTPUT}/${DATASET}_${EVAL}.predictions.${CONFIG} >> Results_${CONFIG}/results_${OUTPUT}/${DATASET}_${EVAL}.scores\n",
    "\n",
    "export EVAL=test\n",
    "export TEST_A_PATH=${CONV}${DATASET}\"_\"${EVAL}\".ner.\"${CONFIG}\n",
    "export OUTPUT=${DATASET}\"_\"${MODEL}\n",
    "export GOLD=${GOLD_BASE}${DATASET}\"/\"${DATASET}\"_\"${EVAL}\".\"${CONFIG}\n",
    "allennlp predict --use-dataset-reader --silent --output-file Results_${CONFIG}/results_${OUTPUT}/${DATASET}_${EVAL}.predictions.json Results_${CONFIG}/results_${OUTPUT}/model.tar.gz ${TEST_A_PATH}\n",
    "#convert to disrpt format \n",
    "python json2conll.py Results_${CONFIG}/results_${OUTPUT}/${DATASET}_${EVAL}.predictions.json ${CONFIG} > Results_${CONFIG}/results_${OUTPUT}/${DATASET}_${EVAL}.predictions.${CONFIG}\n",
    "# eval with disrpt script\n",
    "python ../utils/seg_eval.py $GOLD Results_${CONFIG}/results_${OUTPUT}/${DATASET}_${EVAL}.predictions.${CONFIG} >> Results_${CONFIG}/results_${OUTPUT}/${DATASET}_${EVAL}.scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../tony/code/contextual_embeddings/train_tony.sh\n",
    "\n",
    "sh expes.sh rus.rst.rrt conll bertM\n",
    "sh expes.sh rus.rst.rrt conll bertM_crf\n",
    "sh expes.sh rus.rst.rrt conll elmo\n",
    "sh expes.sh rus.rst.rrt conll stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to contextual_embeddings path and run ``train_tony.sh``. \n",
    "\n",
    "Trained models along with evaluations appear in the path: ``tony/code/contextual_embeddings/Results_conll``"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
