{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Binary structure classification used in tree building: Step 1. Negative samples generation\n",
    "\n",
    "Create train and test sets; Save negative samples of file ``filename.rs3`` as `filename.neg`\n",
    "\n",
    "Output:\n",
    " - ``data/*.neg``\n",
    " - ``data_structure/*``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile symbol_map.py\n",
    "\n",
    "SYMBOL_MAP = {\n",
    "    'â€”': '-',\n",
    "    'â€œ': 'Â«',\n",
    "    'â€˜': 'Â«',\n",
    "    'â€': 'Â»',\n",
    "    'â€™': 'Â»',\n",
    "    'ðŸ˜†': 'ðŸ˜„',\n",
    "    'ðŸ˜Š': 'ðŸ˜„',\n",
    "    'ðŸ˜‘': 'ðŸ˜„',\n",
    "    'ðŸ˜”': 'ðŸ˜„',\n",
    "    'ðŸ˜‰': 'ðŸ˜„',\n",
    "    'â—': 'ðŸ˜„',\n",
    "    'ðŸ¤”': 'ðŸ˜„',\n",
    "    'ðŸ˜…': 'ðŸ˜„',\n",
    "    'âš“': 'ðŸ˜„',\n",
    "    'Îµ': 'Î±',\n",
    "    'Î¶': 'Î±',\n",
    "    'Î·': 'Î±',\n",
    "    'Î¼': 'Î±',\n",
    "    'Î´': 'Î±',\n",
    "    'Î»': 'Î±',\n",
    "    'Î½': 'Î±',\n",
    "    'Î²': 'Î±',\n",
    "    'Î³': 'Î±',\n",
    "    'ã¨': 'å°‹',\n",
    "    'ã®': 'å°‹',\n",
    "    'ç¥ž': 'å°‹',\n",
    "    'éš ': 'å°‹',\n",
    "    'ã—': 'å°‹',\n",
    "    'Ã¨': 'e',\n",
    "    'Ä•': 'e',\n",
    "    'Ã§': 'c',\n",
    "    'Ò«': 'c',\n",
    "    'Ñ‘': 'Ðµ',\n",
    "    'Ð': 'Ð•',\n",
    "    u'Ãº': 'u',\n",
    "    u'ÃŽ': 'I',\n",
    "    u'Ã‡': 'C',\n",
    "    u'Òª': 'C',\n",
    "    'Â£': '$',\n",
    "    'â‚½': '$',\n",
    "    'Ó‘': 'a',\n",
    "    'Ä‚': 'A',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../../')\n",
    "\n",
    "import pandas as pd\n",
    "from isanlp.annotation_rst import DiscourseUnit\n",
    "from _isanlp_rst.src.isanlp_rst.rst_tree_predictor import RSTTreePredictor, GoldTreePredictor\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from utils.file_reading import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RandomNegativeGenerator(object):\n",
    "    def __call__(self, edus, corpus, annot_text):\n",
    "        new_set = self.create_training_set(edus, corpus)\n",
    "        result = []\n",
    "        for item in new_set:\n",
    "            result.append((filename, item[0], item[1], item[2]))\n",
    "\n",
    "        tmp = pd.DataFrame(result, columns=['filename', 'snippet_x', 'snippet_y', 'relation'])\n",
    "\n",
    "        def place_locations(row):\n",
    "            row['loc_x'] = annot_text.find(row.snippet_x)\n",
    "            row['loc_y'] = annot_text.find(row.snippet_y, row['loc_x'] + 1)\n",
    "            return row\n",
    "\n",
    "        return tmp.apply(place_locations, axis=1)\n",
    "    \n",
    "    def __name__(self):\n",
    "        return 'RandomNegativeGenerator'\n",
    "    \n",
    "    def create_training_set(self, edus, gold):\n",
    "        training_set = []\n",
    "        \n",
    "        snippet_cache = []\n",
    "        for num, e in enumerate(gold.index):\n",
    "            snippet_x = gold.loc[e, 'snippet_x']\n",
    "            cache_x = self.extract_snippet_ids(snippet_x, edus)\n",
    "\n",
    "            snippet_y = gold.loc[e, 'snippet_y']\n",
    "            cache_y = self.extract_snippet_ids(snippet_y, edus)\n",
    "\n",
    "            if cache_x and cache_y:\n",
    "                snippet_cache.append((cache_x, snippet_x))\n",
    "                snippet_cache.append((cache_y, snippet_y))\n",
    "\n",
    "        for i in range(len(edus) - 1):\n",
    "            if not self.check_snippet_pair_in_dataset(gold, edus[i], edus[i+1]):\n",
    "                training_set.append((edus[i], edus[i+1], False))\n",
    "\n",
    "        for i in gold.index:\n",
    "            training_set += self.extract_negative_samples_for_snippet(gold, edus, gold.loc[i, 'snippet_x'])\n",
    "            training_set += self.extract_negative_samples_for_snippet(gold, edus, gold.loc[i, 'snippet_y'])\n",
    "\n",
    "        for i in range(len(snippet_cache)):\n",
    "            for j in range(i, len(snippet_cache)):\n",
    "                cache_i, snippet_i = snippet_cache[i]\n",
    "                cache_j, snippet_j = snippet_cache[j]\n",
    "\n",
    "                if cache_i[-1] + 1 == cache_j[0]:\n",
    "                    if not self.check_snippet_pair_in_dataset(gold, snippet_i, snippet_j):\n",
    "                        training_set.append((snippet_i, snippet_j, False))\n",
    "\n",
    "                if cache_j[-1] + 1 == cache_i[0]:\n",
    "                    if not self.check_snippet_pair_in_dataset(gold, snippet_j, snippet_i):\n",
    "                        training_set.append((snippet_j, snippet_i, False))\n",
    "\n",
    "        return list(set(training_set))\n",
    "    \n",
    "    def extract_snippet_ids(self, snippet, edus):\n",
    "        return [edu_nm for edu_nm, edu in enumerate(edus) if (edu in snippet)]\n",
    "    \n",
    "    def check_snippet_pair_in_dataset(self, dataset, snippet_left, snippet_right):\n",
    "        return ((((dataset.snippet_x == snippet_left) & (dataset.snippet_y == snippet_right)).sum(axis=0) != 0) \n",
    "                or ((dataset.snippet_y == snippet_left) & (dataset.snippet_x == snippet_right)).sum(axis=0) != 0)\n",
    "    \n",
    "    def extract_negative_samples_for_snippet(self, gold, edus, snippet):\n",
    "        training_set = []\n",
    "\n",
    "        snippet_ids = self.extract_snippet_ids(snippet, edus)\n",
    "\n",
    "        if not snippet_ids:\n",
    "            return []\n",
    "\n",
    "        if snippet_ids[0] > 0:\n",
    "            if not self.check_snippet_pair_in_dataset(gold, snippet, edus[snippet_ids[0] - 1]):\n",
    "                training_set.append((edus[snippet_ids[0] - 1], snippet, False))\n",
    "\n",
    "        if snippet_ids[-1] < len(edus) - 1:\n",
    "            if not self.check_snippet_pair_in_dataset(gold, snippet, edus[snippet_ids[-1] + 1]):\n",
    "                training_set.append((snippet, edus[snippet_ids[-1] + 1], False))\n",
    "\n",
    "        return training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class GreedyNegativeGenerator:\n",
    "    \"\"\" Inversed greedy parser based on gold tree predictor. \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.forest_threshold = 0.01\n",
    "        self._same_sentence_bonus = 0\n",
    "\n",
    "    def __call__(self, edus, corpus,\n",
    "                 annot_text, annot_tokens,\n",
    "                 annot_sentences,\n",
    "                 annot_lemma, annot_morph, annot_postag,\n",
    "                 annot_syntax_dep_tree):\n",
    "\n",
    "        def to_merge(scores):\n",
    "            return np.argmax(np.array(scores))\n",
    "\n",
    "        negative_nodes = []\n",
    "\n",
    "        self.tree_predictor = GoldTreePredictor(corpus)\n",
    "        nodes = edus\n",
    "        max_id = edus[-1].id\n",
    "\n",
    "        # initialize scores\n",
    "        features = self.tree_predictor.initialize_features(nodes,\n",
    "                                                           annot_text, annot_tokens,\n",
    "                                                           annot_sentences,\n",
    "                                                           annot_lemma, annot_morph, annot_postag,\n",
    "                                                           annot_syntax_dep_tree)\n",
    "\n",
    "        scores = self.tree_predictor.predict_pair_proba(features, _same_sentence_bonus=self._same_sentence_bonus)\n",
    "\n",
    "        for i, score in enumerate(scores):\n",
    "            if score == 0:\n",
    "                negative_nodes.append(\n",
    "                    DiscourseUnit(\n",
    "                        id=None,\n",
    "                        left=nodes[i],\n",
    "                        right=nodes[i + 1],\n",
    "                        relation='no_relation',\n",
    "                        nuclearity='NN',\n",
    "                        proba=score,\n",
    "                        text=annot_text[nodes[i].start:nodes[i + 1].end].strip()\n",
    "                    ))\n",
    "\n",
    "        while len(nodes) > 2 and any([score > self.forest_threshold for score in scores]):\n",
    "            # select two nodes to merge\n",
    "            j = to_merge(scores)  # position of the pair in list\n",
    "\n",
    "            # make the new node by merging node[j] + node[j+1]\n",
    "            relation = self.tree_predictor.predict_label(features.iloc[j])\n",
    "            relation, nuclearity = relation.split('_')\n",
    "\n",
    "            temp = DiscourseUnit(\n",
    "                id=max_id + 1,\n",
    "                left=nodes[j],\n",
    "                right=nodes[j + 1],\n",
    "                relation=relation,\n",
    "                nuclearity=nuclearity,\n",
    "                proba=scores[j],\n",
    "                text=annot_text[nodes[j].start:nodes[j + 1].end].strip()\n",
    "            )\n",
    "\n",
    "            max_id += 1\n",
    "\n",
    "            # modify the node list\n",
    "            nodes = nodes[:j] + [temp] + nodes[j + 2:]\n",
    "\n",
    "            # modify the scores list\n",
    "            if j == 0:\n",
    "                _features = self.tree_predictor.extract_features(nodes[j], nodes[j + 1],\n",
    "                                                                 annot_text, annot_tokens,\n",
    "                                                                 annot_sentences,\n",
    "                                                                 annot_lemma, annot_morph, annot_postag,\n",
    "                                                                 annot_syntax_dep_tree)\n",
    "\n",
    "                _scores = self.tree_predictor.predict_pair_proba(_features,\n",
    "                                                                 _same_sentence_bonus=self._same_sentence_bonus)\n",
    "                scores = _scores + scores[j + 2:]\n",
    "                features = pd.concat([_features, features.iloc[j + 2:]])\n",
    "\n",
    "                if _scores[0] == 0:\n",
    "                    negative_nodes.append(\n",
    "                        DiscourseUnit(\n",
    "                            id=None,\n",
    "                            left=nodes[j],\n",
    "                            right=nodes[j + 1],\n",
    "                            relation='no_relation',\n",
    "                            nuclearity='NN',\n",
    "                            proba=_scores[0],\n",
    "                            text=annot_text[nodes[j].start:nodes[j + 1].end].strip()\n",
    "                        ))\n",
    "\n",
    "            elif j + 1 < len(nodes):\n",
    "                _features = self.tree_predictor.initialize_features([nodes[j - 1], nodes[j], nodes[j + 1]],\n",
    "                                                                    annot_text, annot_tokens,\n",
    "                                                                    annot_sentences,\n",
    "                                                                    annot_lemma, annot_morph, annot_postag,\n",
    "                                                                    annot_syntax_dep_tree)\n",
    "\n",
    "                _scores = self.tree_predictor.predict_pair_proba(_features,\n",
    "                                                                 _same_sentence_bonus=self._same_sentence_bonus)\n",
    "                features = pd.concat([features.iloc[:j - 1], _features, features.iloc[j + 2:]])\n",
    "                scores = scores[:j - 1] + _scores + scores[j + 2:]\n",
    "\n",
    "                if _scores[0] == 0:\n",
    "                    negative_nodes.append(\n",
    "                        DiscourseUnit(\n",
    "                            id=None,\n",
    "                            left=nodes[j - 1],\n",
    "                            right=nodes[j],\n",
    "                            relation='no_relation',\n",
    "                            nuclearity='NN',\n",
    "                            proba=_scores[0],\n",
    "                            text=annot_text[nodes[j - 1].start:nodes[j].end].strip()\n",
    "                        ))\n",
    "\n",
    "                if _scores[1] == 0:\n",
    "                    negative_nodes.append(\n",
    "                        DiscourseUnit(\n",
    "                            id=None,\n",
    "                            left=nodes[j],\n",
    "                            right=nodes[j + 1],\n",
    "                            relation='no_relation',\n",
    "                            nuclearity='NN',\n",
    "                            proba=_scores[1],\n",
    "                            text=annot_text[nodes[j].start:nodes[j + 1].end].strip()\n",
    "                        ))\n",
    "\n",
    "            else:\n",
    "                _features = self.tree_predictor.extract_features(nodes[j - 1], nodes[j],\n",
    "                                                                 annot_text, annot_tokens,\n",
    "                                                                 annot_sentences,\n",
    "                                                                 annot_lemma, annot_morph, annot_postag,\n",
    "                                                                 annot_syntax_dep_tree)\n",
    "\n",
    "                _scores = self.tree_predictor.predict_pair_proba(_features,\n",
    "                                                                 _same_sentence_bonus=self._same_sentence_bonus)\n",
    "                scores = scores[:j - 1] + _scores\n",
    "                features = pd.concat([features.iloc[:j - 1], _features])\n",
    "\n",
    "                if _scores[0] == 0:\n",
    "                    negative_nodes.append(\n",
    "                        DiscourseUnit(\n",
    "                            id=None,\n",
    "                            left=nodes[j - 1],\n",
    "                            right=nodes[j],\n",
    "                            relation='no_relation',\n",
    "                            nuclearity='NN',\n",
    "                            proba=_scores,\n",
    "                            text=annot_text[nodes[j - 1].start:nodes[j].end].strip()\n",
    "                        ))\n",
    "\n",
    "        return negative_nodes\n",
    "\n",
    "    def __name__(self):\n",
    "        return 'GreedyNegativeGenerator'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Make negative samples, save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gen1 = RandomNegativeGenerator()\n",
    "## gen2 = GreedyNegativeGenerator()\n",
    "\n",
    "paths = ['./data_ru/*.gold.pkl']\n",
    "for path in paths:\n",
    "    for filename in tqdm(glob.glob(path)):\n",
    "        filename = filename.replace('.gold.pkl', '')\n",
    "        df = read_gold(filename, features=True)\n",
    "        edus = read_edus(filename)\n",
    "        \n",
    "        annot = read_annotation(filename)\n",
    "\n",
    "        tmp = gen1(edus, df, annot['text'])\n",
    "\n",
    "        tmp = tmp[(tmp.loc_x < tmp.loc_y) & (tmp.loc_x > -1)]\n",
    "\n",
    "        tt = pd.concat([df, tmp])\n",
    "        tt['relation'] = tt.relation.map(lambda row: False if row == False else True)\n",
    "        tt = tt.sort_values('relation', ascending=False).drop_duplicates(\n",
    "            ['filename', 'snippet_x', 'snippet_y'])\n",
    "        tmp = tt[tt.relation == False]\n",
    "\n",
    "        tmp.drop_duplicates(['snippet_x', 'snippet_y']).reset_index()[\n",
    "            ['filename', 'snippet_x', 'snippet_y', 'relation', 'loc_x', 'loc_y']].to_json(filename + '.json.neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp.shape  # > 11 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from _isanlp_rst.src.isanlp_rst.features_processor_default import FeaturesProcessor\n",
    "\n",
    "features_processor = FeaturesProcessor(model_dir_path='../../models', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "MAX_LEN = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "paths = ['data_ru/*.json.neg']#, 'dep_data/*.json.neg']\n",
    "for path in paths:\n",
    "    for filename in tqdm(glob.glob(path)):    \n",
    "        filename = filename.replace('.json.neg', '')\n",
    "\n",
    "        df = read_negative(filename).drop(columns=['loc_y'])\n",
    "        df = df[df.snippet_x.str.len() > 0]\n",
    "        df = df[df.snippet_y.str.len() > 0]\n",
    "\n",
    "        annotation = read_annotation(filename)\n",
    "\n",
    "        result = features_processor(df,\n",
    "                                    annotation['text'],\n",
    "                                    annotation['tokens'],\n",
    "                                    annotation['sentences'],\n",
    "                                    annotation['lemma'],\n",
    "                                    annotation['morph'],\n",
    "                                    annotation['postag'],\n",
    "                                    annotation['syntax_dep_tree'])\n",
    "\n",
    "        result = result[result.is_broken == False]\n",
    "\n",
    "        result = result[result.tokens_x.map(len) < MAX_LEN]\n",
    "        result = result[result.tokens_y.map(len) < MAX_LEN]\n",
    "\n",
    "        result.to_pickle(filename + '.neg.features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Make train/test splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from utils.train_test_split import split_rstreebank, split_essays\n",
    "\n",
    "print('Loading RSTreebank:')\n",
    "train, dev, test = split_rstreebank('./data_ru')\n",
    "print('Train length:', len(train), 'Dev length:', len(dev), 'Test length:', len(test), '(files)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.file_reading import read_gold\n",
    "\n",
    "\n",
    "random_state = 45\n",
    "\n",
    "train_samples = []\n",
    "test_samples = []\n",
    "dev_samples = []\n",
    "\n",
    "for file in tqdm(train):\n",
    "    gold = read_gold(file.replace('.edus', ''), features=True)\n",
    "    gold['relation'] = 1\n",
    "    train_samples.append(gold)\n",
    "    negative = read_negative(file.replace('.edus', ''), features=True)\n",
    "    negative['relation'] = 0\n",
    "    train_samples.append(negative)\n",
    "\n",
    "for file in tqdm(dev):\n",
    "    gold = read_gold(file.replace('.edus', ''), features=True)\n",
    "    gold['relation'] = 1\n",
    "    dev_samples.append(gold)\n",
    "    negative = read_negative(file.replace('.edus', ''), features=True)\n",
    "    negative['relation'] = 0\n",
    "    dev_samples.append(negative)\n",
    "    \n",
    "for file in tqdm(test):\n",
    "    gold = read_gold(file.replace('.edus', ''), features=True)\n",
    "    gold['relation'] = 1\n",
    "    test_samples.append(gold)\n",
    "    negative = read_negative(file.replace('.edus', ''), features=True)\n",
    "    negative['relation'] = 0\n",
    "    test_samples.append(negative)\n",
    "\n",
    "train_samples = pd.concat(train_samples)\n",
    "dev_samples = pd.concat(dev_samples)\n",
    "test_samples = pd.concat(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.prepare_sequence import _prepare_sequence\n",
    "\n",
    "\n",
    "def correct_samples(row):\n",
    "    if row.snippet_x[0] in (',', '.', '!', '?'):\n",
    "        row.snippet_x = row.snippet_x[1:].strip()\n",
    "    if row.snippet_y[0] in (',', '.'):\n",
    "        row.snippet_x += row.snippet_y[0]\n",
    "        row.snippet_y = row.snippet_y[1:].strip()\n",
    "    return row\n",
    "\n",
    "def prepare_data(data, max_len=10000000):\n",
    "    data = data[data.tokens_x.map(len) < max_len]\n",
    "    data = data[data.tokens_y.map(len) < max_len]\n",
    "    \n",
    "    data['snippet_x'] = data.tokens_x.map(lambda row: ' '.join(row))\n",
    "    data['snippet_y'] = data.tokens_y.map(lambda row: ' '.join(row))\n",
    "    \n",
    "    data = data.apply(correct_samples, axis=1)\n",
    "    \n",
    "    data = data[data.snippet_x.map(len) > 0]\n",
    "    data = data[data.snippet_y.map(len) > 0]\n",
    "    \n",
    "    data['snippet_x'] = data.snippet_x.map(_prepare_sequence)\n",
    "    data['snippet_y'] = data.snippet_y.map(_prepare_sequence)\n",
    "    \n",
    "    data = data.sort_values(['relation'], ascending=True).drop_duplicates(['snippet_x', 'snippet_y'], keep='last')\n",
    "    data = data.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "train_samples = prepare_data(train_samples)\n",
    "dev_samples = prepare_data(dev_samples)\n",
    "test_samples = prepare_data(test_samples)\n",
    "\n",
    "OUT_PATH = 'data_structure'\n",
    "if not os.path.isdir(OUT_PATH):\n",
    "    os.mkdir(OUT_PATH)\n",
    "\n",
    "train_samples.to_pickle(os.path.join(OUT_PATH, 'train_samples.pkl'))\n",
    "dev_samples.to_pickle(os.path.join(OUT_PATH, 'dev_samples.pkl'))\n",
    "test_samples.to_pickle(os.path.join(OUT_PATH, 'test_samples.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_samples[['snippet_x', 'snippet_y', 'relation', 'filename']].sort_values('snippet_x').tail(1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_samples['len_x'] = train_samples.snippet_x.map(lambda row: len(row.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_samples.relation.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}