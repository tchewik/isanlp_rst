{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary structure classification used in tree building: Step 1. Negative samples generation\n",
    "\n",
    "Create train and test sets; Save negative samples of file ``filename.rs3`` as `filename.neg`\n",
    "\n",
    "Output:\n",
    " - ``data/*.neg``\n",
    " - ``data_structure/*``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from isanlp.annotation_rst import DiscourseUnit\n",
    "from _isanlp_rst.src.isanlp_rst.rst_tree_predictor import RSTTreePredictor, GoldTreePredictor\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from utils.evaluation import extr_pairs, extr_pairs_forest\n",
    "from utils.file_reading import *\n",
    "from utils.print_tree import printBTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomNegativeGenerator(object):\n",
    "    def __call__(self, edus, corpus, annot_text):\n",
    "        new_set = self.create_training_set(edus, corpus)\n",
    "        result = []\n",
    "        for item in new_set:\n",
    "            result.append((filename, item[0], item[1], item[2]))\n",
    "\n",
    "        tmp = pd.DataFrame(result, columns=['filename', 'snippet_x', 'snippet_y', 'relation'])\n",
    "\n",
    "        def place_locations(row):\n",
    "            row['loc_x'] = annot_text.find(row.snippet_x)\n",
    "            row['loc_y'] = annot_text.find(row.snippet_y, row['loc_x'] + 1)\n",
    "            return row\n",
    "\n",
    "        return tmp.apply(place_locations, axis=1)\n",
    "    \n",
    "    def __name__(self):\n",
    "        return 'RandomNegativeGenerator'\n",
    "    \n",
    "    def create_training_set(self, edus, gold):\n",
    "        training_set = []\n",
    "        \n",
    "        snippet_cache = []\n",
    "        for num, e in enumerate(gold.index):\n",
    "            snippet_x = gold.loc[e, 'snippet_x']\n",
    "            cache_x = self.extract_snippet_ids(snippet_x, edus)\n",
    "\n",
    "            snippet_y = gold.loc[e, 'snippet_y']\n",
    "            cache_y = self.extract_snippet_ids(snippet_y, edus)\n",
    "\n",
    "            if cache_x and cache_y:\n",
    "                snippet_cache.append((cache_x, snippet_x))\n",
    "                snippet_cache.append((cache_y, snippet_y))\n",
    "\n",
    "        for i in range(len(edus) - 1):\n",
    "            if not self.check_snippet_pair_in_dataset(gold, edus[i], edus[i+1]):\n",
    "                training_set.append((edus[i], edus[i+1], False))\n",
    "\n",
    "        for i in gold.index:\n",
    "            training_set += self.extract_negative_samples_for_snippet(gold, edus, gold.loc[i, 'snippet_x'])\n",
    "            training_set += self.extract_negative_samples_for_snippet(gold, edus, gold.loc[i, 'snippet_y'])\n",
    "\n",
    "        for i in range(len(snippet_cache)):\n",
    "            for j in range(i, len(snippet_cache)):\n",
    "                cache_i, snippet_i = snippet_cache[i]\n",
    "                cache_j, snippet_j = snippet_cache[j]\n",
    "\n",
    "                if cache_i[-1] + 1 == cache_j[0]:\n",
    "                    if not self.check_snippet_pair_in_dataset(gold, snippet_i, snippet_j):\n",
    "                        training_set.append((snippet_i, snippet_j, False))\n",
    "\n",
    "                if cache_j[-1] + 1 == cache_i[0]:\n",
    "                    if not self.check_snippet_pair_in_dataset(gold, snippet_j, snippet_i):\n",
    "                        training_set.append((snippet_j, snippet_i, False))\n",
    "\n",
    "        return list(set(training_set))\n",
    "    \n",
    "    def extract_snippet_ids(self, snippet, edus):\n",
    "        return [edu_nm for edu_nm, edu in enumerate(edus) if (edu in snippet)]\n",
    "    \n",
    "    def check_snippet_pair_in_dataset(self, dataset, snippet_left, snippet_right):\n",
    "        return ((((dataset.snippet_x == snippet_left) & (dataset.snippet_y == snippet_right)).sum(axis=0) != 0) \n",
    "                or ((dataset.snippet_y == snippet_left) & (dataset.snippet_x == snippet_right)).sum(axis=0) != 0)\n",
    "    \n",
    "    def extract_negative_samples_for_snippet(self, gold, edus, snippet):\n",
    "        training_set = []\n",
    "\n",
    "        snippet_ids = self.extract_snippet_ids(snippet, edus)\n",
    "\n",
    "        if not snippet_ids:\n",
    "            return []\n",
    "\n",
    "        if snippet_ids[0] > 0:\n",
    "            if not self.check_snippet_pair_in_dataset(gold, snippet, edus[snippet_ids[0] - 1]):\n",
    "                training_set.append((edus[snippet_ids[0] - 1], snippet, False))\n",
    "\n",
    "        if snippet_ids[-1] < len(edus) - 1:\n",
    "            if not self.check_snippet_pair_in_dataset(gold, snippet, edus[snippet_ids[-1] + 1]):\n",
    "                training_set.append((snippet, edus[snippet_ids[-1] + 1], False))\n",
    "\n",
    "        return training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class GreedyNegativeGenerator:\n",
    "    \"\"\" Inversed greedy parser based on gold tree predictor. \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.forest_threshold = 0.01\n",
    "        self._same_sentence_bonus = 0\n",
    "\n",
    "    def __call__(self, edus, corpus,\n",
    "                 annot_text, annot_tokens,\n",
    "                 annot_sentences,\n",
    "                 annot_lemma, annot_morph, annot_postag,\n",
    "                 annot_syntax_dep_tree):\n",
    "\n",
    "        def to_merge(scores):\n",
    "            return np.argmax(np.array(scores))\n",
    "\n",
    "        negative_nodes = []\n",
    "\n",
    "        self.tree_predictor = GoldTreePredictor(corpus)\n",
    "        nodes = edus\n",
    "        max_id = edus[-1].id\n",
    "\n",
    "        # initialize scores\n",
    "        features = self.tree_predictor.initialize_features(nodes,\n",
    "                                                           annot_text, annot_tokens,\n",
    "                                                           annot_sentences,\n",
    "                                                           annot_lemma, annot_morph, annot_postag,\n",
    "                                                           annot_syntax_dep_tree)\n",
    "\n",
    "        scores = self.tree_predictor.predict_pair_proba(features, _same_sentence_bonus=self._same_sentence_bonus)\n",
    "\n",
    "        for i, score in enumerate(scores):\n",
    "            if score == 0:\n",
    "                negative_nodes.append(\n",
    "                    DiscourseUnit(\n",
    "                        id=None,\n",
    "                        left=nodes[i],\n",
    "                        right=nodes[i + 1],\n",
    "                        relation='no_relation',\n",
    "                        nuclearity='NN',\n",
    "                        proba=score,\n",
    "                        text=annot_text[nodes[i].start:nodes[i + 1].end].strip()\n",
    "                    ))\n",
    "\n",
    "        while len(nodes) > 2 and any([score > self.forest_threshold for score in scores]):\n",
    "            # select two nodes to merge\n",
    "            j = to_merge(scores)  # position of the pair in list\n",
    "\n",
    "            # make the new node by merging node[j] + node[j+1]\n",
    "            relation = self.tree_predictor.predict_label(features.iloc[j])\n",
    "            relation, nuclearity = relation.split('_')\n",
    "\n",
    "            temp = DiscourseUnit(\n",
    "                id=max_id + 1,\n",
    "                left=nodes[j],\n",
    "                right=nodes[j + 1],\n",
    "                relation=relation,\n",
    "                nuclearity=nuclearity,\n",
    "                proba=scores[j],\n",
    "                text=annot_text[nodes[j].start:nodes[j + 1].end].strip()\n",
    "            )\n",
    "\n",
    "            max_id += 1\n",
    "\n",
    "            # modify the node list\n",
    "            nodes = nodes[:j] + [temp] + nodes[j + 2:]\n",
    "\n",
    "            # modify the scores list\n",
    "            if j == 0:\n",
    "                _features = self.tree_predictor.extract_features(nodes[j], nodes[j + 1],\n",
    "                                                                 annot_text, annot_tokens,\n",
    "                                                                 annot_sentences,\n",
    "                                                                 annot_lemma, annot_morph, annot_postag,\n",
    "                                                                 annot_syntax_dep_tree)\n",
    "\n",
    "                _scores = self.tree_predictor.predict_pair_proba(_features,\n",
    "                                                                 _same_sentence_bonus=self._same_sentence_bonus)\n",
    "                scores = _scores + scores[j + 2:]\n",
    "                features = pd.concat([_features, features.iloc[j + 2:]])\n",
    "\n",
    "                if _scores[0] == 0:\n",
    "                    negative_nodes.append(\n",
    "                        DiscourseUnit(\n",
    "                            id=None,\n",
    "                            left=nodes[j],\n",
    "                            right=nodes[j + 1],\n",
    "                            relation='no_relation',\n",
    "                            nuclearity='NN',\n",
    "                            proba=_scores[0],\n",
    "                            text=annot_text[nodes[j].start:nodes[j + 1].end].strip()\n",
    "                        ))\n",
    "\n",
    "            elif j + 1 < len(nodes):\n",
    "                _features = self.tree_predictor.initialize_features([nodes[j - 1], nodes[j], nodes[j + 1]],\n",
    "                                                                    annot_text, annot_tokens,\n",
    "                                                                    annot_sentences,\n",
    "                                                                    annot_lemma, annot_morph, annot_postag,\n",
    "                                                                    annot_syntax_dep_tree)\n",
    "\n",
    "                _scores = self.tree_predictor.predict_pair_proba(_features,\n",
    "                                                                 _same_sentence_bonus=self._same_sentence_bonus)\n",
    "                features = pd.concat([features.iloc[:j - 1], _features, features.iloc[j + 2:]])\n",
    "                scores = scores[:j - 1] + _scores + scores[j + 2:]\n",
    "\n",
    "                if _scores[0] == 0:\n",
    "                    negative_nodes.append(\n",
    "                        DiscourseUnit(\n",
    "                            id=None,\n",
    "                            left=nodes[j - 1],\n",
    "                            right=nodes[j],\n",
    "                            relation='no_relation',\n",
    "                            nuclearity='NN',\n",
    "                            proba=_scores[0],\n",
    "                            text=annot_text[nodes[j - 1].start:nodes[j].end].strip()\n",
    "                        ))\n",
    "\n",
    "                if _scores[1] == 0:\n",
    "                    negative_nodes.append(\n",
    "                        DiscourseUnit(\n",
    "                            id=None,\n",
    "                            left=nodes[j],\n",
    "                            right=nodes[j + 1],\n",
    "                            relation='no_relation',\n",
    "                            nuclearity='NN',\n",
    "                            proba=_scores[1],\n",
    "                            text=annot_text[nodes[j].start:nodes[j + 1].end].strip()\n",
    "                        ))\n",
    "\n",
    "            else:\n",
    "                _features = self.tree_predictor.extract_features(nodes[j - 1], nodes[j],\n",
    "                                                                 annot_text, annot_tokens,\n",
    "                                                                 annot_sentences,\n",
    "                                                                 annot_lemma, annot_morph, annot_postag,\n",
    "                                                                 annot_syntax_dep_tree)\n",
    "\n",
    "                _scores = self.tree_predictor.predict_pair_proba(_features,\n",
    "                                                                 _same_sentence_bonus=self._same_sentence_bonus)\n",
    "                scores = scores[:j - 1] + _scores\n",
    "                features = pd.concat([features.iloc[:j - 1], _features])\n",
    "\n",
    "                if _scores[0] == 0:\n",
    "                    negative_nodes.append(\n",
    "                        DiscourseUnit(\n",
    "                            id=None,\n",
    "                            left=nodes[j - 1],\n",
    "                            right=nodes[j],\n",
    "                            relation='no_relation',\n",
    "                            nuclearity='NN',\n",
    "                            proba=_scores,\n",
    "                            text=annot_text[nodes[j - 1].start:nodes[j].end].strip()\n",
    "                        ))\n",
    "\n",
    "        return negative_nodes\n",
    "\n",
    "    def __name__(self):\n",
    "        return 'GreedyNegativeGenerator'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make negative samples, save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7661c39785064b76a72cb5594aeaeec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/233 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gen1 = RandomNegativeGenerator()\n",
    "# gen2 = GreedyNegativeGenerator()\n",
    "\n",
    "for filename in tqdm(glob.glob('./data/*.gold.pkl')):\n",
    "    filename = filename.replace('.gold.pkl', '')\n",
    "    df = read_gold(filename, features=True)\n",
    "    edus = read_edus(filename)\n",
    "    annot = read_annotation(filename)\n",
    "\n",
    "    tmp = gen1(edus, df, annot['text'])\n",
    "#     tmp1 = gen1(edus, df, annot['text'])\n",
    "    \n",
    "#     _edus = []\n",
    "#     last_end = 0\n",
    "#     last_id = 0\n",
    "#     for max_id in range(len(edus)):\n",
    "#         start = len(annot['text'][:last_end]) + annot['text'][last_end:].find(edus[max_id])\n",
    "#         end = start + len(edus[max_id])\n",
    "#         temp = DiscourseUnit(\n",
    "#             id=max_id + last_id,\n",
    "#             left=None,\n",
    "#             right=None,\n",
    "#             relation='edu',\n",
    "#             start=start,\n",
    "#             end=end,\n",
    "#             orig_text=annot['text'],\n",
    "#             proba=1.,\n",
    "#             )\n",
    "#         _edus.append(temp)\n",
    "#         last_end = end\n",
    "#         last_id += 1\n",
    "\n",
    "#     tmp = gen2(_edus, df, annot['text'],\n",
    "#                annot['tokens'], annot['sentences'],\n",
    "#                annot['lemma'], annot['morph'],\n",
    "#                annot['postag'], annot['syntax_dep_tree'])\n",
    "    \n",
    "#     tmp = pd.DataFrame(extr_pairs_forest(tmp, annot['text'], locations=True), \n",
    "#                        columns=['snippet_x', 'snippet_y', \n",
    "#                                 'category_id', 'order', \n",
    "#                                 'loc_x', 'loc_y'])\n",
    "    \n",
    "#     tmp = tmp[tmp.category_id == 'no_relation']\n",
    "#     tmp = tmp.drop(columns=['order', 'category_id'])\n",
    "#     tmp['filename'] = filename\n",
    "#     tmp['relation'] = False\n",
    "    \n",
    "#     tmp = pd.concat([tmp, tmp1])\n",
    "    tmp = tmp[(tmp.loc_x < tmp.loc_y) & (tmp.loc_x > -1)]\n",
    "    \n",
    "    tt = pd.concat([df, tmp])\n",
    "    tt['relation'] = tt.relation.map(lambda row: False if row == False else True)\n",
    "    tt = tt.sort_values('relation', ascending=False).drop_duplicates(\n",
    "        ['filename', 'snippet_x', 'snippet_y'])\n",
    "    tmp = tt[tt.relation == False]\n",
    "    \n",
    "    tmp.drop_duplicates(['snippet_x', 'snippet_y']).reset_index()[\n",
    "        ['filename', 'snippet_x', 'snippet_y', 'relation', 'loc_x', 'loc_y']].to_json(filename + '.json.neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.8 s, sys: 534 ms, total: 37.4 s\n",
      "Wall time: 37.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from _isanlp_rst.src.isanlp_rst.features_processor_default import FeaturesProcessor\n",
    "\n",
    "features_processor = FeaturesProcessor(model_dir_path='models', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8e1efeb9f24ee490a9145bb46c50cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/233 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to locate first snippet >>> [['–∏ –∑–∞–ø—É—Ç–∞–ª–∏—Å—å,' '–∞ –≤–µ–¥—å —ç—Ç–æ –¥–∞–ª–µ–∫–æ –Ω–µ –≤—Å–µ' 955 955 5443 5443]\n",
      " ['–¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏,'\n",
      "  '—á—Ç–æ –æ–ø–∏—Å–∞–Ω–æ –≤ 12.2.6.9 Runtime Semantics: Propert—ÉDefinitionEvaluation.'\n",
      "  913 913 5186 5186]\n",
      " ['AST-–∞–Ω–æ–≤–∏—Ç–µ—Å—å!'\n",
      "  '–ù–∞ –º–æ–π —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω—ã–π –≤–∑–≥–ª—è–¥, –ø–æ –±–æ–ª—å—à–µ–π –º–µ—Ä–µ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è —ç—Ç–æ –ø—Ä–µ–¥–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ç–æ—Ä–∞ EcmaScript, –¥–µ—Ä–∂–∞—Ç—å —Ç–∞–∫–æ–π –∂–µ –≤ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–π –≥–æ–ª–æ–≤–µ –µ—Å—Ç—å –¥–µ–ª–æ —Ç—è–∂–µ–ª–æ–µ –∏ –Ω–µ–±–ª–∞–≥–æ–¥–∞—Ä–Ω–æ–µ.'\n",
      "  1414 1414 7913 7913]\n",
      " ['–¥–ª—è —Ä–∞–Ω–µ–µ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤,' '—Å –ø–æ–º–æ—â—å—é –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ ASTE—Öplorer.' 1483\n",
      "  1483 8408 8409]\n",
      " ['–∏ —Å—Ç–∞–≤–∏–º —Ç–æ—á–∫—É.' '–í–µ—Å—å —ç—Ç–æ—Ç –ø—É—Ç—å –±—ã–ª –ø—Ä–æ–¥–µ–ª–∞–Ω –Ω–µ –∑—Ä—è,' 1045 1045 5975\n",
      "  5975]\n",
      " ['—Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ bind,'\n",
      "  '—á—Ç–æ –æ–ø–∏—Å–∞–Ω–æ –≤ —Ä–∞–∑–¥–µ–ª–µ 19.2.3.2 Function.protot—Épe.bind,' 1361 1361\n",
      "  7628 7628]\n",
      " ['–í—ã–≤–æ–¥' '–ö–∞–∫ –º—ã –≤—ã—è—Å–Ω–∏–ª–∏ —Ñ—É–Ω–∫—Ü–∏—è, –±—É–¥—É—á–∏ –∞–Ω–æ–Ω–∏–º–Ω–æ–π, –º–æ–∂–µ—Ç –∏–º–µ—Ç—å –∏–º—è,'\n",
      "  1495 1495 8471 8471]\n",
      " ['–í—ã–≤–æ–¥'\n",
      "  '–ö–∞–∫ –º—ã –≤—ã—è—Å–Ω–∏–ª–∏ —Ñ—É–Ω–∫—Ü–∏—è, –±—É–¥—É—á–∏ –∞–Ω–æ–Ω–∏–º–Ω–æ–π, –º–æ–∂–µ—Ç –∏–º–µ—Ç—å –∏–º—è, –ø–æ—Å–∫–æ–ª—å–∫—É –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —è–≤–ª—è–µ—Ç—Å—è —Ç–∞–∫–∂–µ –∏ –æ–±—ä–µ–∫—Ç–æ–º, —á—Ç–æ –µ—Å—Ç—å —Å–ª–µ–¥—Å—Ç–≤–∏–µ –º—É–ª—å—Ç–∏–ø–∞—Ä–∞–¥–∏–≥–º–∞–ª—å–Ω–æ–π –ø—Ä–∏—Ä–æ–¥—ã —è–∑—ã–∫–∞ JavaScript.'\n",
      "  1495 1495 8471 8471]\n",
      " ['–∏ –∑–∞–ø—É—Ç–∞–ª–∏—Å—å,'\n",
      "  '–∞ –≤–µ–¥—å —ç—Ç–æ –¥–∞–ª–µ–∫–æ –Ω–µ –≤—Å–µ –∏ —è –æ–ø—É—Å—Ç–∏–ª —á–∞—Å—Ç—å –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –º–µ–∂–¥—É —Ä–∞–∑–¥–µ–ª–∞–º–∏ –∏ –ø—É–Ω–∫—Ç–∞–º–∏ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏.'\n",
      "  955 955 5443 5443]]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "MAX_LEN = 10000\n",
    "for filename in tqdm(glob.glob(\"data/*.json.neg\")):    \n",
    "    filename = filename.replace('.json.neg', '')\n",
    "    \n",
    "    df = read_negative(filename).drop(columns=['loc_y'])\n",
    "    df = df[df.snippet_x.str.len() > 0]\n",
    "    df = df[df.snippet_y.str.len() > 0]\n",
    "    \n",
    "    annotation = read_annotation(filename)\n",
    "        \n",
    "    result = features_processor(df,\n",
    "                                annotation['text'],\n",
    "                                annotation['tokens'],\n",
    "                                annotation['sentences'],\n",
    "                                annotation['lemma'],\n",
    "                                annotation['morph'],\n",
    "                                annotation['postag'],\n",
    "                                annotation['syntax_dep_tree'])\n",
    "    \n",
    "    result = result[result.is_broken == False]\n",
    "    \n",
    "    result = result[result.tokens_x.map(len) < MAX_LEN]\n",
    "    result = result[result.tokens_y.map(len) < MAX_LEN]\n",
    "    \n",
    "    result.to_pickle(filename + '.neg.features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make train/test splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news in train: 0.5344827586206896,\tin dev: 0.6470588235294118,\tin test: 0.6086956521739131\n",
      "ling in train: 0.0,\tin dev: 0.0,\tin test: 0.0\n",
      "comp in train: 0.0,\tin dev: 0.0,\tin test: 0.0\n",
      "blog in train: 0.43103448275862066,\tin dev: 0.5294117647058824,\tin test: 0.4782608695652174\n"
     ]
    }
   ],
   "source": [
    "from utils.train_test_split import split_train_dev_test\n",
    "\n",
    "train, dev, test = split_train_dev_test('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709b413153e94762a9f51cd80c1bf006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa126f271e6a48cc93cd252356496884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260a27f28d84415c8e9b05aee1d0d6cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils.file_reading import read_gold\n",
    "\n",
    "\n",
    "random_state = 45\n",
    "\n",
    "train_samples = []\n",
    "test_samples = []\n",
    "dev_samples = []\n",
    "\n",
    "for file in tqdm(train):\n",
    "    gold = read_gold(file.replace('.edus', ''), features=True)\n",
    "    gold['relation'] = 1\n",
    "    train_samples.append(gold)\n",
    "    negative = read_negative(file.replace('.edus', ''), features=True)\n",
    "    negative['relation'] = 0\n",
    "    train_samples.append(negative)\n",
    "\n",
    "for file in tqdm(dev):\n",
    "    gold = read_gold(file.replace('.edus', ''), features=True)\n",
    "    gold['relation'] = 1\n",
    "    dev_samples.append(gold)\n",
    "    negative = read_negative(file.replace('.edus', ''), features=True)\n",
    "    negative['relation'] = 0\n",
    "    dev_samples.append(negative)\n",
    "    \n",
    "for file in tqdm(test):\n",
    "    gold = read_gold(file.replace('.edus', ''), features=True)\n",
    "    gold['relation'] = 1\n",
    "    test_samples.append(gold)\n",
    "    negative = read_negative(file.replace('.edus', ''), features=True)\n",
    "    negative['relation'] = 0\n",
    "    test_samples.append(negative)\n",
    "\n",
    "train_samples = pd.concat(train_samples)\n",
    "dev_samples = pd.concat(dev_samples)\n",
    "test_samples = pd.concat(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.prepare_sequence import _prepare_sequence\n",
    "\n",
    "\n",
    "def correct_samples(row):\n",
    "    if row.snippet_x[0] in (',', '.', '!', '?'):\n",
    "        row.snippet_x = row.snippet_x[1:].strip()\n",
    "    if row.snippet_y[0] in (',', '.'):\n",
    "        row.snippet_x += row.snippet_y[0]\n",
    "        row.snippet_y = row.snippet_y[1:].strip()\n",
    "    return row\n",
    "\n",
    "def prepare_data(data, max_len=100):\n",
    "\n",
    "    data = data[data.tokens_x.map(len) < max_len]\n",
    "    data = data[data.tokens_y.map(len) < max_len]\n",
    "    \n",
    "    data['snippet_x'] = data.tokens_x.map(lambda row: ' '.join(row))\n",
    "    data['snippet_y'] = data.tokens_y.map(lambda row: ' '.join(row))\n",
    "    \n",
    "    data = data.apply(correct_samples, axis=1)\n",
    "    \n",
    "    data = data[data.snippet_x.map(len) > 0]\n",
    "    data = data[data.snippet_y.map(len) > 0]\n",
    "    \n",
    "    data['snippet_x'] = data.snippet_x.map(_prepare_sequence)\n",
    "    data['snippet_y'] = data.snippet_y.map(_prepare_sequence)\n",
    "    \n",
    "    data = data.sort_values(['relation'], ascending=True).drop_duplicates(['snippet_x', 'snippet_y'], keep='last')\n",
    "    data = data.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "train_samples = prepare_data(train_samples)\n",
    "dev_samples = prepare_data(dev_samples)\n",
    "test_samples = prepare_data(test_samples)\n",
    "\n",
    "OUT_PATH = 'data_structure'\n",
    "if not os.path.isdir(OUT_PATH):\n",
    "    os.path.mkdir(OUT_PATH)\n",
    "\n",
    "train_samples.to_pickle(os.path.join(OUT_PATH, 'train_samples.pkl'))\n",
    "dev_samples.to_pickle(os.path.join(OUT_PATH, 'dev_samples.pkl'))\n",
    "test_samples.to_pickle(os.path.join(OUT_PATH, 'test_samples.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snippet_x</th>\n",
       "      <th>snippet_y</th>\n",
       "      <th>relation</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6658</th>\n",
       "      <td>Ôªø–ù–æ–≤–æ—Å—Ç—å : –ó–∞–Ω—è—Ç–∏—è –≤ –í–æ—Å–∫—Ä–µ—Å–Ω–æ–π —à–∫–æ–ª–µ</td>\n",
       "      <td>–í –∫–∞–∂–¥–æ–µ –≤–æ—Å–∫—Ä–µ—Å–µ–Ω–∏–µ , —É—Ç—Ä–æ–º , –ø—Ä–∏—Ö–æ–∂–∞–Ω–µ –°–≤—è—Ç–æ...</td>\n",
       "      <td>0</td>\n",
       "      <td>./data/news2_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19489</th>\n",
       "      <td>Ôªø–ù–æ–≤–æ—Å—Ç—å : –ó–∞–Ω—è—Ç–∏—è –≤ –í–æ—Å–∫—Ä–µ—Å–Ω–æ–π —à–∫–æ–ª–µ</td>\n",
       "      <td>–í –∫–∞–∂–¥–æ–µ –≤–æ—Å–∫—Ä–µ—Å–µ–Ω–∏–µ , —É—Ç—Ä–æ–º , –ø—Ä–∏—Ö–æ–∂–∞–Ω–µ –°–≤—è—Ç–æ...</td>\n",
       "      <td>0</td>\n",
       "      <td>./data/news2_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41053</th>\n",
       "      <td>Ôªø–ù–æ–≤–æ—Å—Ç—å : –ó–∞–Ω—è—Ç–∏—è –≤ –í–æ—Å–∫—Ä–µ—Å–Ω–æ–π —à–∫–æ–ª–µ</td>\n",
       "      <td>–í –∫–∞–∂–¥–æ–µ –≤–æ—Å–∫—Ä–µ—Å–µ–Ω–∏–µ , —É—Ç—Ä–æ–º , –ø—Ä–∏—Ö–æ–∂–∞–Ω–µ –°–≤—è—Ç–æ...</td>\n",
       "      <td>0</td>\n",
       "      <td>./data/news2_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41443</th>\n",
       "      <td>üòÑÔ∏è–ò–≥–æ—Ä—å –ú–æ—Ä—Å–∫–æ–πüòÑÔ∏èüá∑üá∫ ( @ 812 Hocke —É ) April 18...</td>\n",
       "      <td>–ß—Ç–æ-—Ç–æ –Ω–µ –æ—á–µ–Ω—å-—Ç–æ —Ä–∞–∑–æ–±—Ä–∞–ª–∏—Å—å</td>\n",
       "      <td>0</td>\n",
       "      <td>./data/blogs_88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16720</th>\n",
       "      <td>üòÑÔ∏è–ò–≥–æ—Ä—å –ú–æ—Ä—Å–∫–æ–πüòÑÔ∏èüá∑üá∫ ( @ 812 Hocke —É ) April 18...</td>\n",
       "      <td>–ß—Ç–æ-—Ç–æ –Ω–µ –æ—á–µ–Ω—å-—Ç–æ —Ä–∞–∑–æ–±—Ä–∞–ª–∏—Å—å ! –ì–æ—Ä–æ–¥ –≤ –ø–æ–ª–Ω–æ...</td>\n",
       "      <td>0</td>\n",
       "      <td>./data/blogs_88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               snippet_x  \\\n",
       "6658               Ôªø–ù–æ–≤–æ—Å—Ç—å : –ó–∞–Ω—è—Ç–∏—è –≤ –í–æ—Å–∫—Ä–µ—Å–Ω–æ–π —à–∫–æ–ª–µ   \n",
       "19489              Ôªø–ù–æ–≤–æ—Å—Ç—å : –ó–∞–Ω—è—Ç–∏—è –≤ –í–æ—Å–∫—Ä–µ—Å–Ω–æ–π —à–∫–æ–ª–µ   \n",
       "41053              Ôªø–ù–æ–≤–æ—Å—Ç—å : –ó–∞–Ω—è—Ç–∏—è –≤ –í–æ—Å–∫—Ä–µ—Å–Ω–æ–π —à–∫–æ–ª–µ   \n",
       "41443  üòÑÔ∏è–ò–≥–æ—Ä—å –ú–æ—Ä—Å–∫–æ–πüòÑÔ∏èüá∑üá∫ ( @ 812 Hocke —É ) April 18...   \n",
       "16720  üòÑÔ∏è–ò–≥–æ—Ä—å –ú–æ—Ä—Å–∫–æ–πüòÑÔ∏èüá∑üá∫ ( @ 812 Hocke —É ) April 18...   \n",
       "\n",
       "                                               snippet_y  relation  \\\n",
       "6658   –í –∫–∞–∂–¥–æ–µ –≤–æ—Å–∫—Ä–µ—Å–µ–Ω–∏–µ , —É—Ç—Ä–æ–º , –ø—Ä–∏—Ö–æ–∂–∞–Ω–µ –°–≤—è—Ç–æ...         0   \n",
       "19489  –í –∫–∞–∂–¥–æ–µ –≤–æ—Å–∫—Ä–µ—Å–µ–Ω–∏–µ , —É—Ç—Ä–æ–º , –ø—Ä–∏—Ö–æ–∂–∞–Ω–µ –°–≤—è—Ç–æ...         0   \n",
       "41053  –í –∫–∞–∂–¥–æ–µ –≤–æ—Å–∫—Ä–µ—Å–µ–Ω–∏–µ , —É—Ç—Ä–æ–º , –ø—Ä–∏—Ö–æ–∂–∞–Ω–µ –°–≤—è—Ç–æ...         0   \n",
       "41443                     –ß—Ç–æ-—Ç–æ –Ω–µ –æ—á–µ–Ω—å-—Ç–æ —Ä–∞–∑–æ–±—Ä–∞–ª–∏—Å—å         0   \n",
       "16720  –ß—Ç–æ-—Ç–æ –Ω–µ –æ—á–µ–Ω—å-—Ç–æ —Ä–∞–∑–æ–±—Ä–∞–ª–∏—Å—å ! –ì–æ—Ä–æ–¥ –≤ –ø–æ–ª–Ω–æ...         0   \n",
       "\n",
       "              filename  \n",
       "6658    ./data/news2_2  \n",
       "19489   ./data/news2_2  \n",
       "41053   ./data/news2_2  \n",
       "41443  ./data/blogs_88  \n",
       "16720  ./data/blogs_88  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples[['snippet_x', 'snippet_y', 'relation', 'filename']].sort_values('snippet_x').tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['üòÑÔ∏è–ò–≥–æ—Ä—å –ú–æ—Ä—Å–∫–æ–πüòÑÔ∏èüá∑üá∫ ( @ 812 Hocke —É ) April 18, 2019',\n",
       "        '–ß—Ç–æ-—Ç–æ –Ω–µ –æ—á–µ–Ω—å-—Ç–æ —Ä–∞–∑–æ–±—Ä–∞–ª–∏—Å—å ! –ì–æ—Ä–æ–¥ –≤ –ø–æ–ª–Ω–æ–º –∑–∞–ø—É—Å—Ç–µ–Ω–∏–∏ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è . –ö–∞–Ω–∞–ª—ã –∏ —Ä–µ–∫–∏ –≤ –ø–∞—Ä–∫–æ–≤–∫–∞—Ö , —Ç—Ä–∞–º–≤–∞–∏ –≤ –ø—Ä–æ–±–∫–∞—Ö —Å—Ç–æ—è—Ç , –ø–æ–ª—è —Å—Ç–∏—Ö–∏–π–Ω—ã—Ö –ø–∞—Ä–∫–∏–Ω–≥–æ–≤ , –ø—ã–ª–∏—â–∞ –∏ –≥—Ä—è–∑–∏—â–∞ . –≠—Ç–æ –ø–æ–ª–∑—É—á–µ–µ —É–Ω–∏—á—Ç–æ–∂–µ–Ω–∏–µ –≥–æ—Ä–æ–¥–∞ , –ª—é–¥–∏ –¥–∞–∂–µ –Ω–µ –ø–æ–Ω–∏–º–∞—é—Ç , —á—Ç–æ —á—Ç–æ-—Ç–æ –∏–¥–µ—Ç –Ω–µ —Ç–∞–∫ . –û–¥–Ω–æ –¥–µ—Ä–µ–≤–æ —Å–ø–∏–ª–∏–ª–∏ , –¥—Ä—É–≥–æ–µ –∫—Ä–æ–Ω–∏—Ä–æ–≤–∞–ª–∏ , –Ω–∞ —Ç—Ä–µ—Ç—å–µ–π –ø–ª–æ—â–∞–¥–∏ —Å–¥–µ–ª–∞–ª–∏ –ø–∞—Ä–∫–∏–Ω–≥ , –≤–æ–∫—Ä—É–≥ —á–µ—Ç–≤–µ—Ä—Ç–æ–π –∑–∞–±–æ—Ä , –ø—è—Ç—É—é —Ä–µ–∫—É –∑–∞–±—Ä–æ—Å–∏–ª–∏ –∏ –≤–æ—Ç —É–∂–µ –≤–º–µ—Å—Ç–æ –∫—É–ª—å—Ç—É—Ä–Ω–æ–π –°–µ–≤–µ—Ä–Ω–æ–π –°—Ç–æ–ª–∏—Ü—ã –∫—É–¥–∞ –ø–æ–ª—Å—Ç—Ä–∞–Ω—ã –ø—Ä–∏–µ–∑–∂–∞–µ—Ç –Ω–∞ –±–µ–ª—ã–µ –Ω–æ—á–∏ , —É –Ω–∞—Å —Ñ–∏–≥ –∑–Ω–∞–µ—Ç —á—Ç–æ',\n",
       "        0, './data/blogs_88']], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples[['snippet_x', 'snippet_y', 'relation', 'filename']].sort_values('snippet_x').tail(1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples['len_x'] = train_samples.snippet_x.map(lambda row: len(row.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    30972\n",
       "1    16400\n",
       "Name: relation, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples.relation.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
